============================= test session starts =============================
platform win32 -- Python 3.10.11, pytest-8.4.1, pluggy-1.6.0 -- C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\Scripts\python.exe
cachedir: .pytest_cache
rootdir: C:\Users\Mateusz\source\repos\llm-interactive-proxy
configfile: pyproject.toml
plugins: anyio-4.10.0, asyncio-1.1.0, cov-6.2.1, httpx-0.35.0, respx-0.22.0
asyncio: mode=auto, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 808 items / 118 deselected / 690 selected

tests/integration/test_app.py::test_chat_completions_endpoint PASSED     [  0%]
tests/integration/test_app.py::test_streaming_chat_completions_endpoint PASSED [  0%]
tests/integration/test_app.py::test_command_processing PASSED            [  0%]
tests/integration/test_end_to_end_loop_detection.py::test_loop_detection_with_mocked_backend FAILED [  0%]
tests/integration/test_end_to_end_loop_detection.py::test_loop_detection_in_streaming_response FAILED [  0%]
tests/integration/test_end_to_end_loop_detection.py::test_loop_detection_integration_with_middleware_chain PASSED [  0%]
tests/integration/test_end_to_end_loop_detection.py::test_request_processor_uses_response_processor SKIPPED [  1%]
tests/integration/test_failover_routes.py::test_failover_route_commands PASSED [  1%]
tests/integration/test_failover_routes.py::test_failover_service_routes PASSED [  1%]
tests/integration/test_failover_routes.py::test_backend_service_failover PASSED [  1%]
tests/integration/test_hello_command_integration.py::test_hello_command_integration PASSED [  1%]
tests/integration/test_loop_detection_integration.py::TestLoopDetectionIntegration::test_loop_detection_initialization_on_startup FAILED [  1%]
tests/integration/test_loop_detection_integration.py::TestLoopDetectionIntegration::test_loop_detection_disabled_on_startup PASSED [  1%]
tests/integration/test_loop_detection_integration.py::TestLoopDetectionIntegration::test_streaming_response_loop_detection PASSED [  2%]
tests/integration/test_loop_detection_integration.py::TestLoopDetectionIntegration::test_non_streaming_response_loop_detection PASSED [  2%]
tests/integration/test_loop_detection_integration.py::TestLoopDetectionIntegration::test_environment_variable_configuration FAILED [  2%]
tests/integration/test_loop_detection_integration.py::TestLoopDetectionIntegration::test_per_session_detector_management PASSED [  2%]
tests/integration/test_loop_detection_integration.py::TestLoopDetectionIntegration::test_whitelist_patterns_respected PASSED [  2%]
tests/integration/test_loop_detection_integration.py::TestLoopDetectionCommands::test_loop_detection_status_in_help PASSED [  2%]
tests/integration/test_loop_detection_middleware.py::test_loop_detection_middleware_no_loop PASSED [  2%]
tests/integration/test_loop_detection_middleware.py::test_loop_detection_middleware_with_loop PASSED [  3%]
tests/integration/test_loop_detection_middleware.py::test_full_processing_pipeline PASSED [  3%]
tests/integration/test_loop_detection_middleware.py::test_request_processor_integration PASSED [  3%]
tests/integration/test_loop_detection_middleware.py::test_end_to_end_with_real_app FAILED [  3%]
tests/integration/test_loop_detection_simple.py::TestLoopDetectionSimpleIntegration::test_middleware_configuration PASSED [  3%]
tests/integration/test_loop_detection_simple.py::TestLoopDetectionSimpleIntegration::test_middleware_disabled_configuration PASSED [  3%]
tests/integration/test_loop_detection_simple.py::TestLoopDetectionSimpleIntegration::test_environment_variable_configuration PASSED [  3%]
tests/integration/test_loop_detection_simple.py::TestLoopDetectionSimpleIntegration::test_detector_basic_functionality PASSED [  4%]
tests/integration/test_loop_detection_simple.py::TestLoopDetectionSimpleIntegration::test_whitelist_functionality PASSED [  4%]
tests/integration/test_loop_detection_simple.py::TestLoopDetectionSimpleIntegration::test_streaming_wrapper_basic PASSED [  4%]
tests/integration/test_loop_detection_simple.py::TestLoopDetectionSimpleIntegration::test_config_validation PASSED [  4%]
tests/integration/test_loop_detection_simple.py::TestLoopDetectionSimpleIntegration::test_pattern_detection_thresholds PASSED [  4%]
tests/integration/test_loop_detection_simple.py::TestLoopDetectionSimpleIntegration::test_app_startup_integration PASSED [  4%]
tests/integration/test_models_endpoints.py::TestModelsEndpoints::test_models_endpoint_no_auth PASSED [  4%]
tests/integration/test_models_endpoints.py::TestModelsEndpoints::test_v1_models_endpoint_no_auth PASSED [  5%]
tests/integration/test_models_endpoints.py::TestModelsEndpoints::test_models_endpoint_with_auth PASSED [  5%]
tests/integration/test_models_endpoints.py::TestModelsEndpoints::test_models_endpoint_invalid_auth PASSED [  5%]
tests/integration/test_models_endpoints.py::TestModelsEndpoints::test_models_with_configured_backends FAILED [  5%]
tests/integration/test_models_endpoints.py::TestModelsEndpoints::test_models_format_compliance PASSED [  5%]
tests/integration/test_models_endpoints.py::TestModelsEndpoints::test_models_endpoint_error_handling FAILED [  5%]
tests/integration/test_models_endpoints.py::TestModelsDiscovery::test_discover_openai_models PASSED [  5%]
tests/integration/test_models_endpoints.py::TestModelsDiscovery::test_discover_anthropic_models PASSED [  6%]
tests/integration/test_models_endpoints.py::TestModelsDiscovery::test_discover_models_with_failover PASSED [  6%]
tests/integration/test_models_endpoints.py::TestModelsEndpointIntegration::test_both_endpoints_return_same_data[/models] PASSED [  6%]
tests/integration/test_models_endpoints.py::TestModelsEndpointIntegration::test_both_endpoints_return_same_data[/v1/models] PASSED [  6%]
tests/integration/test_multiple_oneoff_commands.py::TestMultipleOneoffCommands::test_multiple_oneoff_commands_sequence FAILED [  6%]
tests/integration/test_multiple_oneoff_commands.py::TestMultipleOneoffCommands::test_oneoff_commands_different_sessions FAILED [  6%]
tests/integration/test_multiple_oneoff_commands.py::TestMultipleOneoffCommands::test_oneoff_command_with_prompt_in_same_message FAILED [  6%]
tests/integration/test_oneoff_command_integration.py::test_oneoff_command_integration FAILED [  7%]
tests/integration/test_phase1_integration.py::test_integration_bridge_initialization FAILED [  7%]
tests/integration/test_phase1_integration.py::test_hybrid_endpoints_available PASSED [  7%]
tests/integration/test_phase1_integration.py::test_legacy_endpoints_still_work PASSED [  7%]
tests/integration/test_phase1_integration.py::test_feature_flags_environment PASSED [  7%]
tests/integration/test_phase1_integration.py::test_integration_bridge_async_initialization PASSED [  7%]
tests/integration/test_phase1_integration.py::test_adapter_creation PASSED [  7%]
tests/integration/test_pwd_command_integration.py::test_pwd_command_integration_with_project_dir PASSED [  8%]
tests/integration/test_pwd_command_integration.py::test_pwd_command_integration_without_project_dir PASSED [  8%]
tests/integration/test_real_world_loop_detection.py::TestRealWorldLoopDetection::test_example1_kiro_loop_detection PASSED [  8%]
tests/integration/test_real_world_loop_detection.py::TestRealWorldLoopDetection::test_example2_platinum_futures_loop_detection PASSED [  8%]
tests/integration/test_real_world_loop_detection.py::TestRealWorldLoopDetection::test_example3_no_loop_false_positive_check PASSED [  8%]
tests/integration/test_real_world_loop_detection.py::TestRealWorldLoopDetection::test_streaming_loop_detection_example1 PASSED [  8%]
tests/integration/test_real_world_loop_detection.py::TestRealWorldLoopDetection::test_streaming_no_false_positive_example3 PASSED [  8%]
tests/integration/test_real_world_loop_detection.py::TestRealWorldLoopDetection::test_unicode_character_counting PASSED [  9%]
tests/integration/test_tool_call_loop_detection.py::TestToolCallLoopDetection::test_break_mode_blocks_repeated_tool_calls PASSED [  9%]
tests/integration/test_tool_call_loop_detection.py::TestToolCallLoopDetection::test_chance_then_break_mode_transparent_retry_success SKIPPED [  9%]
tests/integration/test_tool_call_loop_detection.py::TestToolCallLoopDetection::test_chance_then_break_mode_transparent_retry_fail SKIPPED [  9%]
tests/integration/test_tool_call_loop_detection.py::TestToolCallLoopDetection::test_different_tool_calls_not_blocked PASSED [  9%]
tests/integration/test_tool_call_loop_detection.py::TestToolCallLoopDetection::test_disabled_tool_call_loop_detection PASSED [  9%]
tests/integration/test_tool_call_loop_detection.py::TestToolCallLoopDetection::test_session_override_takes_precedence SKIPPED [ 10%]
tests/integration/test_updated_hybrid_controller.py::test_get_service_provider_if_available PASSED [ 10%]
tests/integration/test_updated_hybrid_controller.py::test_get_service_provider_if_available_error PASSED [ 10%]
tests/integration/test_updated_hybrid_controller.py::test_hybrid_chat_completions FAILED [ 10%]
tests/integration/test_updated_hybrid_controller.py::test_hybrid_chat_completions_error_handling PASSED [ 10%]
tests/integration/test_updated_hybrid_controller.py::test_hybrid_anthropic_messages FAILED [ 10%]
tests/integration/test_updated_hybrid_controller.py::test_hybrid_anthropic_messages_error_handling PASSED [ 10%]
tests/integration/test_versioned_api.py::test_versioned_endpoint_exists PASSED [ 11%]
tests/integration/test_versioned_api.py::test_versioned_endpoint_with_backend_service PASSED [ 11%]
tests/integration/test_versioned_api.py::test_versioned_endpoint_with_commands PASSED [ 11%]
tests/integration/test_versioned_api.py::test_compatibility_endpoint PASSED [ 11%]
tests/regression/test_chat_completion_regression.py::TestChatCompletionRegression::test_basic_chat_completion PASSED [ 11%]
tests/regression/test_chat_completion_regression.py::TestChatCompletionRegression::test_streaming_chat_completion FAILED [ 11%]
tests/regression/test_chat_completion_regression.py::TestChatCompletionRegression::test_error_handling PASSED [ 11%]
tests/regression/test_chat_completion_regression.py::TestChatCompletionRegression::test_command_processing PASSED [ 12%]
tests/regression/test_mock_backend_regression.py::TestMockBackendRegression::test_legacy_chat_completion PASSED [ 12%]
tests/regression/test_mock_backend_regression.py::TestMockBackendRegression::test_new_chat_completion PASSED [ 12%]
tests/regression/test_mock_backend_regression.py::TestMockBackendRegression::test_legacy_streaming_chat_completion PASSED [ 12%]
tests/regression/test_mock_backend_regression.py::TestMockBackendRegression::test_new_streaming_chat_completion PASSED [ 12%]
tests/regression/test_mock_backend_regression.py::TestMockBackendRegression::test_legacy_tool_calling PASSED [ 12%]
tests/regression/test_mock_backend_regression.py::TestMockBackendRegression::test_new_tool_calling PASSED [ 12%]
tests/test_top_p_fix.py::test_top_p_fix_with_actual_request PASSED       [ 13%]
tests/unit/chat_completions_tests/test_backend_commands.py::test_set_backend_command_integration SKIPPED [ 13%]
tests/unit/chat_completions_tests/test_backend_commands.py::test_unset_backend_command_integration SKIPPED [ 13%]
tests/unit/chat_completions_tests/test_backend_commands.py::test_set_backend_rejects_nonfunctional SKIPPED [ 13%]
tests/unit/chat_completions_tests/test_backend_commands.py::test_set_default_backend_command_integration SKIPPED [ 13%]
tests/unit/chat_completions_tests/test_backend_commands.py::test_unset_default_backend_command_integration SKIPPED [ 13%]
tests/unit/chat_completions_tests/test_basic_proxying.py::test_basic_request_proxying_non_streaming PASSED [ 13%]
tests/unit/chat_completions_tests/test_basic_proxying.py::test_basic_request_proxying_streaming PASSED [ 14%]
tests/unit/chat_completions_tests/test_command_only_requests.py::test_command_only_request_direct_response FAILED [ 14%]
tests/unit/chat_completions_tests/test_command_only_requests.py::test_command_plus_text_direct_response FAILED [ 14%]
tests/unit/chat_completions_tests/test_command_only_requests.py::test_command_with_agent_prefix_direct_response FAILED [ 14%]
tests/unit/chat_completions_tests/test_command_only_requests.py::test_command_only_request_direct_response_explicit_mock FAILED [ 14%]
tests/unit/chat_completions_tests/test_command_only_requests.py::test_hello_command_with_agent_prefix FAILED [ 14%]
tests/unit/chat_completions_tests/test_command_only_requests.py::test_hello_command_followed_by_text FAILED [ 14%]
tests/unit/chat_completions_tests/test_command_only_requests.py::test_hello_command_with_prefix_and_suffix FAILED [ 15%]
tests/unit/chat_completions_tests/test_commands_disabled.py::test_commands_ignored FAILED [ 15%]
tests/unit/chat_completions_tests/test_error_handling.py::test_empty_messages_after_processing_no_commands_bad_request FAILED [ 15%]
tests/unit/chat_completions_tests/test_error_handling.py::test_get_openrouter_headers_no_api_key FAILED [ 15%]
tests/unit/chat_completions_tests/test_error_handling.py::test_invalid_model_noninteractive FAILED [ 15%]
tests/unit/chat_completions_tests/test_failover.py::test_failover_key_rotation PASSED [ 15%]
tests/unit/chat_completions_tests/test_failover.py::test_failover_key_rotation ERROR [ 15%]
tests/unit/chat_completions_tests/test_failover.py::test_failover_missing_keys FAILED [ 15%]
tests/unit/chat_completions_tests/test_help_command.py::test_help_list_commands PASSED [ 16%]
tests/unit/chat_completions_tests/test_help_command.py::test_help_specific_command PASSED [ 16%]
tests/unit/chat_completions_tests/test_interactive_banner.py::test_first_reply_no_automatic_banner FAILED [ 16%]
tests/unit/chat_completions_tests/test_interactive_banner.py::test_hello_command_returns_banner FAILED [ 16%]
tests/unit/chat_completions_tests/test_interactive_commands.py::test_unknown_command_error FAILED [ 16%]
tests/unit/chat_completions_tests/test_interactive_commands.py::test_set_command_confirmation FAILED [ 16%]
tests/unit/chat_completions_tests/test_interactive_commands.py::test_set_backend_confirmation FAILED [ 16%]
tests/unit/chat_completions_tests/test_interactive_commands.py::test_set_backend_nonfunctional FAILED [ 17%]
tests/unit/chat_completions_tests/test_interactive_commands.py::test_set_backend_nonfunctional ERROR [ 17%]
tests/unit/chat_completions_tests/test_interactive_commands.py::test_set_redaction_flag FAILED [ 17%]
tests/unit/chat_completions_tests/test_interactive_commands.py::test_unset_redaction_flag FAILED [ 17%]
tests/unit/chat_completions_tests/test_model_commands.py::test_set_model_command_integration FAILED [ 17%]
tests/unit/chat_completions_tests/test_model_commands.py::test_unset_model_command_integration FAILED [ 17%]
tests/unit/chat_completions_tests/test_multimodal_cross_protocol.py::test_openai_frontend_to_gemini_backend_multimodal FAILED [ 17%]
tests/unit/chat_completions_tests/test_multimodal_cross_protocol.py::test_gemini_frontend_to_openai_backend_multimodal FAILED [ 17%]
tests/unit/chat_completions_tests/test_oneoff_command.py::test_oneoff_command[oneoff_with_prompt-request_payload0] FAILED [ 18%]
tests/unit/chat_completions_tests/test_oneoff_command.py::test_oneoff_command[oneoff_alias_with_prompt-request_payload1] FAILED [ 18%]
tests/unit/chat_completions_tests/test_oneoff_command.py::test_oneoff_command[oneoff_without_prompt-request_payload2] FAILED [ 18%]
tests/unit/chat_completions_tests/test_project_commands.py::test_set_project_command_integration FAILED [ 18%]
tests/unit/chat_completions_tests/test_project_commands.py::test_unset_project_command_integration FAILED [ 18%]
tests/unit/chat_completions_tests/test_project_commands.py::test_set_project_name_alias_integration FAILED [ 18%]
tests/unit/chat_completions_tests/test_project_commands.py::test_unset_project_name_alias_integration FAILED [ 18%]
tests/unit/chat_completions_tests/test_project_commands.py::test_force_set_project_blocks_requests FAILED [ 19%]
tests/unit/chat_completions_tests/test_project_commands.py::test_force_set_project_allows_after_set FAILED [ 19%]

=================================== ERRORS ====================================
_______________ ERROR at teardown of test_failover_key_rotation _______________
.venv\lib\site-packages\pytest_httpx\__init__.py:67: in httpx_mock
    mock._assert_options()
.venv\lib\site-packages\pytest_httpx\_httpx_mock.py:319: in _assert_options
    assert not callbacks_not_executed, (
E   AssertionError: The following responses are mocked but not requested:
E     - Match POST request on https://openrouter.ai/api/v1/chat/completions
E     - Match POST request on https://openrouter.ai/api/v1/chat/completions
E     
E     If this is on purpose, refer to https://github.com/Colin-b/pytest_httpx/blob/master/README.md#allow-to-register-more-responses-than-what-will-be-requested
E   assert not [<pytest_httpx._request_matcher._RequestMatcher object at 0x0000013984A75510>, <pytest_httpx._request_matcher._RequestMatcher object at 0x0000013984A76A10>]
---------------------------- Captured stdout call -----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Error fetching models: 500: API key is not set.
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T14:55:15.319316Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:315 2025-08-17T14:55:15.319815Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.app.middleware_config:middleware_config.py:45 2025-08-17T14:55:15.326849Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:352 2025-08-17T14:55:15.332316Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:369 2025-08-17T14:55:15.334313Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: backend
INFO     src.core.services.command_service:command_service.py:103 Registered command: model
INFO     src.core.services.command_service:command_service.py:103 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:103 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:103 Registered command: openai-url
INFO     src.core.services.command_service:command_service.py:103 Registered command: project
INFO     src.core.services.command_service:command_service.py:103 Registered command: set
INFO     src.core.services.command_service:command_service.py:103 Registered command: unset
INFO     src.core.services.command_service:command_service.py:103 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:103 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:103 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:103 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:281 Registered 27 command handlers with registry
INFO     src.core.app.application_factory:application_factory.py:417 2025-08-17T14:55:15.342813Z [info     ] Initializing legacy backends and state attributes... [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:442 2025-08-17T14:55:15.342813Z [info     ] Initialized legacy backend 'anthropic' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:462 2025-08-17T14:55:15.342813Z [info     ] Initialized legacy backend 'openrouter' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:481 2025-08-17T14:55:15.342813Z [info     ] Initialized legacy backend 'gemini' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:547 2025-08-17T14:55:15.343313Z [info     ] Initialized 3 functional backends: {'gemini', 'anthropic', 'openrouter'} [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:602 2025-08-17T14:55:15.343313Z [info     ] Registered tool call loop detection middleware [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:228 Executing command: create-failover-route with session: 0cdc415d-d615-4d34-9879-6971bef27f2a
INFO     src.core.services.command_service:command_service.py:228 Executing command: route-append with session: 53975d3e-1a92-4247-9b8e-d548c4370bc2
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openai: {'api_key': [], 'api_url': None, 'models': [], 'timeout': 120, 'extra': {}}
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 91, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 97, in _handle_non_streaming_response
    raise HTTPException(
fastapi.exceptions.HTTPException: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
INFO     src.core.app.application_factory:application_factory.py:389 2025-08-17T14:55:15.350313Z [info     ] Shutting down application      [src.core.app.application_factory]
_____________ ERROR at teardown of test_set_backend_nonfunctional _____________
.venv\lib\site-packages\pytest_httpx\__init__.py:67: in httpx_mock
    mock._assert_options()
.venv\lib\site-packages\pytest_httpx\_httpx_mock.py:333: in _assert_options
    assert not self._requests_not_matched, (
E   AssertionError: The following requests were not expected:
E     - GET request on https://api.openai.com/v1/models
E     - POST request on https://api.openai.com/v1/chat/completions
E     
E     If this is on purpose, refer to https://github.com/Colin-b/pytest_httpx/blob/master/README.md#allow-to-not-register-responses-for-every-request
E   assert not [<Request('GET', 'https://api.openai.com/v1/models')>, <Request('POST', 'https://api.openai.com/v1/chat/completions')>]
E    +  where [<Request('GET', 'https://api.openai.com/v1/models')>, <Request('POST', 'https://api.openai.com/v1/chat/completions')>] = <pytest_httpx._httpx_mock.HTTPXMock object at 0x0000013984EAE200>._requests_not_matched
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
---------------------------- Captured stdout call -----------------------------
Error fetching models: No response can be found for GET request on https://api.openai.com/v1/models
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: No response can be found for POST request on https://api.openai.com/v1/chat/completions
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Backend call failed: No response can be found for POST request on https://api.openai.com/v1/chat/completions
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 91, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 101, in _handle_non_streaming_response
    response = await self.client.post(url, json=payload, headers=headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1859, in post
    return await self.request(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1540, in request
    return await self.send(request, auth=auth, follow_redirects=follow_redirects)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pytest_httpx\__init__.py", line 56, in mocked_handle_async_request
    return await mock._handle_async_request(transport, request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pytest_httpx\_httpx_mock.py", line 183, in _handle_async_request
    self._request_not_matched(real_transport, request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pytest_httpx\_httpx_mock.py", line 191, in _request_not_matched
    raise httpx.TimeoutException(
httpx.TimeoutException: No response can be found for POST request on https://api.openai.com/v1/chat/completions

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: No response can be found for POST request on https://api.openai.com/v1/chat/completions
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: No response can be found for POST request on https://api.openai.com/v1/chat/completions
================================== FAILURES ===================================
___________________ test_loop_detection_with_mocked_backend ___________________
tests\integration\test_end_to_end_loop_detection.py:128: in test_loop_detection_with_mocked_backend
    assert response.status_code == 500
E   assert 401 == 500
E    +  where 401 = <Response [401 Unauthorized]>.status_code
---------------------------- Captured stdout call -----------------------------
Response status: 401
Response body: {"detail":"Invalid or missing API key"}
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T14:54:59.398076Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:315 2025-08-17T14:54:59.398076Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.app.middleware_config:middleware_config.py:45 2025-08-17T14:54:59.406575Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:352 2025-08-17T14:54:59.412075Z [info     ] Application built successfully [src.core.app.application_factory]
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for POST /v2/chat/completions from client testclient
__________________ test_loop_detection_in_streaming_response __________________
tests\integration\test_end_to_end_loop_detection.py:221: in test_loop_detection_in_streaming_response
    assert response.status_code == 400
E   assert 401 == 400
E    +  where 401 = <Response [401 Unauthorized]>.status_code
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T14:54:59.664589Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:315 2025-08-17T14:54:59.665089Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.app.middleware_config:middleware_config.py:45 2025-08-17T14:54:59.672590Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:352 2025-08-17T14:54:59.678089Z [info     ] Application built successfully [src.core.app.application_factory]
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for POST /v2/chat/completions from client testclient
_ TestLoopDetectionIntegration.test_loop_detection_initialization_on_startup __
tests\integration\test_loop_detection_integration.py:39: in test_loop_detection_initialization_on_startup
    assert len(middleware.middleware_stack) > 0
E   assert 0 > 0
E    +  where 0 = len([])
E    +    where [] = <src.response_middleware.ResponseMiddleware object at 0x0000013982D25F60>.middleware_stack
---------------------------- Captured stdout call -----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T14:54:59.776507Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:315 2025-08-17T14:54:59.776507Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.app.middleware_config:middleware_config.py:45 2025-08-17T14:54:59.784043Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:352 2025-08-17T14:54:59.789543Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:369 2025-08-17T14:54:59.791004Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: backend
INFO     src.core.services.command_service:command_service.py:103 Registered command: model
INFO     src.core.services.command_service:command_service.py:103 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:103 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:103 Registered command: openai-url
INFO     src.core.services.command_service:command_service.py:103 Registered command: project
INFO     src.core.services.command_service:command_service.py:103 Registered command: set
INFO     src.core.services.command_service:command_service.py:103 Registered command: unset
INFO     src.core.services.command_service:command_service.py:103 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:103 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:103 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:103 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:281 Registered 27 command handlers with registry
INFO     src.core.app.application_factory:application_factory.py:417 2025-08-17T14:54:59.799004Z [info     ] Initializing legacy backends and state attributes... [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:442 2025-08-17T14:54:59.799504Z [info     ] Initialized legacy backend 'anthropic' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:462 2025-08-17T14:54:59.799504Z [info     ] Initialized legacy backend 'openrouter' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:481 2025-08-17T14:54:59.799504Z [info     ] Initialized legacy backend 'gemini' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:547 2025-08-17T14:54:59.799504Z [info     ] Initialized 3 functional backends: {'gemini', 'anthropic', 'openrouter'} [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:602 2025-08-17T14:54:59.799504Z [info     ] Registered tool call loop detection middleware [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:389 2025-08-17T14:54:59.800004Z [info     ] Shutting down application      [src.core.app.application_factory]
____ TestLoopDetectionIntegration.test_environment_variable_configuration _____
tests\integration\test_loop_detection_integration.py:209: in test_environment_variable_configuration
    assert len(loop_processors) == 0
E   assert 1 == 0
E    +  where 1 = len([<src.response_middleware.LoopDetectionProcessor object at 0x0000013983580910>])
---------------------------- Captured stdout call -----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T14:54:59.840507Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:315 2025-08-17T14:54:59.840507Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.app.middleware_config:middleware_config.py:45 2025-08-17T14:54:59.848004Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:352 2025-08-17T14:54:59.853504Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:369 2025-08-17T14:54:59.854507Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: backend
INFO     src.core.services.command_service:command_service.py:103 Registered command: model
INFO     src.core.services.command_service:command_service.py:103 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:103 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:103 Registered command: openai-url
INFO     src.core.services.command_service:command_service.py:103 Registered command: project
INFO     src.core.services.command_service:command_service.py:103 Registered command: set
INFO     src.core.services.command_service:command_service.py:103 Registered command: unset
INFO     src.core.services.command_service:command_service.py:103 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:103 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:103 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:103 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:281 Registered 27 command handlers with registry
INFO     src.core.app.application_factory:application_factory.py:417 2025-08-17T14:54:59.863047Z [info     ] Initializing legacy backends and state attributes... [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:442 2025-08-17T14:54:59.863047Z [info     ] Initialized legacy backend 'anthropic' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:462 2025-08-17T14:54:59.863548Z [info     ] Initialized legacy backend 'openrouter' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:481 2025-08-17T14:54:59.863548Z [info     ] Initialized legacy backend 'gemini' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:547 2025-08-17T14:54:59.863548Z [info     ] Initialized 3 functional backends: {'gemini', 'anthropic', 'openrouter'} [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:602 2025-08-17T14:54:59.863548Z [info     ] Registered tool call loop detection middleware [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:389 2025-08-17T14:54:59.864049Z [info     ] Shutting down application      [src.core.app.application_factory]
________________________ test_end_to_end_with_real_app ________________________
tests\integration\test_loop_detection_middleware.py:359: in test_end_to_end_with_real_app
    assert response.status_code == 400
E   assert 200 == 400
E    +  where 200 = <Response [200 OK]>.status_code
---------------------------- Captured stdout call -----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T14:54:59.939504Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:315 2025-08-17T14:54:59.939504Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.app.middleware_config:middleware_config.py:45 2025-08-17T14:54:59.949006Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:352 2025-08-17T14:54:59.954005Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.integration.bridge:bridge.py:50 New architecture initialized
INFO     src.core.app.application_factory:application_factory.py:369 2025-08-17T14:54:59.957005Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: backend
INFO     src.core.services.command_service:command_service.py:103 Registered command: model
INFO     src.core.services.command_service:command_service.py:103 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:103 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:103 Registered command: openai-url
INFO     src.core.services.command_service:command_service.py:103 Registered command: project
INFO     src.core.services.command_service:command_service.py:103 Registered command: set
INFO     src.core.services.command_service:command_service.py:103 Registered command: unset
INFO     src.core.services.command_service:command_service.py:103 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:103 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:103 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:103 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:281 Registered 27 command handlers with registry
INFO     src.core.app.application_factory:application_factory.py:417 2025-08-17T14:54:59.966004Z [info     ] Initializing legacy backends and state attributes... [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:442 2025-08-17T14:54:59.966004Z [info     ] Initialized legacy backend 'anthropic' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:462 2025-08-17T14:54:59.966004Z [info     ] Initialized legacy backend 'openrouter' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:481 2025-08-17T14:54:59.966004Z [info     ] Initialized legacy backend 'gemini' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:547 2025-08-17T14:54:59.966504Z [info     ] Initialized 3 functional backends: {'gemini', 'anthropic', 'openrouter'} [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:602 2025-08-17T14:54:59.966504Z [info     ] Registered tool call loop detection middleware [src.core.app.application_factory]
INFO     src.core.app.controllers.chat_controller:chat_controller.py:59 Handling chat completion request: model=test-model
INFO     src.core.app.application_factory:application_factory.py:389 2025-08-17T14:54:59.970005Z [info     ] Shutting down application      [src.core.app.application_factory]
__________ TestModelsEndpoints.test_models_with_configured_backends ___________
  + Exception Group Traceback (most recent call last):
  |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 77, in collapse_excgroups
  |     yield
  |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 183, in __call__
  |     async with anyio.create_task_group() as task_group:
  |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\anyio\_backends\_asyncio.py", line 772, in __aexit__
  |     raise BaseExceptionGroup(
  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)
  +-+---------------- 1 ----------------
    | Traceback (most recent call last):
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\_pytest\runner.py", line 344, in from_call
    |     result: TResult | None = func()
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\_pytest\runner.py", line 246, in <lambda>
    |     lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_hooks.py", line 512, in __call__
    |     return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_manager.py", line 120, in _hookexec
    |     return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 167, in _multicall
    |     raise exception
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 139, in _multicall
    |     teardown.throw(exception)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\_pytest\logging.py", line 850, in pytest_runtest_call
    |     yield
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 139, in _multicall
    |     teardown.throw(exception)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\_pytest\capture.py", line 900, in pytest_runtest_call
    |     return (yield)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 139, in _multicall
    |     teardown.throw(exception)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\_pytest\skipping.py", line 263, in pytest_runtest_call
    |     return (yield)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 121, in _multicall
    |     res = hook_impl.function(*args)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\_pytest\runner.py", line 178, in pytest_runtest_call
    |     item.runtest()
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\_pytest\python.py", line 1671, in runtest
    |     self.ihook.pytest_pyfunc_call(pyfuncitem=self)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_hooks.py", line 512, in __call__
    |     return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_manager.py", line 120, in _hookexec
    |     return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 167, in _multicall
    |     raise exception
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 139, in _multicall
    |     teardown.throw(exception)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 53, in run_old_style_hookwrapper
    |     return result.get_result()
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_result.py", line 103, in get_result
    |     raise exc.with_traceback(tb)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 38, in run_old_style_hookwrapper
    |     res = yield
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 121, in _multicall
    |     res = hook_impl.function(*args)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\_pytest\python.py", line 157, in pytest_pyfunc_call
    |     result = testfunction(**testargs)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\tests\integration\test_models_endpoints.py", line 127, in test_models_with_configured_backends
    |     response = client.get("/models")
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\testclient.py", line 479, in get
    |     return super().get(
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1053, in get
    |     return self.request(
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\testclient.py", line 451, in request
    |     return super().request(
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 825, in request
    |     return self.send(request, auth=auth, follow_redirects=follow_redirects)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 914, in send
    |     response = self._send_handling_auth(
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    |     response = self._send_handling_redirects(
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    |     response = self._send_single_request(request)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1014, in _send_single_request
    |     response = transport.handle_request(request)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\testclient.py", line 354, in handle_request
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\testclient.py", line 351, in handle_request
    |     portal.call(self.app, scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\anyio\from_thread.py", line 291, in call
    |     return cast(T_Retval, self.start_task_soon(func, *args).result())
    |   File "C:\Program Files\Python310\lib\concurrent\futures\_base.py", line 458, in result
    |     return self.__get_result()
    |   File "C:\Program Files\Python310\lib\concurrent\futures\_base.py", line 403, in __get_result
    |     raise self._exception
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\anyio\from_thread.py", line 222, in _call_func
    |     retval = await retval_or_awaitable
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\applications.py", line 1054, in __call__
    |     await super().__call__(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\applications.py", line 113, in __call__
    |     await self.middleware_stack(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\errors.py", line 186, in __call__
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\errors.py", line 164, in __call__
    |     await self.app(scope, receive, _send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 182, in __call__
    |     with recv_stream, send_stream, collapse_excgroups():
    |   File "C:\Program Files\Python310\lib\contextlib.py", line 153, in __exit__
    |     self.gen.throw(typ, value, traceback)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 83, in collapse_excgroups
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 184, in __call__
    |     response = await self.dispatch_func(request, call_next)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\response_middleware.py", line 437, in dispatch
    |     response = await call_next(request)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 159, in call_next
    |     raise app_exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 144, in coro
    |     await self.app(scope, receive_or_disconnect, send_no_error)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 182, in __call__
    |     with recv_stream, send_stream, collapse_excgroups():
    |   File "C:\Program Files\Python310\lib\contextlib.py", line 153, in __exit__
    |     self.gen.throw(typ, value, traceback)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 83, in collapse_excgroups
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 184, in __call__
    |     response = await self.dispatch_func(request, call_next)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\request_middleware.py", line 210, in dispatch
    |     response = await call_next(request)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 159, in call_next
    |     raise app_exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 144, in coro
    |     await self.app(scope, receive_or_disconnect, send_no_error)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\cors.py", line 85, in __call__
    |     await self.app(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\exceptions.py", line 63, in __call__
    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    |     await app(scope, receive, sender)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 716, in __call__
    |     await self.middleware_stack(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 736, in app
    |     await route.handle(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 290, in handle
    |     await self.app(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 78, in app
    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    |     await app(scope, receive, sender)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 75, in app
    |     response = await f(request)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\routing.py", line 292, in app
    |     solved_result = await solve_dependencies(
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\dependencies\utils.py", line 638, in solve_dependencies
    |     solved = await call(**solved_result.values)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\app\controllers\models_controller.py", line 32, in get_backend_service
    |     return service_provider.get_required_service(IBackendService)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\di\container.py", line 140, in get_required_service
    |     raise KeyError(f"No service registered for {service_type.__name__}")
    |   File "C:\Program Files\Python310\lib\unittest\mock.py", line 645, in __getattr__
    |     raise AttributeError(name)
    | AttributeError: __name__. Did you mean: '__hash__'?
    +------------------------------------

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\tests\integration\test_models_endpoints.py", line 127, in test_models_with_configured_backends
    response = client.get("/models")
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\testclient.py", line 479, in get
    return super().get(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1053, in get
    return self.request(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\testclient.py", line 451, in request
    return super().request(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 825, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\testclient.py", line 354, in handle_request
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\testclient.py", line 351, in handle_request
    portal.call(self.app, scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\anyio\from_thread.py", line 291, in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
  File "C:\Program Files\Python310\lib\concurrent\futures\_base.py", line 458, in result
    return self.__get_result()
  File "C:\Program Files\Python310\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\anyio\from_thread.py", line 222, in _call_func
    retval = await retval_or_awaitable
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\applications.py", line 1054, in __call__
    await super().__call__(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\applications.py", line 113, in __call__
    await self.middleware_stack(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\errors.py", line 186, in __call__
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 182, in __call__
    with recv_stream, send_stream, collapse_excgroups():
  File "C:\Program Files\Python310\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 83, in collapse_excgroups
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 184, in __call__
    response = await self.dispatch_func(request, call_next)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\response_middleware.py", line 437, in dispatch
    response = await call_next(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 159, in call_next
    raise app_exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 144, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 182, in __call__
    with recv_stream, send_stream, collapse_excgroups():
  File "C:\Program Files\Python310\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 83, in collapse_excgroups
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 184, in __call__
    response = await self.dispatch_func(request, call_next)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\request_middleware.py", line 210, in dispatch
    response = await call_next(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 159, in call_next
    raise app_exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 144, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\cors.py", line 85, in __call__
    await self.app(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 78, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 75, in app
    response = await f(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\routing.py", line 292, in app
    solved_result = await solve_dependencies(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\dependencies\utils.py", line 638, in solve_dependencies
    solved = await call(**solved_result.values)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\app\controllers\models_controller.py", line 32, in get_backend_service
    return service_provider.get_required_service(IBackendService)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\di\container.py", line 140, in get_required_service
    raise KeyError(f"No service registered for {service_type.__name__}")
  File "C:\Program Files\Python310\lib\unittest\mock.py", line 645, in __getattr__
    raise AttributeError(name)
AttributeError: __name__. Did you mean: '__hash__'?

During handling of the above exception, another exception occurred:
tests\integration\test_models_endpoints.py:127: in test_models_with_configured_backends
    response = client.get("/models")
.venv\lib\site-packages\starlette\testclient.py:479: in get
    return super().get(
.venv\lib\site-packages\httpx\_client.py:1053: in get
    return self.request(
.venv\lib\site-packages\starlette\testclient.py:451: in request
    return super().request(
.venv\lib\site-packages\httpx\_client.py:825: in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
.venv\lib\site-packages\httpx\_client.py:914: in send
    response = self._send_handling_auth(
.venv\lib\site-packages\httpx\_client.py:942: in _send_handling_auth
    response = self._send_handling_redirects(
.venv\lib\site-packages\httpx\_client.py:979: in _send_handling_redirects
    response = self._send_single_request(request)
.venv\lib\site-packages\httpx\_client.py:1014: in _send_single_request
    response = transport.handle_request(request)
.venv\lib\site-packages\starlette\testclient.py:354: in handle_request
    raise exc
.venv\lib\site-packages\starlette\testclient.py:351: in handle_request
    portal.call(self.app, scope, receive, send)
.venv\lib\site-packages\anyio\from_thread.py:291: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
C:\Program Files\Python310\lib\concurrent\futures\_base.py:458: in result
    return self.__get_result()
C:\Program Files\Python310\lib\concurrent\futures\_base.py:403: in __get_result
    raise self._exception
.venv\lib\site-packages\anyio\from_thread.py:222: in _call_func
    retval = await retval_or_awaitable
.venv\lib\site-packages\fastapi\applications.py:1054: in __call__
    await super().__call__(scope, receive, send)
.venv\lib\site-packages\starlette\applications.py:113: in __call__
    await self.middleware_stack(scope, receive, send)
.venv\lib\site-packages\starlette\middleware\errors.py:186: in __call__
    raise exc
.venv\lib\site-packages\starlette\middleware\errors.py:164: in __call__
    await self.app(scope, receive, _send)
.venv\lib\site-packages\starlette\middleware\base.py:182: in __call__
    with recv_stream, send_stream, collapse_excgroups():
C:\Program Files\Python310\lib\contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
.venv\lib\site-packages\starlette\_utils.py:83: in collapse_excgroups
    raise exc
.venv\lib\site-packages\starlette\middleware\base.py:184: in __call__
    response = await self.dispatch_func(request, call_next)
src\response_middleware.py:437: in dispatch
    response = await call_next(request)
.venv\lib\site-packages\starlette\middleware\base.py:159: in call_next
    raise app_exc
.venv\lib\site-packages\starlette\middleware\base.py:144: in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
.venv\lib\site-packages\starlette\middleware\base.py:182: in __call__
    with recv_stream, send_stream, collapse_excgroups():
C:\Program Files\Python310\lib\contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
.venv\lib\site-packages\starlette\_utils.py:83: in collapse_excgroups
    raise exc
.venv\lib\site-packages\starlette\middleware\base.py:184: in __call__
    response = await self.dispatch_func(request, call_next)
src\request_middleware.py:210: in dispatch
    response = await call_next(request)
.venv\lib\site-packages\starlette\middleware\base.py:159: in call_next
    raise app_exc
.venv\lib\site-packages\starlette\middleware\base.py:144: in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
.venv\lib\site-packages\starlette\middleware\cors.py:85: in __call__
    await self.app(scope, receive, send)
.venv\lib\site-packages\starlette\middleware\exceptions.py:63: in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
.venv\lib\site-packages\starlette\_exception_handler.py:53: in wrapped_app
    raise exc
.venv\lib\site-packages\starlette\_exception_handler.py:42: in wrapped_app
    await app(scope, receive, sender)
.venv\lib\site-packages\starlette\routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
.venv\lib\site-packages\starlette\routing.py:736: in app
    await route.handle(scope, receive, send)
.venv\lib\site-packages\starlette\routing.py:290: in handle
    await self.app(scope, receive, send)
.venv\lib\site-packages\starlette\routing.py:78: in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
.venv\lib\site-packages\starlette\_exception_handler.py:53: in wrapped_app
    raise exc
.venv\lib\site-packages\starlette\_exception_handler.py:42: in wrapped_app
    await app(scope, receive, sender)
.venv\lib\site-packages\starlette\routing.py:75: in app
    response = await f(request)
.venv\lib\site-packages\fastapi\routing.py:292: in app
    solved_result = await solve_dependencies(
.venv\lib\site-packages\fastapi\dependencies\utils.py:638: in solve_dependencies
    solved = await call(**solved_result.values)
src\core\app\controllers\models_controller.py:32: in get_backend_service
    return service_provider.get_required_service(IBackendService)
src\core\di\container.py:140: in get_required_service
    raise KeyError(f"No service registered for {service_type.__name__}")
C:\Program Files\Python310\lib\unittest\mock.py:645: in __getattr__
    raise AttributeError(name)
E   AttributeError: __name__. Did you mean: '__hash__'?
---------------------------- Captured stdout call -----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T14:55:00.128005Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:315 2025-08-17T14:55:00.128005Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.app.middleware_config:middleware_config.py:48 2025-08-17T14:55:00.136004Z [info     ] API Key authentication is disabled [src.core.app.middleware_config]
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:352 2025-08-17T14:55:00.141004Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:369 2025-08-17T14:55:00.142510Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: backend
INFO     src.core.services.command_service:command_service.py:103 Registered command: model
INFO     src.core.services.command_service:command_service.py:103 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:103 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:103 Registered command: openai-url
INFO     src.core.services.command_service:command_service.py:103 Registered command: project
INFO     src.core.services.command_service:command_service.py:103 Registered command: set
INFO     src.core.services.command_service:command_service.py:103 Registered command: unset
INFO     src.core.services.command_service:command_service.py:103 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:103 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:103 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:103 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:281 Registered 27 command handlers with registry
INFO     src.core.app.application_factory:application_factory.py:417 2025-08-17T14:55:00.151004Z [info     ] Initializing legacy backends and state attributes... [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:442 2025-08-17T14:55:00.151004Z [info     ] Initialized legacy backend 'anthropic' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:462 2025-08-17T14:55:00.151004Z [info     ] Initialized legacy backend 'openrouter' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:481 2025-08-17T14:55:00.151004Z [info     ] Initialized legacy backend 'gemini' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:498 2025-08-17T14:55:00.712164Z [info     ] Initialized legacy backend 'openai' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:547 2025-08-17T14:55:00.712164Z [info     ] Initialized 4 functional backends: {'openai', 'gemini', 'anthropic', 'openrouter'} [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:602 2025-08-17T14:55:00.712164Z [info     ] Registered tool call loop detection middleware [src.core.app.application_factory]
ERROR    src.core.app.error_handlers:error_handlers.py:110 Unhandled exception
  + Exception Group Traceback (most recent call last):
  |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 77, in collapse_excgroups
  |     yield
  |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 183, in __call__
  |     async with anyio.create_task_group() as task_group:
  |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\anyio\_backends\_asyncio.py", line 772, in __aexit__
  |     raise BaseExceptionGroup(
  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)
  +-+---------------- 1 ----------------
    | Traceback (most recent call last):
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\errors.py", line 164, in __call__
    |     await self.app(scope, receive, _send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 182, in __call__
    |     with recv_stream, send_stream, collapse_excgroups():
    |   File "C:\Program Files\Python310\lib\contextlib.py", line 153, in __exit__
    |     self.gen.throw(typ, value, traceback)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 83, in collapse_excgroups
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 184, in __call__
    |     response = await self.dispatch_func(request, call_next)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\response_middleware.py", line 437, in dispatch
    |     response = await call_next(request)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 159, in call_next
    |     raise app_exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 144, in coro
    |     await self.app(scope, receive_or_disconnect, send_no_error)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 182, in __call__
    |     with recv_stream, send_stream, collapse_excgroups():
    |   File "C:\Program Files\Python310\lib\contextlib.py", line 153, in __exit__
    |     self.gen.throw(typ, value, traceback)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 83, in collapse_excgroups
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 184, in __call__
    |     response = await self.dispatch_func(request, call_next)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\request_middleware.py", line 210, in dispatch
    |     response = await call_next(request)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 159, in call_next
    |     raise app_exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 144, in coro
    |     await self.app(scope, receive_or_disconnect, send_no_error)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\cors.py", line 85, in __call__
    |     await self.app(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\exceptions.py", line 63, in __call__
    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    |     await app(scope, receive, sender)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 716, in __call__
    |     await self.middleware_stack(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 736, in app
    |     await route.handle(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 290, in handle
    |     await self.app(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 78, in app
    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    |     await app(scope, receive, sender)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 75, in app
    |     response = await f(request)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\routing.py", line 292, in app
    |     solved_result = await solve_dependencies(
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\dependencies\utils.py", line 638, in solve_dependencies
    |     solved = await call(**solved_result.values)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\app\controllers\models_controller.py", line 32, in get_backend_service
    |     return service_provider.get_required_service(IBackendService)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\di\container.py", line 140, in get_required_service
    |     raise KeyError(f"No service registered for {service_type.__name__}")
    |   File "C:\Program Files\Python310\lib\unittest\mock.py", line 645, in __getattr__
    |     raise AttributeError(name)
    | AttributeError: __name__. Did you mean: '__hash__'?
    +------------------------------------

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 182, in __call__
    with recv_stream, send_stream, collapse_excgroups():
  File "C:\Program Files\Python310\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 83, in collapse_excgroups
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 184, in __call__
    response = await self.dispatch_func(request, call_next)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\response_middleware.py", line 437, in dispatch
    response = await call_next(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 159, in call_next
    raise app_exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 144, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 182, in __call__
    with recv_stream, send_stream, collapse_excgroups():
  File "C:\Program Files\Python310\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 83, in collapse_excgroups
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 184, in __call__
    response = await self.dispatch_func(request, call_next)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\request_middleware.py", line 210, in dispatch
    response = await call_next(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 159, in call_next
    raise app_exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 144, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\cors.py", line 85, in __call__
    await self.app(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 78, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 75, in app
    response = await f(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\routing.py", line 292, in app
    solved_result = await solve_dependencies(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\dependencies\utils.py", line 638, in solve_dependencies
    solved = await call(**solved_result.values)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\app\controllers\models_controller.py", line 32, in get_backend_service
    return service_provider.get_required_service(IBackendService)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\di\container.py", line 140, in get_required_service
    raise KeyError(f"No service registered for {service_type.__name__}")
  File "C:\Program Files\Python310\lib\unittest\mock.py", line 645, in __getattr__
    raise AttributeError(name)
AttributeError: __name__. Did you mean: '__hash__'?
INFO     src.core.app.application_factory:application_factory.py:389 2025-08-17T14:55:00.719666Z [info     ] Shutting down application      [src.core.app.application_factory]
___________ TestModelsEndpoints.test_models_endpoint_error_handling ___________
tests\integration\test_models_endpoints.py:170: in test_models_endpoint_error_handling
    assert response.status_code == 500
E   assert 200 == 500
E    +  where 200 = <Response [200 OK]>.status_code
---------------------------- Captured stdout call -----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T14:55:01.071664Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:315 2025-08-17T14:55:01.071664Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.app.middleware_config:middleware_config.py:48 2025-08-17T14:55:01.079663Z [info     ] API Key authentication is disabled [src.core.app.middleware_config]
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:352 2025-08-17T14:55:01.085164Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:369 2025-08-17T14:55:01.088664Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: backend
INFO     src.core.services.command_service:command_service.py:103 Registered command: model
INFO     src.core.services.command_service:command_service.py:103 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:103 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:103 Registered command: openai-url
INFO     src.core.services.command_service:command_service.py:103 Registered command: project
INFO     src.core.services.command_service:command_service.py:103 Registered command: set
INFO     src.core.services.command_service:command_service.py:103 Registered command: unset
INFO     src.core.services.command_service:command_service.py:103 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:103 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:103 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:103 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:281 Registered 27 command handlers with registry
INFO     src.core.app.application_factory:application_factory.py:417 2025-08-17T14:55:01.097164Z [info     ] Initializing legacy backends and state attributes... [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:442 2025-08-17T14:55:01.097164Z [info     ] Initialized legacy backend 'anthropic' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:462 2025-08-17T14:55:01.097164Z [info     ] Initialized legacy backend 'openrouter' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:481 2025-08-17T14:55:01.097164Z [info     ] Initialized legacy backend 'gemini' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:547 2025-08-17T14:55:01.097664Z [info     ] Initialized 3 functional backends: {'gemini', 'anthropic', 'openrouter'} [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:602 2025-08-17T14:55:01.097664Z [info     ] Registered tool call loop detection middleware [src.core.app.application_factory]
INFO     src.core.app.controllers.models_controller:models_controller.py:47 Listing available models
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for anthropic: {'api_key': ['test-key-anthropic'], 'api_url': None, 'models': [], 'timeout': 120, 'extra': {}}
INFO     src.core.services.backend_service:backend_service.py:352 Converted config for Anthropic: {'key_name': <BackendType.ANTHROPIC: 'anthropic'>, 'api_key': 'test-key-anthropic'}
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openrouter: {'api_key': ['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'], 'api_url': None, 'models': [], 'timeout': 120, 'extra': {}}
WARNING  src.core.app.controllers.models_controller:models_controller.py:122 Failed to get models from openrouter: Failed to create backend openrouter: OpenRouterBackend requires 'openrouter_headers_provider' (Callable) and 'key_name' (str) in kwargs.
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for gemini: {'api_key': ['AIzaSyAcpFWc7ijSBO4xM48MKRVyV6LNCFnihUM', 'AIzaSyA2LTvTF9H-CuQtAGVLnolFhCc79sAUzm0', 'AIzaSyA2LTvTF9H-CuQtAGVLnolFhCc79sAUzm0', 'AIzaSyBO5QAP9t4ilOrnzJFLMrsO67WeYuUNS5A', 'AIzaSyAwEGYb4X8mEreuWy9UViZjkYkZJljhBbE'], 'api_url': None, 'models': [], 'timeout': 120, 'extra': {}}
WARNING  src.core.app.controllers.models_controller:models_controller.py:122 Failed to get models from gemini: Failed to create backend gemini: gemini_api_base_url, key_name, and api_key are required for GeminiBackend
INFO     src.core.app.controllers.models_controller:models_controller.py:128 No models discovered from backends, using default models
INFO     src.core.app.controllers.models_controller:models_controller.py:146 Returning 6 models
INFO     src.core.app.application_factory:application_factory.py:389 2025-08-17T14:55:01.100664Z [info     ] Shutting down application      [src.core.app.application_factory]
______ TestMultipleOneoffCommands.test_multiple_oneoff_commands_sequence ______
tests\integration\test_multiple_oneoff_commands.py:106: in test_multiple_oneoff_commands_sequence
    assert response1.status_code == 200
E   assert 500 == 200
E    +  where 500 = <Response [500 Internal Server Error]>.status_code
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 91, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 107, in _handle_non_streaming_response
    raise HTTPException(status_code=response.status_code, detail=err)
fastapi.exceptions.HTTPException: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
_____ TestMultipleOneoffCommands.test_oneoff_commands_different_sessions ______
tests\integration\test_multiple_oneoff_commands.py:292: in test_oneoff_commands_different_sessions
    assert response2.status_code == 200
E   assert 500 == 200
E    +  where 500 = <Response [500 Internal Server Error]>.status_code
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 91, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 107, in _handle_non_streaming_response
    raise HTTPException(status_code=response.status_code, detail=err)
fastapi.exceptions.HTTPException: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
_ TestMultipleOneoffCommands.test_oneoff_command_with_prompt_in_same_message __
tests\integration\test_multiple_oneoff_commands.py:386: in test_oneoff_command_with_prompt_in_same_message
    assert (
E   AssertionError: assert 'Mocked response' in 'One-off route set to openrouter/cypher-alpha:free.'
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
_______________________ test_oneoff_command_integration _______________________
tests\integration\test_oneoff_command_integration.py:134: in test_oneoff_command_integration
    assert response.json()["model"] == "gpt-4"
E   AssertionError: assert 'gpt-3.5-turbo' == 'gpt-4'
E     
E     - gpt-4
E     + gpt-3.5-turbo
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T14:55:04.428238Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:315 2025-08-17T14:55:04.428238Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.app.middleware_config:middleware_config.py:48 2025-08-17T14:55:04.435777Z [info     ] API Key authentication is disabled [src.core.app.middleware_config]
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:352 2025-08-17T14:55:04.441234Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
INFO     src.core.app.controllers.chat_controller:chat_controller.py:59 Handling chat completion request: model=gpt-3.5-turbo
INFO     src.core.services.command_service:command_service.py:228 Executing command: oneoff with session: test-oneoff-session
INFO     src.core.app.controllers.chat_controller:chat_controller.py:59 Handling chat completion request: model=gpt-3.5-turbo
___________________ test_integration_bridge_initialization ____________________
tests\integration\test_phase1_integration.py:19: in test_integration_bridge_initialization
    assert hasattr(app.state, "config")
E   AssertionError: assert False
E    +  where False = hasattr(<starlette.datastructures.State object at 0x0000013984D91330>, 'config')
E    +    where <starlette.datastructures.State object at 0x0000013984D91330> = <fastapi.applications.FastAPI object at 0x00000139835888E0>.state
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T14:55:04.452231Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:315 2025-08-17T14:55:04.452731Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.app.middleware_config:middleware_config.py:45 2025-08-17T14:55:04.459731Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:352 2025-08-17T14:55:04.464731Z [info     ] Application built successfully [src.core.app.application_factory]
________________________ test_hybrid_chat_completions _________________________
tests\integration\test_updated_hybrid_controller.py:117: in test_hybrid_chat_completions
    assert response.json() == {"message": "processed"}
E   AssertionError: assert {} == {'message': 'processed'}
E     
E     Right contains 1 more item:
E     {'message': 'processed'}
E     
E     Full diff:
E     + {}
E     - {
E     -     'message': 'processed',
E     - }
_______________________ test_hybrid_anthropic_messages ________________________
tests\integration\test_updated_hybrid_controller.py:147: in test_hybrid_anthropic_messages
    response = client.post(
.venv\lib\site-packages\starlette\testclient.py:552: in post
    return super().post(
.venv\lib\site-packages\httpx\_client.py:1144: in post
    return self.request(
.venv\lib\site-packages\starlette\testclient.py:451: in request
    return super().request(
.venv\lib\site-packages\httpx\_client.py:825: in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
.venv\lib\site-packages\httpx\_client.py:914: in send
    response = self._send_handling_auth(
.venv\lib\site-packages\httpx\_client.py:942: in _send_handling_auth
    response = self._send_handling_redirects(
.venv\lib\site-packages\httpx\_client.py:979: in _send_handling_redirects
    response = self._send_single_request(request)
.venv\lib\site-packages\httpx\_client.py:1014: in _send_single_request
    response = transport.handle_request(request)
.venv\lib\site-packages\starlette\testclient.py:354: in handle_request
    raise exc
.venv\lib\site-packages\starlette\testclient.py:351: in handle_request
    portal.call(self.app, scope, receive, send)
.venv\lib\site-packages\anyio\from_thread.py:291: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
C:\Program Files\Python310\lib\concurrent\futures\_base.py:451: in result
    return self.__get_result()
C:\Program Files\Python310\lib\concurrent\futures\_base.py:403: in __get_result
    raise self._exception
.venv\lib\site-packages\anyio\from_thread.py:222: in _call_func
    retval = await retval_or_awaitable
.venv\lib\site-packages\fastapi\applications.py:1054: in __call__
    await super().__call__(scope, receive, send)
.venv\lib\site-packages\starlette\applications.py:113: in __call__
    await self.middleware_stack(scope, receive, send)
.venv\lib\site-packages\starlette\middleware\errors.py:186: in __call__
    raise exc
.venv\lib\site-packages\starlette\middleware\errors.py:164: in __call__
    await self.app(scope, receive, _send)
.venv\lib\site-packages\starlette\middleware\exceptions.py:63: in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
.venv\lib\site-packages\starlette\_exception_handler.py:53: in wrapped_app
    raise exc
.venv\lib\site-packages\starlette\_exception_handler.py:42: in wrapped_app
    await app(scope, receive, sender)
.venv\lib\site-packages\starlette\routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
.venv\lib\site-packages\starlette\routing.py:736: in app
    await route.handle(scope, receive, send)
.venv\lib\site-packages\starlette\routing.py:290: in handle
    await self.app(scope, receive, send)
.venv\lib\site-packages\starlette\routing.py:78: in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
.venv\lib\site-packages\starlette\_exception_handler.py:53: in wrapped_app
    raise exc
.venv\lib\site-packages\starlette\_exception_handler.py:42: in wrapped_app
    await app(scope, receive, sender)
.venv\lib\site-packages\starlette\routing.py:75: in app
    response = await f(request)
.venv\lib\site-packages\fastapi\routing.py:302: in app
    raw_response = await run_endpoint_function(
.venv\lib\site-packages\fastapi\routing.py:213: in run_endpoint_function
    return await dependant.call(**values)
src\core\integration\hybrid_controller.py:139: in hybrid_anthropic_messages
    anthropic_response_data = openai_to_anthropic_response(openai_response_data)
src\anthropic_converters.py:49: in openai_to_anthropic_response
    choice = oai_dict["choices"][0]
E   KeyError: 'choices'
_________ TestChatCompletionRegression.test_streaming_chat_completion _________
tests\regression\test_chat_completion_regression.py:129: in test_streaming_chat_completion
    assert "Hello" in content
E   assert 'Hello' in "ERROR: Stream processing failed: 'async for' requires an object with __aiter__ method, got StreamingResponse"
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T14:55:05.281707Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:315 2025-08-17T14:55:05.281707Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.app.middleware_config:middleware_config.py:48 2025-08-17T14:55:05.289206Z [info     ] API Key authentication is disabled [src.core.app.middleware_config]
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:352 2025-08-17T14:55:05.294207Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openai: {'api_key': [], 'api_url': None, 'models': [], 'timeout': 120, 'extra': {}}
ERROR    src.core.services.response_processor:response_processor.py:199 Error in stream processing: 'async for' requires an object with __aiter__ method, got StreamingResponse
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\response_processor.py", line 137, in _process_stream
    async for chunk in response_iterator:
TypeError: 'async for' requires an object with __aiter__ method, got StreamingResponse
__________________ test_command_only_request_direct_response __________________
tests\unit\chat_completions_tests\test_command_only_requests.py:33: in test_command_only_request_direct_response
    assert session.state.override_model == "command-only-model"
E   AssertionError: assert None == 'command-only-model'
E    +  where None = <src.core.domain.session.SessionStateAdapter object at 0x0000013984DA9F90>.override_model
E    +    where <src.core.domain.session.SessionStateAdapter object at 0x0000013984DA9F90> = <src.core.domain.session.Session object at 0x0000013984DA8280>.state
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
___________________ test_command_plus_text_direct_response ____________________
tests\unit\chat_completions_tests\test_command_only_requests.py:70: in test_command_plus_text_direct_response
    assert response.status_code == 200
E   assert 500 == 200
E    +  where 500 = <Response [500 Internal Server Error]>.status_code
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 91, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 107, in _handle_non_streaming_response
    raise HTTPException(status_code=response.status_code, detail=err)
fastapi.exceptions.HTTPException: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
_______________ test_command_with_agent_prefix_direct_response ________________
tests\unit\chat_completions_tests\test_command_only_requests.py:116: in test_command_with_agent_prefix_direct_response
    assert response.status_code == 200
E   assert 500 == 200
E    +  where 500 = <Response [500 Internal Server Error]>.status_code
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 91, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 107, in _handle_non_streaming_response
    raise HTTPException(status_code=response.status_code, detail=err)
fastapi.exceptions.HTTPException: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
___________ test_command_only_request_direct_response_explicit_mock ___________
tests\unit\chat_completions_tests\test_command_only_requests.py:175: in test_command_only_request_direct_response_explicit_mock
    assert session.state.override_model == model_to_set
E   AssertionError: assert None == 'm2'
E    +  where None = <src.core.domain.session.SessionStateAdapter object at 0x0000013984F48B20>.override_model
E    +    where <src.core.domain.session.SessionStateAdapter object at 0x0000013984F48B20> = <src.core.domain.session.Session object at 0x0000013984AB1C00>.state
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
____________________ test_hello_command_with_agent_prefix _____________________
tests\unit\chat_completions_tests\test_command_only_requests.py:192: in test_hello_command_with_agent_prefix
    assert response.status_code == 200
E   assert 500 == 200
E    +  where 500 = <Response [500 Internal Server Error]>.status_code
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 91, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 107, in _handle_non_streaming_response
    raise HTTPException(status_code=response.status_code, detail=err)
fastapi.exceptions.HTTPException: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
_____________________ test_hello_command_followed_by_text _____________________
tests\unit\chat_completions_tests\test_command_only_requests.py:222: in test_hello_command_followed_by_text
    assert "Functional backends:" in content
E   AssertionError: assert 'Functional backends:' in 'Hello, this is llm-interactive-proxy v0.1.0. hello acknowledged'
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
__________________ test_hello_command_with_prefix_and_suffix __________________
tests\unit\chat_completions_tests\test_command_only_requests.py:244: in test_hello_command_with_prefix_and_suffix
    assert response.status_code == 200
E   assert 500 == 200
E    +  where 500 = <Response [500 Internal Server Error]>.status_code
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 91, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 107, in _handle_non_streaming_response
    raise HTTPException(status_code=response.status_code, detail=err)
fastapi.exceptions.HTTPException: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
____________________________ test_commands_ignored ____________________________
tests\unit\chat_completions_tests\test_commands_disabled.py:20: in test_commands_ignored
    assert resp.status_code == 200
E   assert 500 == 200
E    +  where 500 = <Response [500 Internal Server Error]>.status_code
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 91, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 107, in _handle_non_streaming_response
    raise HTTPException(status_code=response.status_code, detail=err)
fastapi.exceptions.HTTPException: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
________ test_empty_messages_after_processing_no_commands_bad_request _________
tests\unit\chat_completions_tests\test_error_handling.py:27: in test_empty_messages_after_processing_no_commands_bad_request
    assert response.status_code == 400
E   assert 500 == 400
E    +  where 500 = <Response [500 Internal Server Error]>.status_code
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 91, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 107, in _handle_non_streaming_response
    raise HTTPException(status_code=response.status_code, detail=err)
fastapi.exceptions.HTTPException: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
___________________ test_get_openrouter_headers_no_api_key ____________________
tests\unit\chat_completions_tests\test_error_handling.py:49: in test_get_openrouter_headers_no_api_key
    "Simulated backend error due to bad headers" in response_json["detail"]["error"]
E   KeyError: 'detail'
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 91, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 107, in _handle_non_streaming_response
    raise HTTPException(status_code=response.status_code, detail=err)
fastapi.exceptions.HTTPException: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
______________________ test_invalid_model_noninteractive ______________________
tests\unit\chat_completions_tests\test_error_handling.py:62: in test_invalid_model_noninteractive
    assert "model openrouter:bad not available" in content
E   AssertionError: assert 'model openrouter:bad not available' in 'model set to openrouter:bad'
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
_________________________ test_failover_missing_keys __________________________
tests\unit\chat_completions_tests\test_failover.py:103: in test_failover_missing_keys
    assert resp.json()["detail"]["error"] == "all backends failed"
E   KeyError: 'detail'
---------------------------- Captured stdout call -----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Error fetching models: 500: API key is not set.
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T14:55:15.362315Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:315 2025-08-17T14:55:15.362315Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.app.middleware_config:middleware_config.py:45 2025-08-17T14:55:15.369813Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:352 2025-08-17T14:55:15.374813Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:369 2025-08-17T14:55:15.376818Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: backend
INFO     src.core.services.command_service:command_service.py:103 Registered command: model
INFO     src.core.services.command_service:command_service.py:103 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:103 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:103 Registered command: openai-url
INFO     src.core.services.command_service:command_service.py:103 Registered command: project
INFO     src.core.services.command_service:command_service.py:103 Registered command: set
INFO     src.core.services.command_service:command_service.py:103 Registered command: unset
INFO     src.core.services.command_service:command_service.py:103 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:103 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:103 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:103 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:103 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:281 Registered 27 command handlers with registry
INFO     src.core.app.application_factory:application_factory.py:417 2025-08-17T14:55:15.384313Z [info     ] Initializing legacy backends and state attributes... [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:442 2025-08-17T14:55:15.384313Z [info     ] Initialized legacy backend 'anthropic' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:462 2025-08-17T14:55:15.384313Z [info     ] Initialized legacy backend 'openrouter' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:481 2025-08-17T14:55:15.384813Z [info     ] Initialized legacy backend 'gemini' and attached to app.state [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:547 2025-08-17T14:55:15.384813Z [info     ] Initialized 3 functional backends: {'gemini', 'anthropic', 'openrouter'} [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:602 2025-08-17T14:55:15.384813Z [info     ] Registered tool call loop detection middleware [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:228 Executing command: create-failover-route with session: a5228e12-df13-4454-82fe-128360550b8d
INFO     src.core.services.command_service:command_service.py:228 Executing command: route-append with session: ade938e7-67a2-45f0-8388-bdf5072b9762
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openai: {'api_key': [], 'api_url': None, 'models': [], 'timeout': 120, 'extra': {}}
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 91, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 97, in _handle_non_streaming_response
    raise HTTPException(
fastapi.exceptions.HTTPException: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
INFO     src.core.app.application_factory:application_factory.py:389 2025-08-17T14:55:15.391313Z [info     ] Shutting down application      [src.core.app.application_factory]
____________________ test_first_reply_no_automatic_banner _____________________
tests\unit\chat_completions_tests\test_interactive_banner.py:16: in test_first_reply_no_automatic_banner
    assert resp.status_code == 200
E   assert 500 == 200
E    +  where 500 = <Response [500 Internal Server Error]>.status_code
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 91, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 107, in _handle_non_streaming_response
    raise HTTPException(status_code=response.status_code, detail=err)
fastapi.exceptions.HTTPException: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
______________________ test_hello_command_returns_banner ______________________
tests\unit\chat_completions_tests\test_interactive_banner.py:40: in test_hello_command_returns_banner
    project_name = interactive_client.app.state.app_config.name
.venv\lib\site-packages\pydantic\main.py:991: in __getattr__
    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
E   AttributeError: 'AppConfig' object has no attribute 'name'
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
_________________________ test_unknown_command_error __________________________
tests\unit\chat_completions_tests\test_interactive_commands.py:17: in test_unknown_command_error
    assert resp.status_code == 200
E   assert 500 == 200
E    +  where 500 = <Response [500 Internal Server Error]>.status_code
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 91, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 107, in _handle_non_streaming_response
    raise HTTPException(status_code=response.status_code, detail=err)
fastapi.exceptions.HTTPException: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
________________________ test_set_command_confirmation ________________________
tests\unit\chat_completions_tests\test_interactive_commands.py:51: in test_set_command_confirmation
    assert response.status_code == 200
E   assert 500 == 200
E    +  where 500 = <Response [500 Internal Server Error]>.status_code
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 91, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 107, in _handle_non_streaming_response
    raise HTTPException(status_code=response.status_code, detail=err)
fastapi.exceptions.HTTPException: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
________________________ test_set_backend_confirmation ________________________
tests\unit\chat_completions_tests\test_interactive_commands.py:90: in test_set_backend_confirmation
    assert resp.status_code == 200
E   assert 500 == 200
E    +  where 500 = <Response [500 Internal Server Error]>.status_code
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 91, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 107, in _handle_non_streaming_response
    raise HTTPException(status_code=response.status_code, detail=err)
fastapi.exceptions.HTTPException: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
_______________________ test_set_backend_nonfunctional ________________________
tests\unit\chat_completions_tests\test_interactive_commands.py:124: in test_set_backend_nonfunctional
    assert resp.status_code == 200
E   assert 500 == 200
E    +  where 500 = <Response [500 Internal Server Error]>.status_code
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
---------------------------- Captured stdout call -----------------------------
Error fetching models: No response can be found for GET request on https://api.openai.com/v1/models
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: No response can be found for POST request on https://api.openai.com/v1/chat/completions
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Backend call failed: No response can be found for POST request on https://api.openai.com/v1/chat/completions
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 91, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 101, in _handle_non_streaming_response
    response = await self.client.post(url, json=payload, headers=headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1859, in post
    return await self.request(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1540, in request
    return await self.send(request, auth=auth, follow_redirects=follow_redirects)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1629, in send
    response = await self._send_handling_auth(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pytest_httpx\__init__.py", line 56, in mocked_handle_async_request
    return await mock._handle_async_request(transport, request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pytest_httpx\_httpx_mock.py", line 183, in _handle_async_request
    self._request_not_matched(real_transport, request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pytest_httpx\_httpx_mock.py", line 191, in _request_not_matched
    raise httpx.TimeoutException(
httpx.TimeoutException: No response can be found for POST request on https://api.openai.com/v1/chat/completions

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: No response can be found for POST request on https://api.openai.com/v1/chat/completions
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: No response can be found for POST request on https://api.openai.com/v1/chat/completions
___________________________ test_set_redaction_flag ___________________________
.venv\lib\site-packages\starlette\datastructures.py:686: in __getattr__
    return self._state[key]
E   KeyError: 'api_key_redaction_enabled'

During handling of the above exception, another exception occurred:
tests\unit\chat_completions_tests\test_interactive_commands.py:155: in test_set_redaction_flag
    assert interactive_client.app.state.api_key_redaction_enabled is False
.venv\lib\site-packages\starlette\datastructures.py:689: in __getattr__
    raise AttributeError(message.format(self.__class__.__name__, key))
E   AttributeError: 'State' object has no attribute 'api_key_redaction_enabled'
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
__________________________ test_unset_redaction_flag __________________________
.venv\lib\site-packages\starlette\datastructures.py:686: in __getattr__
    return self._state[key]
E   KeyError: 'default_api_key_redaction_enabled'

During handling of the above exception, another exception occurred:
tests\unit\chat_completions_tests\test_interactive_commands.py:190: in test_unset_redaction_flag
    is interactive_client.app.state.default_api_key_redaction_enabled
.venv\lib\site-packages\starlette\datastructures.py:689: in __getattr__
    raise AttributeError(message.format(self.__class__.__name__, key))
E   AttributeError: 'State' object has no attribute 'default_api_key_redaction_enabled'
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
_____________________ test_set_model_command_integration ______________________
tests\unit\chat_completions_tests\test_model_commands.py:39: in test_set_model_command_integration
    assert response.status_code == 200
E   assert 500 == 200
E    +  where 500 = <Response [500 Internal Server Error]>.status_code
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 91, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 107, in _handle_non_streaming_response
    raise HTTPException(status_code=response.status_code, detail=err)
fastapi.exceptions.HTTPException: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
____________________ test_unset_model_command_integration _____________________
tests\unit\chat_completions_tests\test_model_commands.py:91: in test_unset_model_command_integration
    assert response.status_code == 200
E   assert 500 == 200
E    +  where 500 = <Response [500 Internal Server Error]>.status_code
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 91, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 107, in _handle_non_streaming_response
    raise HTTPException(status_code=response.status_code, detail=err)
fastapi.exceptions.HTTPException: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': "Incorrect API key provided: ['test_o*******ey']. You can find your API key at https://platform.openai.com/account/api-keys.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
______________ test_openai_frontend_to_gemini_backend_multimodal ______________
tests\unit\chat_completions_tests\test_multimodal_cross_protocol.py:93: in test_openai_frontend_to_gemini_backend_multimodal
    assert r.status_code == 200
E   assert 401 == 200
E    +  where 401 = <Response [401 Unauthorized]>.status_code
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T14:55:26.197224Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:315 2025-08-17T14:55:26.197724Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.app.middleware_config:middleware_config.py:45 2025-08-17T14:55:26.204723Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:352 2025-08-17T14:55:26.210227Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for POST /v1/chat/completions from client testclient
______________ test_gemini_frontend_to_openai_backend_multimodal ______________
tests\unit\chat_completions_tests\test_multimodal_cross_protocol.py:163: in test_gemini_frontend_to_openai_backend_multimodal
    assert r.status_code == 200
E   assert 401 == 200
E    +  where 401 = <Response [401 Unauthorized]>.status_code
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T14:55:26.221723Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:315 2025-08-17T14:55:26.221723Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:103 Registered command: hello
INFO     src.core.services.command_service:command_service.py:103 Registered command: help
INFO     src.core.services.command_service:command_service.py:103 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:103 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:103 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-prepend
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:103 Registered command: route-clear
INFO     src.core.app.middleware_config:middleware_config.py:45 2025-08-17T14:55:26.229724Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:352 2025-08-17T14:55:26.234723Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for POST /v1beta/models/openrouter:gpt-4:generateContent from client testclient
__________ test_oneoff_command[oneoff_with_prompt-request_payload0] ___________
tests\unit\chat_completions_tests\test_oneoff_command.py:110: in test_oneoff_command
    assert response.status_code == 200
E   assert 500 == 200
E    +  where 500 = <Response [500 Internal Server Error]>.status_code
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Failed to initialize backend openrouter
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 358, in _get_or_create_backend
    await self._factory.initialize_backend(backend, config)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_factory.py", line 75, in initialize_backend
    await backend.initialize(**config)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openrouter.py", line 48, in initialize
    raise TypeError(
TypeError: OpenRouterBackend requires 'openrouter_headers_provider' (Callable) and 'key_name' (str) in kwargs.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 93, in call_completion
    backend = await self._get_or_create_backend(backend_type)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 365, in _get_or_create_backend
    raise BackendError(
src.core.common.exceptions.BackendError: Failed to create backend openrouter: OpenRouterBackend requires 'openrouter_headers_provider' (Callable) and 'key_name' (str) in kwargs.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 95, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Failed to initialize backend openrouter
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Failed to initialize backend openrouter
_______ test_oneoff_command[oneoff_alias_with_prompt-request_payload1] ________
tests\unit\chat_completions_tests\test_oneoff_command.py:110: in test_oneoff_command
    assert response.status_code == 200
E   assert 500 == 200
E    +  where 500 = <Response [500 Internal Server Error]>.status_code
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Failed to initialize backend openrouter
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 358, in _get_or_create_backend
    await self._factory.initialize_backend(backend, config)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_factory.py", line 75, in initialize_backend
    await backend.initialize(**config)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openrouter.py", line 48, in initialize
    raise TypeError(
TypeError: OpenRouterBackend requires 'openrouter_headers_provider' (Callable) and 'key_name' (str) in kwargs.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 93, in call_completion
    backend = await self._get_or_create_backend(backend_type)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 365, in _get_or_create_backend
    raise BackendError(
src.core.common.exceptions.BackendError: Failed to create backend openrouter: OpenRouterBackend requires 'openrouter_headers_provider' (Callable) and 'key_name' (str) in kwargs.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 95, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Failed to initialize backend openrouter
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Failed to initialize backend openrouter
_________ test_oneoff_command[oneoff_without_prompt-request_payload2] _________
tests\unit\chat_completions_tests\test_oneoff_command.py:97: in test_oneoff_command
    assert response.status_code == 200
E   assert 500 == 200
E    +  where 500 = <Response [500 Internal Server Error]>.status_code
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
------------------------------ Captured log call ------------------------------
ERROR    src.core.services.request_processor:request_processor.py:240 Error calling backend: Failed to initialize backend openrouter
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 358, in _get_or_create_backend
    await self._factory.initialize_backend(backend, config)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_factory.py", line 75, in initialize_backend
    await backend.initialize(**config)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openrouter.py", line 48, in initialize
    raise TypeError(
TypeError: OpenRouterBackend requires 'openrouter_headers_provider' (Callable) and 'key_name' (str) in kwargs.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 93, in call_completion
    backend = await self._get_or_create_backend(backend_type)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 365, in _get_or_create_backend
    raise BackendError(
src.core.common.exceptions.BackendError: Failed to create backend openrouter: OpenRouterBackend requires 'openrouter_headers_provider' (Callable) and 'key_name' (str) in kwargs.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 196, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 95, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Failed to initialize backend openrouter
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Failed to initialize backend openrouter
____________________ test_set_project_command_integration _____________________
tests\unit\chat_completions_tests\test_project_commands.py:20: in test_set_project_command_integration
    session = await session_service.get_session("default")
E   TypeError: object Session can't be used in 'await' expression
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
___________________ test_unset_project_command_integration ____________________
tests\unit\chat_completions_tests\test_project_commands.py:27: in test_unset_project_command_integration
    session = await session_service.get_session("default")
E   TypeError: object Session can't be used in 'await' expression
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
___________________ test_set_project_name_alias_integration ___________________
tests\unit\chat_completions_tests\test_project_commands.py:60: in test_set_project_name_alias_integration
    session = await session_service.get_session("default")
E   TypeError: object Session can't be used in 'await' expression
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
__________________ test_unset_project_name_alias_integration __________________
tests\unit\chat_completions_tests\test_project_commands.py:67: in test_unset_project_name_alias_integration
    session = await session_service.get_session("default")
E   TypeError: object Session can't be used in 'await' expression
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
___________________ test_force_set_project_blocks_requests ____________________
tests\unit\chat_completions_tests\test_project_commands.py:92: in test_force_set_project_blocks_requests
    session = await session_service.get_session("default")
E   TypeError: object Session can't be used in 'await' expression
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
___________________ test_force_set_project_allows_after_set ___________________
tests\unit\chat_completions_tests\test_project_commands.py:131: in test_force_set_project_allows_after_set
    session = await session_service.get_session("default")
E   TypeError: object Session can't be used in 'await' expression
---------------------------- Captured stdout setup ----------------------------
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
ProjectCommandHandler initialized with name=project and aliases=['project-name']
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
Registering handler: project
Registering handler: reasoning-effort
Registering handler: thinking-budget
Registering handler: gemini-generation-config
Registering handler: openai-url
Registering handler: loop-detection
Registering handler: tool-loop-detection
Registering handler: tool-loop-max-repeats
Registering handler: tool-loop-ttl
Registering handler: tool-loop-mode
OpenAIURLHandler initialized with name=openai-url and aliases=['openai_url']
=========================== short test summary info ===========================
FAILED tests/integration/test_end_to_end_loop_detection.py::test_loop_detection_with_mocked_backend
FAILED tests/integration/test_end_to_end_loop_detection.py::test_loop_detection_in_streaming_response
FAILED tests/integration/test_loop_detection_integration.py::TestLoopDetectionIntegration::test_loop_detection_initialization_on_startup
FAILED tests/integration/test_loop_detection_integration.py::TestLoopDetectionIntegration::test_environment_variable_configuration
FAILED tests/integration/test_loop_detection_middleware.py::test_end_to_end_with_real_app
FAILED tests/integration/test_models_endpoints.py::TestModelsEndpoints::test_models_with_configured_backends
FAILED tests/integration/test_models_endpoints.py::TestModelsEndpoints::test_models_endpoint_error_handling
FAILED tests/integration/test_multiple_oneoff_commands.py::TestMultipleOneoffCommands::test_multiple_oneoff_commands_sequence
FAILED tests/integration/test_multiple_oneoff_commands.py::TestMultipleOneoffCommands::test_oneoff_commands_different_sessions
FAILED tests/integration/test_multiple_oneoff_commands.py::TestMultipleOneoffCommands::test_oneoff_command_with_prompt_in_same_message
FAILED tests/integration/test_oneoff_command_integration.py::test_oneoff_command_integration
FAILED tests/integration/test_phase1_integration.py::test_integration_bridge_initialization
FAILED tests/integration/test_updated_hybrid_controller.py::test_hybrid_chat_completions
FAILED tests/integration/test_updated_hybrid_controller.py::test_hybrid_anthropic_messages
FAILED tests/regression/test_chat_completion_regression.py::TestChatCompletionRegression::test_streaming_chat_completion
FAILED tests/unit/chat_completions_tests/test_command_only_requests.py::test_command_only_request_direct_response
FAILED tests/unit/chat_completions_tests/test_command_only_requests.py::test_command_plus_text_direct_response
FAILED tests/unit/chat_completions_tests/test_command_only_requests.py::test_command_with_agent_prefix_direct_response
FAILED tests/unit/chat_completions_tests/test_command_only_requests.py::test_command_only_request_direct_response_explicit_mock
FAILED tests/unit/chat_completions_tests/test_command_only_requests.py::test_hello_command_with_agent_prefix
FAILED tests/unit/chat_completions_tests/test_command_only_requests.py::test_hello_command_followed_by_text
FAILED tests/unit/chat_completions_tests/test_command_only_requests.py::test_hello_command_with_prefix_and_suffix
FAILED tests/unit/chat_completions_tests/test_commands_disabled.py::test_commands_ignored
FAILED tests/unit/chat_completions_tests/test_error_handling.py::test_empty_messages_after_processing_no_commands_bad_request
FAILED tests/unit/chat_completions_tests/test_error_handling.py::test_get_openrouter_headers_no_api_key
FAILED tests/unit/chat_completions_tests/test_error_handling.py::test_invalid_model_noninteractive
FAILED tests/unit/chat_completions_tests/test_failover.py::test_failover_missing_keys
FAILED tests/unit/chat_completions_tests/test_interactive_banner.py::test_first_reply_no_automatic_banner
FAILED tests/unit/chat_completions_tests/test_interactive_banner.py::test_hello_command_returns_banner
FAILED tests/unit/chat_completions_tests/test_interactive_commands.py::test_unknown_command_error
FAILED tests/unit/chat_completions_tests/test_interactive_commands.py::test_set_command_confirmation
FAILED tests/unit/chat_completions_tests/test_interactive_commands.py::test_set_backend_confirmation
FAILED tests/unit/chat_completions_tests/test_interactive_commands.py::test_set_backend_nonfunctional
FAILED tests/unit/chat_completions_tests/test_interactive_commands.py::test_set_redaction_flag
FAILED tests/unit/chat_completions_tests/test_interactive_commands.py::test_unset_redaction_flag
FAILED tests/unit/chat_completions_tests/test_model_commands.py::test_set_model_command_integration
FAILED tests/unit/chat_completions_tests/test_model_commands.py::test_unset_model_command_integration
FAILED tests/unit/chat_completions_tests/test_multimodal_cross_protocol.py::test_openai_frontend_to_gemini_backend_multimodal
FAILED tests/unit/chat_completions_tests/test_multimodal_cross_protocol.py::test_gemini_frontend_to_openai_backend_multimodal
FAILED tests/unit/chat_completions_tests/test_oneoff_command.py::test_oneoff_command[oneoff_with_prompt-request_payload0]
FAILED tests/unit/chat_completions_tests/test_oneoff_command.py::test_oneoff_command[oneoff_alias_with_prompt-request_payload1]
FAILED tests/unit/chat_completions_tests/test_oneoff_command.py::test_oneoff_command[oneoff_without_prompt-request_payload2]
FAILED tests/unit/chat_completions_tests/test_project_commands.py::test_set_project_command_integration
FAILED tests/unit/chat_completions_tests/test_project_commands.py::test_unset_project_command_integration
FAILED tests/unit/chat_completions_tests/test_project_commands.py::test_set_project_name_alias_integration
FAILED tests/unit/chat_completions_tests/test_project_commands.py::test_unset_project_name_alias_integration
FAILED tests/unit/chat_completions_tests/test_project_commands.py::test_force_set_project_blocks_requests
FAILED tests/unit/chat_completions_tests/test_project_commands.py::test_force_set_project_allows_after_set
ERROR tests/unit/chat_completions_tests/test_failover.py::test_failover_key_rotation
ERROR tests/unit/chat_completions_tests/test_interactive_commands.py::test_set_backend_nonfunctional
!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 50 failures !!!!!!!!!!!!!!!!!!!!!!!!!!
===== 48 failed, 76 passed, 9 skipped, 118 deselected, 2 errors in 32.02s =====
