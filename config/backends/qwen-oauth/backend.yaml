# Qwen OAuth Backend Configuration
# This file contains backend-specific configuration for qwen-oauth

# Backend settings
backend_type: "qwen-oauth"
timeout: 120

# Model capabilities and limits for this backend
models:
  "qwen3-coder-plus":
    capabilities:
      - "chat"
      - "completion"
      - "function_calling"
      - "tool_use"
      - "json_mode"
      - "streaming"
      - "reasoning"
      - "coding"
    
    limits:
      context_window: 262144  # 256K tokens total context window
      max_input_tokens: 200000  # ~200K input limit (leaves room for response)
      max_output_tokens: 62144  # ~60K output limit (reasonable for coding tasks)
      requests_per_minute: 60
      tokens_per_minute: 1000000
      max_temperature: 2.0
      min_temperature: 0.0
      max_top_p: 1.0
      min_top_p: 0.0
    
    metadata:
      display_name: "Qwen3 Coder Plus"
      description: "Qwen's advanced coding model with 256K context window"
      provider: "Qwen"
      model_type: "chat"
      architecture: "transformer"
      tags: ["coding", "reasoning", "function-calling", "large-context"]

  "qwen3-coder-flash":
    capabilities:
      - "chat"
      - "completion"
      - "function_calling"
      - "tool_use"
      - "json_mode"
      - "streaming"
      - "reasoning"
      - "coding"
    
    limits:
      context_window: 262144  # 256K tokens total context window
      max_input_tokens: 200000  # ~200K input limit
      max_output_tokens: 62144  # ~60K output limit
      requests_per_minute: 120
      tokens_per_minute: 2000000
      max_temperature: 2.0
      min_temperature: 0.0
      max_top_p: 1.0
      min_top_p: 0.0
    
    metadata:
      display_name: "Qwen3 Coder Flash"
      description: "Fast version of Qwen's coding model with 256K context window"
      provider: "Qwen"
      model_type: "chat"
      architecture: "transformer"
      tags: ["coding", "fast", "function-calling", "large-context"]

# Default model to use when no specific model is requested
default_model: "qwen3-coder-plus"

# Model-specific defaults
model_defaults:
  temperature: 0.7
  top_p: 1.0
  max_tokens: null  # Use context window limits instead