# file: C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py
# hypothesis_version: 6.138.2

['all backends failed', 'backend', 'backend_type', 'backends', 'error', 'extra_body', 'failover_routes', 'last_error', 'model', 'openai', 'session_id', 'stream']