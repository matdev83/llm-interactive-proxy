# file: C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\llm_accounting_utils.py
# hypothesis_version: 6.138.2

[0.0, '<streaming_response>', 'DISABLE_ACCOUNTING', 'USER', 'USERNAME', 'anthropic', 'app_name', 'backend', 'cached_tokens', 'candidatesTokenCount', 'choices', 'cl100k_base', 'completion', 'completion_tokens', 'content', 'cost', 'cost_details', 'days', 'error', 'false', 'gemini', 'gpt-3.5', 'gpt-3.5-turbo', 'gpt-4', 'id', 'input_tokens', 'log_type', 'message', 'model', 'model_rankings', 'note', 'openrouter', 'output_tokens', 'period_stats', 'project', 'promptTokenCount', 'prompt_text', 'prompt_tokens', 'provider', 'provider_info', 'quota_remaining', 'quota_reset', 'rate_limit_info', 'reasoning_tokens', 'remote_completion_id', 'request_id', 'requests_remaining', 'requests_reset', 'response_text', 'role', 'session', 'text', 'timestamp', 'tokens_remaining', 'tokens_reset', 'totalTokenCount', 'total_tokens', 'true', 'type', 'unknown', 'upstream_cost', 'usage', 'usageMetadata', 'user', 'user_name', 'x-goog-quota-reset', 'x-goog-request-id', 'x-or-model', 'x-or-provider']