# file: C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\app\controllers\__init__.py
# hypothesis_version: 6.138.2

[b'data: [DONE]\n\n', 500, 503, 8192, 32768, '/internal/health', '/v1/chat/completions', '/v1/messages', '/v1beta/models', '/v2/chat/completions', '001', '2+2', '2+2 equals 4.', 'ChatController_error', 'GPT-4 model', 'Gemini Pro model', 'I see an image.', 'STOP', 'Test response', '__name__', '_descriptors', 'candidates', 'candidatesTokenCount', 'choices', 'content', 'contents', 'description', 'descriptor_error', 'display_name', 'error', 'finishReason', 'gemini-pro', 'generateContent', 'gpt-4', 'image', 'image/unknown', 'index', 'inline_data', 'input_token_limit', 'llm.di.debug', 'message', 'messages', 'mime_type', 'model', 'models', 'models/gemini-pro', 'models/gpt-4', 'name', 'openrouter_backend', 'output_token_limit', 'parts', 'promptTokenCount', 'role', 'service_provider', 'stream', 'text', 'totalTokenCount', 'usageMetadata', 'user', 'version']