UPDATE: Categories 1 & 2 cleared. Actions taken: added mocked API key env vars to `tests/conftest.py` and ran QA checks. Moving on to Category 3 (Missing API Keys & Auth Failures).

PROGRESS (recent changes):
- **SessionStateAdapter compatibility**: completed — adapter now exposes expected properties/setters and supports in-place mutation used by tests.
- **Command DI & registration**: completed — domain `SetCommand` / `UnsetCommand` now registered into `CommandRegistry`, and project-dir handler wired for `set`.
- **Unset handler fixes**: completed — aliases and success reporting adjusted for `project-dir` unsetting.
- **Backend auth propagation**: in progress/completed for common cases — application factory now seeds test API keys for legacy backends and OpenRouter/Gemini config wiring improved.
- **Temperature validation & command parsing**: completed — `set temperature` validation now enforced and command parsing trims quotes.

NEXT STEPS FOR CATEGORY 3 (UPDATED):
- **Add/verify mock API keys in test fixtures** (some keys added in `tests/conftest.py`, verify other tests use numbered keys like `*_API_KEY_1`).
- **Ensure BackendService provides correct auth headers** (verify `openrouter` header provider and that `headers_override`/`api_key` are passed to connector init and runtime calls).
- **Run failing test subsets** to identify remaining auth-related failures and add targeted mocks.

TODO: IMMEDIATE NEXT STEPS (PRIORITISED, UPDATED)

- **1) Fix session history recording for streaming responses (high)**
  - Ensure `SessionInteraction` objects are recorded both for proxy-processed commands and backend responses.
  - Streaming responses should record a `<streaming>` placeholder and not lose the original prompt/handler metadata.

- **2) Re-run proxy/command unit tests (smoke) (done partially)**
  - Target: `tests/unit/proxy_logic_tests/*`, `tests/unit/chat_completions_tests/*` (command-related tests).
  - Confirm adapter and DI fixes reduce the large failure cluster.

- **3) Harden command service and request processor (in progress)**
  - Ensure `CommandService.process_commands` produces correct `modified_messages` (commands consume content when appropriate) and that command results persist to session.
  - Fix edge cases where commands return `new_state` but `success` may be False — persist when `new_state` present.

- **4) Align application factory model discovery/default backend selection (done partially)**
  - Tests expecting `openrouter`/`gemini` selection should pass when keys/mocks exist. Continue adding unit tests for backend auto-selection behavior.

- **5) HTTP mock & connector auth fixes (ongoing)**
  - Ensure `OpenRouterBackend` computes and passes `Authorization` header consistently (both streaming and non-streaming paths).
  - Verify connectors use the same URLs mocked in tests and propagate `headers_override`.

- **6) Streaming response handling (medium)**
  - Ensure streaming responses implement/return async iterators and are consumed correctly by response processing.

- **7) Cleanups & final test run**
  - Remove temporary debug prints, run full test suite, triage remaining failures, and create small, focused fixes.

STATUS SUMMARY (what I will do next):
- Re-run `tests/unit/chat_completions_tests/test_project_dir_commands.py` and iterate until all API-based project-dir tests return 200.
- After those pass, run a broader subset of command-related unit tests and file a short list of remaining failures for priority fixes.

ADDED PRIORITY ACTIONS:

- **A) Fix asyncio.run() misuse in tests (highest immediate priority)**
  - Remove `asyncio.run()` calls from within `@pytest.mark.asyncio` tests (examples: `tests/integration/test_end_to_end_loop_detection.py`, `tests/integration/test_failover_routes.py`).
  - Rely on pytest-asyncio's event loop management (use `async def` tests or mark appropriately) to avoid "RuntimeError: asyncio.run() cannot be called from a running event loop".

- **B) Ensure config objects (not dicts) are passed through DI and app factory**
  - Audit `src/core/app/application_factory.py` and `src/core/config/config_loader.py` to guarantee `AppConfig` (and nested configs like `backends`) are instances with attributes, not raw dicts.
  - Add explicit conversions/wrappers where tests or legacy code expect attribute access (e.g., `config.backends.openai.api_key`).

- **C) Re-initialize middleware & processors on startup**
  - Verify `src/response_middleware.py` and `ApplicationBuilder` lifecycle hooks in `src/core/app/application_factory.py` correctly register loop-detection and other processors so `middleware.middleware_stack` is non-empty after startup.

- **D) Fix SessionStateAdapter / SessionState usage across handlers**
  - Ensure `src/core/domain/session.py` exposes and mutates session fields consistently and that command handlers in `src/core/commands/handlers/` use the adapter API (e.g., `with_interactive_just_enabled`, `with_project`, `with_override_backend`) to update state.
  - Add/adjust unit tests or small adapters where necessary to maintain compatibility with existing tests expecting attribute-style access.

- **E) Address AppConfig enum/access issues**
  - Investigate `AppConfig.LogLevel` access errors (e.g., in `tests/unit/core/test_config.py`) and ensure enum definitions or access paths match test expectations.

- **F) Smoke test & iterate**
  - After the above edits, run focused test subsets: start with the integration files and unit tests listed in the prioritized failure summary (e.g., `tests/integration/test_end_to_end_loop_detection.py`, `tests/unit/*chat_completions_tests*`).
  - Triage and fix follow-up failures (likely smaller scope: backend mocking, response parsing).

Based on my analysis of the test failures, here are the:

PRIORITIZED FAILURE CATEGORIES

1) Backend Initialization & DI Binding Issues | ImpactScore=0.85 | Confidence=high
   Root cause (concise): OpenRouterBackend missing required constructor args (openrouter_headers_provider, key_name) in DI setup
   Affected dependent code (calling → called):
◦  src/core/services/backend_service.py:BackendService._create_backend:160
◦  src/connectors/openrouter.py:OpenRouterBackend.init:52
◦  src/core/app/application_factory.py:build_application:473
◦  tests/unit/chat_completions_tests/test_oneoff_command.py:test_oneoff_command:97
◦  tests/unit/chat_completions_tests/test_interactive_commands.py:test_set_backend_confirmation:64
   Estimated tests fixed if this category is resolved: 25
   Evidence/derivation:
◦  Tests currently failing due to this category (IDs/patterns): test_oneoff_command*, test_multiple_oneoff_, test_commands.py::test*
◦  Why they pass after fix: Backend will initialize correctly, allowing command processing to work
   Proposed fix strategy (architecturally aligned):
◦  Update BackendService's _create_backend to provide required kwargs for OpenRouterBackend
◦  Ensure DI container properly configures backend dependencies
◦  Add factory method or builder pattern for backend creation
   Risks/Mitigations:
◦  Risk: May break existing backend creation logic
◦  Mitigation: Add comprehensive tests for backend factory

2) Session State Type Mismatch | ImpactScore=0.72 | Confidence=high
   Root cause (concise): Tests passing ISessionState where SessionStateAdapter expected after SOLID refactor
   Affected dependent code (calling → called):
◦  tests/unit/proxy_logic_tests/test_process_text_for_commands.py:multiple_tests:370
◦  src/command_parser.py:CommandParserConfig.init:76
◦  src/core/domain/session.py:SessionStateAdapter:85
◦  tests/unit/proxy_logic_tests/test_process_commands_in_messages.py:test_set_command_prefix_variants:388
   Estimated tests fixed if this category is resolved: 18
   Evidence/derivation:
◦  Tests currently failing due to this category (IDs/patterns): test_process_text_for_commands.py::test_set_*
◦  Why they pass after fix: Type expectations will align between test and production code
   Proposed fix strategy (architecturally aligned):
◦  Update test fixtures to use SessionStateAdapter wrapper
◦  Or modify CommandParserConfig to accept ISessionState interface
◦  Ensure consistent type usage across codebase
   Risks/Mitigations:
◦  Risk: May require changes to many test files
◦  Mitigation: Create helper function for test state creation

3) Missing API Keys & Auth Failures | ImpactScore=0.68 | Confidence=high
   Root cause (concise): Tests not properly mocking API keys causing 401 auth errors
   Affected dependent code (calling → called):
◦  src/core/services/backend_service.py:BackendService.call_completion:289
◦  src/connectors/openai.py:OpenAIConnector.chat_completions:93
◦  tests/unit/chat_completions_tests/test_multimodal_cross_protocol.py:test_openai_frontend_to_gemini_backend:93
   Estimated tests fixed if this category is resolved: 15
   Evidence/derivation:
◦  Tests currently failing due to this category (IDs/patterns): test_multimodal_*, test_failover_missing_keys
◦  Why they pass after fix: Proper auth will allow backend calls to succeed
   Proposed fix strategy (architecturally aligned):
◦  Add proper mock API keys in test fixtures
◦  Ensure BackendService provides correct auth headers
◦  Update backend configurations in tests
   Risks/Mitigations:
◦  Risk: May expose sensitive key patterns
◦  Mitigation: Use clearly fake test keys

4) Command Handler Registration Issues | ImpactScore=0.62 | Confidence=medium
   Root cause (concise): Project/model commands not updating session state correctly
   Affected dependent code (calling → called):
◦  src/core/services/command_service.py:CommandService.execute_command:228
◦  src/core/commands/handler_factory.py:CommandHandlerFactory._handle_command:199
◦  tests/unit/chat_completions_tests/test_project_commands.py:test_set_project_command:21
   Estimated tests fixed if this category is resolved: 12
   Evidence/derivation:
◦  Tests currently failing due to this category (IDs/patterns): test_project_commands.py::test_, test_model_commands.py::test_
◦  Why they pass after fix: Commands will properly update session state
   Proposed fix strategy (architecturally aligned):
◦  Fix command handlers to return correct new_state
◦  Ensure session repository properly updates
◦  Add state transition validation
   Risks/Mitigations:
◦  Risk: May affect command processing flow
◦  Mitigation: Add integration tests for command flow

5) HTTP Mock Configuration | ImpactScore=0.55 | Confidence=medium
   Root cause (concise): httpx mock responses not matching actual request patterns
   Affected dependent code (calling → called):
◦  tests/integration/test_failover_routes.py:test_failover_route_commands:66
◦  pytest_httpx._httpx_mock.py:_handle_async_request:183
   Estimated tests fixed if this category is resolved: 8
   Evidence/derivation:
◦  Tests currently failing due to this category (IDs/patterns): ERROR at teardown messages
◦  Why they pass after fix: Mock expectations will align with actual requests
   Proposed fix strategy (architecturally aligned):
◦  Review and fix httpx_mock configurations
◦  Ensure mock URLs match actual backend URLs
◦  Add non_assert_warnings for unused mocks where appropriate
   Risks/Mitigations:
◦  Risk: May hide real request issues
◦  Mitigation: Add request logging in tests

6) Streaming Response Handling | ImpactScore=0.48 | Confidence=medium
   Root cause (concise): Streaming responses not being properly async iterated
   Affected dependent code (calling → called):
◦  tests/regression/test_chat_completion_regression.py:test_streaming_chat_completion:172
◦  src/core/services/backend_service.py:BackendService.call_completion:308
   Estimated tests fixed if this category is resolved: 5
   Evidence/derivation:
◦  Tests currently failing due to this category (IDs/patterns): test_streaming_, test_streaming*
◦  Why they pass after fix: Streaming will be handled with proper async iteration
   Proposed fix strategy (architecturally aligned):
◦  Fix async iteration in streaming handlers
◦  Ensure StreamingResponse objects are properly awaited
◦  Add streaming response type checking
   Risks/Mitigations:
◦  Risk: May affect streaming performance
◦  Mitigation: Add streaming benchmarks

