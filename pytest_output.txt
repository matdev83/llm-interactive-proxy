============================= test session starts =============================
platform win32 -- Python 3.10.11, pytest-8.4.1, pluggy-1.6.0 -- C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\Scripts\python.exe
cachedir: .pytest_cache
rootdir: C:\Users\Mateusz\source\repos\llm-interactive-proxy
configfile: pyproject.toml
plugins: anyio-4.10.0, asyncio-1.1.0, cov-6.2.1, httpx-0.35.0, respx-0.22.0
asyncio: mode=auto, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 876 items / 79 deselected / 797 selected

tests/chat_completions_tests/test_anthropic_api_compatibility.py::test_anthropic_messages_non_streaming ERROR [  0%]
tests/chat_completions_tests/test_anthropic_api_compatibility.py::test_anthropic_messages_with_tool_use_from_openai_tool_calls ERROR [  0%]
tests/chat_completions_tests/test_anthropic_frontend.py::test_anthropic_messages_non_streaming_frontend ERROR [  0%]
tests/chat_completions_tests/test_anthropic_frontend.py::test_anthropic_messages_streaming_frontend ERROR [  0%]
tests/chat_completions_tests/test_anthropic_frontend.py::test_anthropic_messages_auth_failure ERROR [  0%]
tests/chat_completions_tests/test_anthropic_frontend.py::test_models_endpoint_includes_anthropic ERROR [  0%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_anthropic_sdk_client_creation PASSED [  0%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_async_anthropic_sdk_client_creation PASSED [  1%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_models_endpoint_via_http FAILED [  1%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_messages_endpoint_validation_via_http FAILED [  1%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_messages_endpoint_with_system_message FAILED [  1%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_messages_endpoint_streaming_request FAILED [  1%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_messages_endpoint_with_stop_sequences FAILED [  1%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_conversation_flow_via_http FAILED [  1%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_error_handling_invalid_model FAILED [  1%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_error_handling_missing_required_fields FAILED [  2%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_parameter_validation_ranges FAILED [  2%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_health_and_info_endpoints FAILED [  2%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_anthropic_sdk_models_call_mock PASSED [  2%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_concurrent_requests FAILED [  2%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_large_payload_handling FAILED [  2%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_unicode_and_special_characters FAILED [  2%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_content_type_headers FAILED [  2%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_anthropic_specific_model_names FAILED [  3%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_endpoint_not_found FAILED [  3%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_method_not_allowed FAILED [  3%]
tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendWithoutSDK::test_endpoints_work_without_sdk SKIPPED [  3%]
tests/integration/test_app.py::test_chat_completions_endpoint PASSED     [  3%]
tests/integration/test_app.py::test_streaming_chat_completions_endpoint PASSED [  3%]
tests/integration/test_app.py::test_command_processing PASSED            [  3%]
tests/integration/test_cline_tool_call_implementation.py::TestClineCommandResponses::test_cline_hello_command_returns_tool_calls FAILED [  3%]
tests/integration/test_cline_tool_call_implementation.py::TestClineCommandResponses::test_cline_set_command_returns_tool_calls FAILED [  4%]
tests/integration/test_cline_tool_call_implementation.py::TestClineBackendResponses::test_xml_content_converted_to_tool_calls PASSED [  4%]
tests/integration/test_cline_tool_call_implementation.py::TestClineBackendResponses::test_backend_response_transformation_logic PASSED [  4%]
tests/integration/test_cline_tool_call_implementation.py::TestNonClineAgents::test_non_cline_agents_receive_regular_content PASSED [  4%]
tests/integration/test_cline_tool_call_implementation.py::TestNonClineAgents::test_xml_content_not_converted_for_non_cline PASSED [  4%]
tests/integration/test_cline_tool_call_implementation.py::TestFrontendAgnostic::test_openai_frontend_detection PASSED [  4%]
tests/integration/test_cline_tool_call_implementation.py::TestFrontendAgnostic::test_anthropic_frontend_detection PASSED [  4%]
tests/integration/test_cline_tool_call_implementation.py::TestFrontendAgnostic::test_gemini_frontend_detection PASSED [  4%]
tests/integration/test_cline_tool_call_implementation.py::TestToolCallStructure::test_tool_call_format_compliance PASSED [  5%]
tests/integration/test_cline_tool_call_implementation.py::TestToolCallStructure::test_tool_call_id_uniqueness PASSED [  5%]
tests/integration/test_end_to_end_loop_detection.py::test_loop_detection_with_mocked_backend FAILED [  5%]
tests/integration/test_end_to_end_loop_detection.py::test_loop_detection_in_streaming_response FAILED [  5%]
tests/integration/test_end_to_end_loop_detection.py::test_loop_detection_integration_with_middleware_chain PASSED [  5%]
tests/integration/test_end_to_end_loop_detection.py::test_request_processor_uses_response_processor SKIPPED [  5%]
tests/integration/test_failover_routes.py::test_failover_route_commands FAILED [  5%]
tests/integration/test_failover_routes.py::test_failover_service_routes PASSED [  5%]
tests/integration/test_failover_routes.py::test_backend_service_failover PASSED [  6%]
tests/integration/test_hello_command_integration.py::test_hello_command_integration PASSED [  6%]
tests/integration/test_loop_detection_integration.py::TestLoopDetectionIntegration::test_loop_detection_initialization_on_startup FAILED [  6%]
tests/integration/test_loop_detection_integration.py::TestLoopDetectionIntegration::test_loop_detection_disabled_on_startup PASSED [  6%]
tests/integration/test_loop_detection_integration.py::TestLoopDetectionIntegration::test_streaming_response_loop_detection PASSED [  6%]
tests/integration/test_loop_detection_integration.py::TestLoopDetectionIntegration::test_non_streaming_response_loop_detection PASSED [  6%]
tests/integration/test_loop_detection_integration.py::TestLoopDetectionIntegration::test_environment_variable_configuration FAILED [  6%]
tests/integration/test_loop_detection_integration.py::TestLoopDetectionIntegration::test_per_session_detector_management PASSED [  6%]
tests/integration/test_loop_detection_integration.py::TestLoopDetectionIntegration::test_whitelist_patterns_respected PASSED [  7%]
tests/integration/test_loop_detection_integration.py::TestLoopDetectionCommands::test_loop_detection_status_in_help PASSED [  7%]
tests/integration/test_loop_detection_middleware.py::test_loop_detection_middleware_no_loop PASSED [  7%]
tests/integration/test_loop_detection_middleware.py::test_loop_detection_middleware_with_loop PASSED [  7%]
tests/integration/test_loop_detection_middleware.py::test_full_processing_pipeline PASSED [  7%]
tests/integration/test_loop_detection_middleware.py::test_request_processor_integration PASSED [  7%]
tests/integration/test_loop_detection_middleware.py::test_end_to_end_with_real_app FAILED [  7%]
tests/integration/test_loop_detection_simple.py::TestLoopDetectionSimpleIntegration::test_middleware_configuration PASSED [  7%]
tests/integration/test_loop_detection_simple.py::TestLoopDetectionSimpleIntegration::test_middleware_disabled_configuration PASSED [  8%]
tests/integration/test_loop_detection_simple.py::TestLoopDetectionSimpleIntegration::test_environment_variable_configuration PASSED [  8%]
tests/integration/test_loop_detection_simple.py::TestLoopDetectionSimpleIntegration::test_detector_basic_functionality PASSED [  8%]
tests/integration/test_loop_detection_simple.py::TestLoopDetectionSimpleIntegration::test_whitelist_functionality PASSED [  8%]
tests/integration/test_loop_detection_simple.py::TestLoopDetectionSimpleIntegration::test_streaming_wrapper_basic PASSED [  8%]
tests/integration/test_loop_detection_simple.py::TestLoopDetectionSimpleIntegration::test_config_validation PASSED [  8%]
tests/integration/test_loop_detection_simple.py::TestLoopDetectionSimpleIntegration::test_pattern_detection_thresholds PASSED [  8%]
tests/integration/test_loop_detection_simple.py::TestLoopDetectionSimpleIntegration::test_app_startup_integration PASSED [  8%]
tests/integration/test_models_endpoints.py::TestModelsEndpoints::test_models_endpoint_no_auth PASSED [  9%]
tests/integration/test_models_endpoints.py::TestModelsEndpoints::test_v1_models_endpoint_no_auth PASSED [  9%]
tests/integration/test_models_endpoints.py::TestModelsEndpoints::test_models_endpoint_with_auth FAILED [  9%]
tests/integration/test_models_endpoints.py::TestModelsEndpoints::test_models_endpoint_invalid_auth PASSED [  9%]
tests/integration/test_models_endpoints.py::TestModelsEndpoints::test_models_with_configured_backends FAILED [  9%]
tests/integration/test_models_endpoints.py::TestModelsEndpoints::test_models_format_compliance PASSED [  9%]
tests/integration/test_models_endpoints.py::TestModelsEndpoints::test_models_endpoint_error_handling FAILED [  9%]
tests/integration/test_models_endpoints.py::TestModelsDiscovery::test_discover_openai_models PASSED [  9%]
tests/integration/test_models_endpoints.py::TestModelsDiscovery::test_discover_anthropic_models PASSED [ 10%]
tests/integration/test_models_endpoints.py::TestModelsDiscovery::test_discover_models_with_failover PASSED [ 10%]
tests/integration/test_models_endpoints.py::TestModelsEndpointIntegration::test_both_endpoints_return_same_data[/models] PASSED [ 10%]
tests/integration/test_models_endpoints.py::TestModelsEndpointIntegration::test_both_endpoints_return_same_data[/v1/models] PASSED [ 10%]
tests/integration/test_multiple_oneoff_commands.py::TestMultipleOneoffCommands::test_multiple_oneoff_commands_sequence ERROR [ 10%]
tests/integration/test_multiple_oneoff_commands.py::TestMultipleOneoffCommands::test_oneoff_commands_different_sessions ERROR [ 10%]
tests/integration/test_multiple_oneoff_commands.py::TestMultipleOneoffCommands::test_oneoff_command_with_prompt_in_same_message ERROR [ 10%]
tests/integration/test_oneoff_command_integration.py::test_oneoff_command_integration ERROR [ 10%]
tests/integration/test_phase1_integration.py::test_integration_bridge_initialization PASSED [ 11%]
tests/integration/test_phase1_integration.py::test_hybrid_endpoints_available PASSED [ 11%]
tests/integration/test_phase1_integration.py::test_legacy_endpoints_still_work PASSED [ 11%]
tests/integration/test_phase1_integration.py::test_feature_flags_environment PASSED [ 11%]
tests/integration/test_phase1_integration.py::test_integration_bridge_async_initialization FAILED [ 11%]
tests/integration/test_phase1_integration.py::test_adapter_creation PASSED [ 11%]
tests/integration/test_pwd_command_integration.py::test_pwd_command_integration_with_project_dir FAILED [ 11%]
tests/integration/test_pwd_command_integration.py::test_pwd_command_integration_without_project_dir FAILED [ 11%]
tests/integration/test_real_world_loop_detection.py::TestRealWorldLoopDetection::test_example1_kiro_loop_detection PASSED [ 12%]
tests/integration/test_real_world_loop_detection.py::TestRealWorldLoopDetection::test_example2_platinum_futures_loop_detection PASSED [ 12%]
tests/integration/test_real_world_loop_detection.py::TestRealWorldLoopDetection::test_example3_no_loop_false_positive_check PASSED [ 12%]
tests/integration/test_real_world_loop_detection.py::TestRealWorldLoopDetection::test_streaming_loop_detection_example1 PASSED [ 12%]
tests/integration/test_real_world_loop_detection.py::TestRealWorldLoopDetection::test_streaming_no_false_positive_example3 PASSED [ 12%]
tests/integration/test_real_world_loop_detection.py::TestRealWorldLoopDetection::test_unicode_character_counting PASSED [ 12%]
tests/integration/test_tool_call_loop_detection.py::TestToolCallLoopDetection::test_break_mode_blocks_repeated_tool_calls FAILED [ 12%]
tests/integration/test_tool_call_loop_detection.py::TestToolCallLoopDetection::test_chance_then_break_mode_transparent_retry_success FAILED [ 12%]
tests/integration/test_tool_call_loop_detection.py::TestToolCallLoopDetection::test_chance_then_break_mode_transparent_retry_fail FAILED [ 13%]
tests/integration/test_tool_call_loop_detection.py::TestToolCallLoopDetection::test_different_tool_calls_not_blocked FAILED [ 13%]
tests/integration/test_tool_call_loop_detection.py::TestToolCallLoopDetection::test_disabled_tool_call_loop_detection FAILED [ 13%]
tests/integration/test_tool_call_loop_detection.py::TestToolCallLoopDetection::test_session_override_takes_precedence SKIPPED [ 13%]
tests/integration/test_updated_hybrid_controller.py::test_get_service_provider_if_available PASSED [ 13%]
tests/integration/test_updated_hybrid_controller.py::test_get_service_provider_if_available_error PASSED [ 13%]
tests/integration/test_updated_hybrid_controller.py::test_hybrid_chat_completions FAILED [ 13%]
tests/integration/test_updated_hybrid_controller.py::test_hybrid_chat_completions_error_handling PASSED [ 13%]
tests/integration/test_updated_hybrid_controller.py::test_hybrid_anthropic_messages FAILED [ 14%]
tests/integration/test_updated_hybrid_controller.py::test_hybrid_anthropic_messages_error_handling PASSED [ 14%]
tests/integration/test_versioned_api.py::test_versioned_endpoint_exists PASSED [ 14%]
tests/integration/test_versioned_api.py::test_versioned_endpoint_with_backend_service PASSED [ 14%]
tests/integration/test_versioned_api.py::test_versioned_endpoint_with_commands FAILED [ 14%]
tests/integration/test_versioned_api.py::test_compatibility_endpoint PASSED [ 14%]
tests/regression/test_chat_completion_regression.py::TestChatCompletionRegression::test_basic_chat_completion PASSED [ 14%]
tests/regression/test_chat_completion_regression.py::TestChatCompletionRegression::test_streaming_chat_completion FAILED [ 14%]
tests/regression/test_chat_completion_regression.py::TestChatCompletionRegression::test_error_handling PASSED [ 15%]
tests/regression/test_chat_completion_regression.py::TestChatCompletionRegression::test_command_processing PASSED [ 15%]
tests/regression/test_mock_backend_regression.py::TestMockBackendRegression::test_legacy_chat_completion PASSED [ 15%]
tests/regression/test_mock_backend_regression.py::TestMockBackendRegression::test_new_chat_completion PASSED [ 15%]
tests/regression/test_mock_backend_regression.py::TestMockBackendRegression::test_legacy_streaming_chat_completion PASSED [ 15%]
tests/regression/test_mock_backend_regression.py::TestMockBackendRegression::test_new_streaming_chat_completion PASSED [ 15%]
tests/regression/test_mock_backend_regression.py::TestMockBackendRegression::test_legacy_tool_calling PASSED [ 15%]
tests/regression/test_mock_backend_regression.py::TestMockBackendRegression::test_new_tool_calling PASSED [ 15%]
tests/test_top_p_fix.py::test_top_p_fix_with_actual_request PASSED       [ 16%]
tests/unit/anthropic_frontend_tests/test_anthropic_accounting.py::TestAnthropicFrontendAccounting::test_extract_billing_info_from_headers_anthropic PASSED [ 16%]
tests/unit/anthropic_frontend_tests/test_anthropic_accounting.py::TestAnthropicFrontendAccounting::test_extract_billing_info_from_response_anthropic_dict PASSED [ 16%]
tests/unit/anthropic_frontend_tests/test_anthropic_accounting.py::TestAnthropicFrontendAccounting::test_extract_billing_info_from_response_anthropic_object PASSED [ 16%]
tests/unit/anthropic_frontend_tests/test_anthropic_accounting.py::TestAnthropicFrontendAccounting::test_extract_billing_info_from_response_anthropic_no_usage PASSED [ 16%]
tests/unit/anthropic_frontend_tests/test_anthropic_accounting.py::TestAnthropicFrontendAccounting::test_extract_anthropic_usage_from_dict PASSED [ 16%]
tests/unit/anthropic_frontend_tests/test_anthropic_accounting.py::TestAnthropicFrontendAccounting::test_extract_anthropic_usage_from_object PASSED [ 16%]
tests/unit/anthropic_frontend_tests/test_anthropic_accounting.py::TestAnthropicFrontendAccounting::test_extract_anthropic_usage_empty_response PASSED [ 16%]
tests/unit/anthropic_frontend_tests/test_anthropic_accounting.py::TestAnthropicFrontendAccounting::test_extract_anthropic_usage_invalid_response PASSED [ 17%]
tests/unit/anthropic_frontend_tests/test_anthropic_accounting.py::TestAnthropicFrontendAccounting::test_extract_anthropic_usage_partial_data PASSED [ 17%]
tests/unit/anthropic_frontend_tests/test_anthropic_accounting.py::TestAnthropicFrontendAccounting::test_billing_info_calls_extract_usage PASSED [ 17%]
tests/unit/anthropic_frontend_tests/test_anthropic_accounting.py::TestAnthropicFrontendAccounting::test_billing_info_structure_anthropic PASSED [ 17%]
tests/unit/anthropic_frontend_tests/test_anthropic_accounting.py::TestAnthropicFrontendAccounting::test_anthropic_vs_other_backends PASSED [ 17%]
tests/unit/anthropic_frontend_tests/test_anthropic_accounting.py::TestAnthropicFrontendAccounting::test_streaming_response_billing PASSED [ 17%]
tests/unit/anthropic_frontend_tests/test_anthropic_accounting.py::TestAnthropicFrontendAccounting::test_cost_calculation_placeholder PASSED [ 17%]
tests/unit/anthropic_frontend_tests/test_anthropic_converters.py::TestAnthropicConverters::test_anthropic_message_model PASSED [ 17%]
tests/unit/anthropic_frontend_tests/test_anthropic_converters.py::TestAnthropicConverters::test_anthropic_messages_request_model PASSED [ 18%]
tests/unit/anthropic_frontend_tests/test_anthropic_converters.py::TestAnthropicConverters::test_anthropic_to_openai_request_basic PASSED [ 18%]
tests/unit/anthropic_frontend_tests/test_anthropic_converters.py::TestAnthropicConverters::test_anthropic_to_openai_request_with_system PASSED [ 18%]
tests/unit/anthropic_frontend_tests/test_anthropic_converters.py::TestAnthropicConverters::test_anthropic_to_openai_request_with_parameters PASSED [ 18%]
tests/unit/anthropic_frontend_tests/test_anthropic_converters.py::TestAnthropicConverters::test_openai_to_anthropic_response_basic PASSED [ 18%]
tests/unit/anthropic_frontend_tests/test_anthropic_converters.py::TestAnthropicConverters::test_openai_stream_to_anthropic_stream_start PASSED [ 18%]
tests/unit/anthropic_frontend_tests/test_anthropic_converters.py::TestAnthropicConverters::test_openai_stream_to_anthropic_stream_content PASSED [ 18%]
tests/unit/anthropic_frontend_tests/test_anthropic_converters.py::TestAnthropicConverters::test_openai_stream_to_anthropic_stream_finish PASSED [ 18%]
tests/unit/anthropic_frontend_tests/test_anthropic_converters.py::TestAnthropicConverters::test_openai_stream_to_anthropic_stream_invalid PASSED [ 19%]
tests/unit/anthropic_frontend_tests/test_anthropic_converters.py::TestAnthropicConverters::test_map_finish_reason PASSED [ 19%]
tests/unit/anthropic_frontend_tests/test_anthropic_converters.py::TestAnthropicConverters::test_get_anthropic_models PASSED [ 19%]
tests/unit/anthropic_frontend_tests/test_anthropic_converters.py::TestAnthropicConverters::test_extract_anthropic_usage_dict PASSED [ 19%]
tests/unit/anthropic_frontend_tests/test_anthropic_converters.py::TestAnthropicConverters::test_extract_anthropic_usage_object PASSED [ 19%]
tests/unit/anthropic_frontend_tests/test_anthropic_converters.py::TestAnthropicConverters::test_extract_anthropic_usage_empty PASSED [ 19%]
tests/unit/anthropic_frontend_tests/test_anthropic_converters.py::TestAnthropicConverters::test_conversation_flow PASSED [ 19%]
tests/unit/anthropic_frontend_tests/test_anthropic_router.py::TestAnthropicRouter::test_router_prefix PASSED [ 19%]
tests/unit/anthropic_frontend_tests/test_anthropic_router.py::TestAnthropicRouter::test_health_endpoint PASSED [ 20%]
tests/unit/anthropic_frontend_tests/test_anthropic_router.py::TestAnthropicRouter::test_info_endpoint PASSED [ 20%]
tests/unit/anthropic_frontend_tests/test_anthropic_router.py::TestAnthropicRouter::test_models_endpoint PASSED [ 20%]
tests/unit/anthropic_frontend_tests/test_anthropic_router.py::TestAnthropicRouter::test_anthropic_models_function PASSED [ 20%]
tests/unit/anthropic_frontend_tests/test_anthropic_router.py::TestAnthropicRouter::test_messages_endpoint_not_implemented PASSED [ 20%]
tests/unit/anthropic_frontend_tests/test_anthropic_router.py::TestAnthropicRouter::test_anthropic_messages_function_validation PASSED [ 20%]
tests/unit/anthropic_frontend_tests/test_anthropic_router.py::TestAnthropicRouter::test_messages_endpoint_validation_errors PASSED [ 20%]
tests/unit/anthropic_frontend_tests/test_anthropic_router.py::TestAnthropicRouter::test_messages_endpoint_optional_parameters PASSED [ 20%]
tests/unit/anthropic_frontend_tests/test_anthropic_router.py::TestAnthropicRouter::test_messages_endpoint_streaming_request PASSED [ 21%]
tests/unit/anthropic_frontend_tests/test_anthropic_router.py::TestAnthropicRouter::test_invalid_endpoints PASSED [ 21%]
tests/unit/anthropic_frontend_tests/test_anthropic_router.py::TestAnthropicRouter::test_wrong_http_methods PASSED [ 21%]
tests/unit/anthropic_frontend_tests/test_anthropic_router.py::TestAnthropicRouter::test_large_request_handling PASSED [ 21%]
tests/unit/anthropic_frontend_tests/test_anthropic_router.py::TestAnthropicRouter::test_unicode_content_handling PASSED [ 21%]
tests/unit/anthropic_frontend_tests/test_anthropic_router.py::TestAnthropicRouter::test_edge_case_parameters PASSED [ 21%]
tests/unit/anthropic_frontend_tests/test_anthropic_router.py::TestAnthropicRouter::test_models_endpoint_error_handling PASSED [ 21%]
tests/unit/anthropic_frontend_tests/test_anthropic_router.py::TestAnthropicRouter::test_cors_headers PASSED [ 21%]
tests/unit/anthropic_frontend_tests/test_anthropic_router.py::TestAnthropicRouter::test_content_type_headers PASSED [ 22%]
tests/unit/anthropic_frontend_tests/test_anthropic_router.py::TestAnthropicRouter::test_router_tags_and_metadata PASSED [ 22%]
tests/unit/chat_completions_tests/test_agent_wrapping.py::test_cline_command_wrapping SKIPPED [ 22%]
tests/unit/chat_completions_tests/test_backend_commands.py::test_set_backend_command_integration SKIPPED [ 22%]
tests/unit/chat_completions_tests/test_backend_commands.py::test_unset_backend_command_integration SKIPPED [ 22%]
tests/unit/chat_completions_tests/test_backend_commands.py::test_set_backend_rejects_nonfunctional SKIPPED [ 22%]
tests/unit/chat_completions_tests/test_backend_commands.py::test_set_default_backend_command_integration SKIPPED [ 22%]
tests/unit/chat_completions_tests/test_backend_commands.py::test_unset_default_backend_command_integration SKIPPED [ 22%]
tests/unit/chat_completions_tests/test_basic_proxying.py::test_basic_request_proxying_non_streaming ERROR [ 23%]
tests/unit/chat_completions_tests/test_basic_proxying.py::test_basic_request_proxying_streaming ERROR [ 23%]
tests/unit/chat_completions_tests/test_cline_all_commands.py::test_cline_xml_wrapping_for_all_commands ERROR [ 23%]
tests/unit/chat_completions_tests/test_cline_all_commands.py::test_cline_xml_wrapping_error_commands ERROR [ 23%]
tests/unit/chat_completions_tests/test_cline_all_commands.py::test_cline_xml_wrapping_pure_error_commands ERROR [ 23%]
tests/unit/chat_completions_tests/test_cline_all_commands.py::test_non_cline_commands_no_xml_wrapping ERROR [ 23%]
tests/unit/chat_completions_tests/test_cline_hello_command.py::test_cline_hello_command_tool_calls ERROR [ 23%]
tests/unit/chat_completions_tests/test_cline_hello_command.py::test_cline_hello_command_same_request ERROR [ 23%]
tests/unit/chat_completions_tests/test_cline_hello_command.py::test_cline_hello_with_attempt_completion_only ERROR [ 24%]
tests/unit/chat_completions_tests/test_cline_hello_command.py::test_cline_hello_command_first_message ERROR [ 24%]
tests/unit/chat_completions_tests/test_cline_hello_command.py::test_non_cline_hello_command_no_xml_wrapping ERROR [ 24%]
tests/unit/chat_completions_tests/test_cline_isolation.py::test_non_cline_clients_no_xml_wrapping ERROR [ 24%]
tests/unit/chat_completions_tests/test_cline_isolation.py::test_remote_llm_responses_never_xml_wrapped ERROR [ 24%]
tests/unit/chat_completions_tests/test_cline_isolation.py::test_mixed_cline_command_and_llm_prompt ERROR [ 24%]
tests/unit/chat_completions_tests/test_cline_isolation.py::test_streaming_responses_never_xml_wrapped ERROR [ 24%]
tests/unit/chat_completions_tests/test_cline_isolation.py::test_error_responses_from_llm_never_xml_wrapped ERROR [ 24%]
tests/unit/chat_completions_tests/test_cline_output_format.py::test_cline_output_format_exact ERROR [ 25%]
tests/unit/chat_completions_tests/test_cline_output_format.py::test_cline_output_format_other_commands ERROR [ 25%]
tests/unit/chat_completions_tests/test_command_only_requests.py::test_command_only_request_direct_response ERROR [ 25%]
tests/unit/chat_completions_tests/test_command_only_requests.py::test_command_plus_text_direct_response ERROR [ 25%]
tests/unit/chat_completions_tests/test_command_only_requests.py::test_command_with_agent_prefix_direct_response ERROR [ 25%]
tests/unit/chat_completions_tests/test_command_only_requests.py::test_command_only_request_direct_response_explicit_mock ERROR [ 25%]
tests/unit/chat_completions_tests/test_command_only_requests.py::test_hello_command_with_agent_prefix ERROR [ 25%]
tests/unit/chat_completions_tests/test_command_only_requests.py::test_hello_command_followed_by_text ERROR [ 25%]
tests/unit/chat_completions_tests/test_command_only_requests.py::test_hello_command_with_prefix_and_suffix ERROR [ 26%]
tests/unit/chat_completions_tests/test_commands_disabled.py::test_commands_ignored ERROR [ 26%]
tests/unit/chat_completions_tests/test_error_handling.py::test_empty_messages_after_processing_no_commands_bad_request ERROR [ 26%]
tests/unit/chat_completions_tests/test_error_handling.py::test_get_openrouter_headers_no_api_key ERROR [ 26%]
tests/unit/chat_completions_tests/test_error_handling.py::test_invalid_model_noninteractive ERROR [ 26%]
tests/unit/chat_completions_tests/test_failover.py::test_failover_key_rotation PASSED [ 26%]
tests/unit/chat_completions_tests/test_failover.py::test_failover_key_rotation ERROR [ 26%]
tests/unit/chat_completions_tests/test_failover.py::test_failover_missing_keys FAILED [ 26%]
tests/unit/chat_completions_tests/test_help_command.py::test_help_list_commands ERROR [ 26%]
tests/unit/chat_completions_tests/test_help_command.py::test_help_specific_command ERROR [ 27%]
tests/unit/chat_completions_tests/test_interactive_banner.py::test_first_reply_no_automatic_banner ERROR [ 27%]
tests/unit/chat_completions_tests/test_interactive_banner.py::test_hello_command_returns_banner ERROR [ 27%]
tests/unit/chat_completions_tests/test_interactive_banner.py::test_hello_command_returns_xml_banner_for_cline_agent ERROR [ 27%]
tests/unit/chat_completions_tests/test_interactive_banner.py::test_set_command_returns_xml_for_cline_agent ERROR [ 27%]
tests/unit/chat_completions_tests/test_interactive_commands.py::test_unknown_command_error ERROR [ 27%]
tests/unit/chat_completions_tests/test_interactive_commands.py::test_set_command_confirmation ERROR [ 27%]
tests/unit/chat_completions_tests/test_interactive_commands.py::test_set_backend_confirmation ERROR [ 27%]
tests/unit/chat_completions_tests/test_interactive_commands.py::test_set_backend_nonfunctional ERROR [ 28%]
tests/unit/chat_completions_tests/test_interactive_commands.py::test_set_redaction_flag ERROR [ 28%]
tests/unit/chat_completions_tests/test_interactive_commands.py::test_unset_redaction_flag ERROR [ 28%]
tests/unit/chat_completions_tests/test_model_commands.py::test_set_model_command_integration ERROR [ 28%]
tests/unit/chat_completions_tests/test_model_commands.py::test_unset_model_command_integration ERROR [ 28%]
tests/unit/chat_completions_tests/test_multimodal_cross_protocol.py::test_openai_frontend_to_gemini_backend_multimodal FAILED [ 28%]
tests/unit/chat_completions_tests/test_multimodal_cross_protocol.py::test_gemini_frontend_to_openai_backend_multimodal FAILED [ 28%]
tests/unit/chat_completions_tests/test_oneoff_command.py::test_oneoff_command[oneoff_with_prompt-request_payload0] ERROR [ 28%]
tests/unit/chat_completions_tests/test_oneoff_command.py::test_oneoff_command[oneoff_alias_with_prompt-request_payload1] ERROR [ 29%]
tests/unit/chat_completions_tests/test_oneoff_command.py::test_oneoff_command[oneoff_without_prompt-request_payload2] ERROR [ 29%]
tests/unit/chat_completions_tests/test_project_commands.py::test_set_project_command_integration ERROR [ 29%]
tests/unit/chat_completions_tests/test_project_commands.py::test_unset_project_command_integration ERROR [ 29%]
tests/unit/chat_completions_tests/test_project_commands.py::test_set_project_name_alias_integration ERROR [ 29%]
tests/unit/chat_completions_tests/test_project_commands.py::test_unset_project_name_alias_integration ERROR [ 29%]
tests/unit/chat_completions_tests/test_project_commands.py::test_force_set_project_blocks_requests ERROR [ 29%]
tests/unit/chat_completions_tests/test_project_commands.py::test_force_set_project_allows_after_set ERROR [ 29%]
tests/unit/chat_completions_tests/test_project_dir_commands.py::test_set_project_dir_command_valid[project-dir] FAILED [ 30%]
tests/unit/chat_completions_tests/test_project_dir_commands.py::test_set_project_dir_command_valid[dir] FAILED [ 30%]
tests/unit/chat_completions_tests/test_project_dir_commands.py::test_set_project_dir_command_valid[project-directory] FAILED [ 30%]
tests/unit/chat_completions_tests/test_project_dir_commands.py::test_set_project_dir_command_invalid ERROR [ 30%]
tests/unit/chat_completions_tests/test_project_dir_commands.py::test_unset_project_dir_command[project-dir] ERROR [ 30%]
tests/unit/chat_completions_tests/test_project_dir_commands.py::test_unset_project_dir_command[dir] ERROR [ 30%]
tests/unit/chat_completions_tests/test_project_dir_commands.py::test_unset_project_dir_command[project-directory] ERROR [ 30%]
tests/unit/chat_completions_tests/test_pwd_command.py::test_pwd_command_with_project_dir_set ERROR [ 30%]
tests/unit/chat_completions_tests/test_pwd_command.py::test_pwd_command_without_project_dir_set ERROR [ 31%]
tests/unit/chat_completions_tests/test_rate_limit_wait.py::test_wait_for_rate_limited_backends ERROR [ 31%]
tests/unit/chat_completions_tests/test_rate_limiting.py::test_rate_limit_memory ERROR [ 31%]
tests/unit/chat_completions_tests/test_real_cline_response.py::test_real_cline_hello_response ERROR [ 31%]
tests/unit/chat_completions_tests/test_real_cline_response.py::test_cline_pure_hello_command ERROR [ 31%]
tests/unit/chat_completions_tests/test_real_cline_response.py::test_cline_no_session_id ERROR [ 31%]
tests/unit/chat_completions_tests/test_real_cline_response.py::test_cline_non_command_message ERROR [ 31%]
tests/unit/chat_completions_tests/test_real_cline_response.py::test_cline_first_message_hello ERROR [ 31%]
tests/unit/chat_completions_tests/test_real_cline_response.py::test_cline_first_message_with_detection ERROR [ 32%]
tests/unit/chat_completions_tests/test_real_cline_response.py::test_realistic_cline_hello_request ERROR [ 32%]
tests/unit/chat_completions_tests/test_session_history.py::test_session_records_proxy_and_backend_interactions ERROR [ 32%]
tests/unit/chat_completions_tests/test_session_history.py::test_session_records_streaming_placeholder ERROR [ 32%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_set_temperature_command_valid_float ERROR [ 32%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_set_temperature_command_valid_int ERROR [ 32%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_set_temperature_command_valid_string_number ERROR [ 32%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_set_temperature_command_zero_value ERROR [ 32%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_set_temperature_command_max_openai_value ERROR [ 33%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_set_temperature_command_invalid_negative ERROR [ 33%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_set_temperature_command_invalid_too_high ERROR [ 33%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_set_temperature_command_invalid_string ERROR [ 33%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_unset_temperature_command ERROR [ 33%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_temperature_persistence_across_requests ERROR [ 33%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_temperature_with_message_content ERROR [ 33%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureAPIParameters::test_direct_api_temperature_parameter ERROR [ 34%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureAPIParameters::test_api_temperature_overrides_session_setting ERROR [ 34%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureModelDefaults::test_temperature_model_defaults_applied ERROR [ 34%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureModelDefaults::test_session_temperature_overrides_model_defaults ERROR [ 34%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureProxyState::test_proxy_state_set_temperature_valid PASSED [ 34%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureProxyState::test_proxy_state_set_temperature_invalid FAILED [ 34%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureProxyState::test_proxy_state_unset_temperature PASSED [ 34%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureProxyState::test_proxy_state_reset_clears_temperature PASSED [ 34%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureProxyState::test_proxy_state_apply_model_defaults_temperature PASSED [ 35%]
tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureProxyState::test_proxy_state_apply_model_defaults_no_override PASSED [ 35%]
tests/unit/core/app/test_application_factory.py::TestCreateServiceProvider::test_create_service_provider_registers_all_required_services PASSED [ 35%]
tests/unit/core/app/test_application_factory.py::TestCreateServiceProvider::test_create_service_provider_handles_backend_registration PASSED [ 35%]
tests/unit/core/app/test_application_factory.py::TestBuildApp::test_build_app_loads_config PASSED [ 35%]
tests/unit/core/app/test_application_factory.py::TestBuildApp::test_build_app_creates_fastapi_app PASSED [ 35%]
tests/unit/core/app/test_application_factory.py::TestBuildApp::test_build_app_sets_up_app_state PASSED [ 35%]
tests/unit/core/app/test_application_factory.py::TestIntegration::test_app_handles_models_endpoint PASSED [ 35%]
tests/unit/core/app/test_application_factory.py::TestIntegration::test_app_dependency_injection_works PASSED [ 36%]
tests/unit/core/test_authentication.py::TestAPIKeyMiddleware::test_valid_bearer_key PASSED [ 36%]
tests/unit/core/test_authentication.py::TestAPIKeyMiddleware::test_valid_query_key PASSED [ 36%]
tests/unit/core/test_authentication.py::TestAPIKeyMiddleware::test_invalid_key PASSED [ 36%]
tests/unit/core/test_authentication.py::TestAPIKeyMiddleware::test_missing_key PASSED [ 36%]
tests/unit/core/test_authentication.py::TestAPIKeyMiddleware::test_bypass_path PASSED [ 36%]
tests/unit/core/test_authentication.py::TestAuthMiddleware::test_valid_token PASSED [ 36%]
tests/unit/core/test_authentication.py::TestAuthMiddleware::test_invalid_token PASSED [ 36%]
tests/unit/core/test_authentication.py::TestAuthMiddleware::test_missing_token PASSED [ 37%]
tests/unit/core/test_authentication.py::TestAuthMiddleware::test_bypass_path PASSED [ 37%]
tests/unit/core/test_authentication.py::TestIntegratedAuthentication::test_api_key_auth_valid PASSED [ 37%]
tests/unit/core/test_authentication.py::TestIntegratedAuthentication::test_api_key_auth_invalid PASSED [ 37%]
tests/unit/core/test_authentication.py::TestIntegratedAuthentication::test_api_key_auth_missing PASSED [ 37%]
tests/unit/core/test_authentication.py::TestIntegratedAuthentication::test_api_key_auth_query_param PASSED [ 37%]
tests/unit/core/test_authentication.py::TestIntegratedAuthentication::test_api_key_auth_bypass_path PASSED [ 37%]
tests/unit/core/test_authentication.py::TestIntegratedAuthentication::test_token_auth_valid PASSED [ 37%]
tests/unit/core/test_authentication.py::TestIntegratedAuthentication::test_token_auth_invalid PASSED [ 38%]
tests/unit/core/test_authentication.py::TestIntegratedAuthentication::test_token_auth_missing PASSED [ 38%]
tests/unit/core/test_authentication.py::TestIntegratedAuthentication::test_token_auth_bypass_path PASSED [ 38%]
tests/unit/core/test_authentication.py::TestIntegratedAuthentication::test_no_auth PASSED [ 38%]
tests/unit/core/test_authentication.py::TestAppIntegration::test_app_with_auth_disabled PASSED [ 38%]
tests/unit/core/test_authentication.py::TestAppIntegration::test_app_with_auth_enabled PASSED [ 38%]
tests/unit/core/test_authentication.py::TestAppIntegration::test_app_with_auth_token PASSED [ 38%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendFactory::test_create_backend PASSED [ 38%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendFactory::test_initialize_backend PASSED [ 39%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendFactory::test_create_backend_invalid_type PASSED [ 39%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceBasic::test_get_or_create_backend_cached PASSED [ 39%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceBasic::test_get_or_create_backend_new PASSED [ 39%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceBasic::test_get_or_create_backend_error PASSED [ 39%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceBasic::test_prepare_messages PASSED [ 39%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceCompletions::test_call_completion_basic PASSED [ 39%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceCompletions::test_call_completion_streaming PASSED [ 39%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceCompletions::test_call_completion_streaming_error FAILED [ 40%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceCompletions::test_call_completion_rate_limited PASSED [ 40%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceCompletions::test_call_completion_backend_error PASSED [ 40%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceCompletions::test_call_completion_invalid_response FAILED [ 40%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceCompletions::test_call_completion_invalid_streaming_response FAILED [ 40%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceValidation::test_validate_backend_and_model_valid PASSED [ 40%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceValidation::test_validate_backend_and_model_invalid_model PASSED [ 40%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceValidation::test_validate_backend_and_model_backend_error PASSED [ 40%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceFailover::test_simple_failover PASSED [ 41%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceFailover::test_complex_failover_first_attempt PASSED [ 41%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceFailover::test_complex_failover_second_attempt PASSED [ 41%]
tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceFailover::test_complex_failover_all_fail PASSED [ 41%]
tests/unit/core/test_command_service.py::test_backend_command PASSED     [ 41%]
tests/unit/core/test_command_service.py::test_model_command PASSED       [ 41%]
tests/unit/core/test_command_service.py::test_temperature_command PASSED [ 41%]
tests/unit/core/test_command_service.py::test_project_command PASSED     [ 41%]
tests/unit/core/test_command_service.py::test_help_command PASSED        [ 42%]
tests/unit/core/test_command_service.py::test_help_command_with_specific_command PASSED [ 42%]
tests/unit/core/test_command_service.py::test_unknown_command PASSED     [ 42%]
tests/unit/core/test_command_service.py::test_command_with_remaining_text PASSED [ 42%]
tests/unit/core/test_config.py::test_app_config_defaults PASSED          [ 42%]
tests/unit/core/test_config.py::test_app_config_validation PASSED        [ 42%]
tests/unit/core/test_config.py::test_app_config_to_legacy_config FAILED  [ 42%]
tests/unit/core/test_config.py::test_app_config_from_legacy_config FAILED [ 42%]
tests/unit/core/test_config.py::test_app_config_from_env FAILED          [ 43%]
tests/unit/core/test_config.py::test_load_config FAILED                  [ 43%]
tests/unit/core/test_configuration_interfaces.py::TestBackendConfigInterface::test_backend_config_implements_interface PASSED [ 43%]
tests/unit/core/test_configuration_interfaces.py::TestBackendConfigInterface::test_backend_config_with_methods PASSED [ 43%]
tests/unit/core/test_configuration_interfaces.py::TestBackendConfigInterface::test_backend_config_chaining PASSED [ 43%]
tests/unit/core/test_configuration_interfaces.py::TestReasoningConfigInterface::test_reasoning_config_implements_interface PASSED [ 43%]
tests/unit/core/test_configuration_interfaces.py::TestReasoningConfigInterface::test_reasoning_config_with_methods PASSED [ 43%]
tests/unit/core/test_configuration_interfaces.py::TestReasoningConfigInterface::test_reasoning_config_chaining PASSED [ 43%]
tests/unit/core/test_configuration_interfaces.py::TestLoopDetectionConfigInterface::test_loop_detection_config_implements_interface PASSED [ 44%]
tests/unit/core/test_configuration_interfaces.py::TestLoopDetectionConfigInterface::test_loop_detection_config_with_methods PASSED [ 44%]
tests/unit/core/test_configuration_interfaces.py::TestLoopDetectionConfigInterface::test_loop_detection_config_chaining PASSED [ 44%]
tests/unit/core/test_configuration_interfaces.py::TestConfigurationDefaults::test_backend_config_defaults PASSED [ 44%]
tests/unit/core/test_configuration_interfaces.py::TestConfigurationDefaults::test_reasoning_config_defaults PASSED [ 44%]
tests/unit/core/test_configuration_interfaces.py::TestConfigurationDefaults::test_loop_detection_config_defaults PASSED [ 44%]
tests/unit/core/test_configuration_interfaces.py::TestConfigurationImmutability::test_backend_config_immutability PASSED [ 44%]
tests/unit/core/test_configuration_interfaces.py::TestConfigurationImmutability::test_reasoning_config_immutability PASSED [ 44%]
tests/unit/core/test_configuration_interfaces.py::TestConfigurationImmutability::test_loop_detection_config_immutability PASSED [ 45%]
tests/unit/core/test_di_container.py::test_service_collection_singleton PASSED [ 45%]
tests/unit/core/test_di_container.py::test_service_collection_transient PASSED [ 45%]
tests/unit/core/test_di_container.py::test_service_collection_scoped PASSED [ 45%]
tests/unit/core/test_di_container.py::test_service_provider_get_required_service PASSED [ 45%]
tests/unit/core/test_di_container.py::test_service_factory PASSED        [ 45%]
tests/unit/core/test_di_container.py::test_service_with_dependency PASSED [ 45%]
tests/unit/core/test_domain_models.py::test_backend_config_immutability PASSED [ 45%]
tests/unit/core/test_domain_models.py::test_reasoning_config_immutability PASSED [ 46%]
tests/unit/core/test_domain_models.py::test_loop_detection_config_immutability PASSED [ 46%]
tests/unit/core/test_domain_models.py::test_session_state_immutability PASSED [ 46%]
tests/unit/core/test_domain_models.py::test_session_interaction_immutability PASSED [ 46%]
tests/unit/core/test_domain_models.py::test_session_mutability PASSED    [ 46%]
tests/unit/core/test_failover_service.py::test_get_failover_attempts PASSED [ 46%]
tests/unit/core/test_failover_service.py::test_get_failover_attempts_empty_route PASSED [ 46%]
tests/unit/core/test_failover_service.py::test_get_failover_attempts_invalid_element PASSED [ 46%]
tests/unit/core/test_hello_command.py::test_hello_handler_initialization PASSED [ 47%]
tests/unit/core/test_hello_command.py::test_hello_handler_can_handle PASSED [ 47%]
tests/unit/core/test_hello_command.py::test_hello_handler_execution PASSED [ 47%]
tests/unit/core/test_logging_utils.py::TestRedaction::test_redact PASSED [ 47%]
tests/unit/core/test_logging_utils.py::TestRedaction::test_redact_dict PASSED [ 47%]
tests/unit/core/test_logging_utils.py::TestRedaction::test_redact_text PASSED [ 47%]
tests/unit/core/test_logging_utils.py::TestLogging::test_get_logger PASSED [ 47%]
tests/unit/core/test_logging_utils.py::TestLogging::test_log_call PASSED [ 47%]
tests/unit/core/test_logging_utils.py::TestLogging::test_log_async_call PASSED [ 48%]
tests/unit/core/test_logging_utils.py::TestLogging::test_log_context PASSED [ 48%]
tests/unit/core/test_multimodal.py::TestContentPart::test_text_content_part PASSED [ 48%]
tests/unit/core/test_multimodal.py::TestContentPart::test_image_url_content_part PASSED [ 48%]
tests/unit/core/test_multimodal.py::TestContentPart::test_image_base64_content_part PASSED [ 48%]
tests/unit/core/test_multimodal.py::TestContentPart::test_to_dict PASSED [ 48%]
tests/unit/core/test_multimodal.py::TestContentPart::test_to_openai_format_text PASSED [ 48%]
tests/unit/core/test_multimodal.py::TestContentPart::test_to_openai_format_image_url PASSED [ 48%]
tests/unit/core/test_multimodal.py::TestContentPart::test_to_openai_format_image_base64 PASSED [ 49%]
tests/unit/core/test_multimodal.py::TestContentPart::test_to_anthropic_format_text PASSED [ 49%]
tests/unit/core/test_multimodal.py::TestContentPart::test_to_anthropic_format_image_url PASSED [ 49%]
tests/unit/core/test_multimodal.py::TestContentPart::test_to_anthropic_format_image_base64 PASSED [ 49%]
tests/unit/core/test_multimodal.py::TestContentPart::test_to_gemini_format_text PASSED [ 49%]
tests/unit/core/test_multimodal.py::TestContentPart::test_to_gemini_format_image_base64 PASSED [ 49%]
tests/unit/core/test_multimodal.py::TestMultimodalMessage::test_text_message PASSED [ 49%]
tests/unit/core/test_multimodal.py::TestMultimodalMessage::test_with_image_message PASSED [ 49%]
tests/unit/core/test_multimodal.py::TestMultimodalMessage::test_is_multimodal PASSED [ 50%]
tests/unit/core/test_multimodal.py::TestMultimodalMessage::test_get_text_content PASSED [ 50%]
tests/unit/core/test_multimodal.py::TestMultimodalMessage::test_to_dict PASSED [ 50%]
tests/unit/core/test_multimodal.py::TestMultimodalMessage::test_to_legacy_format PASSED [ 50%]
tests/unit/core/test_multimodal.py::TestMultimodalMessage::test_to_legacy_format_no_text PASSED [ 50%]
tests/unit/core/test_multimodal.py::TestMultimodalMessage::test_to_openai_format PASSED [ 50%]
tests/unit/core/test_multimodal.py::TestMultimodalMessage::test_to_anthropic_format PASSED [ 50%]
tests/unit/core/test_multimodal.py::TestMultimodalMessage::test_to_gemini_format PASSED [ 50%]
tests/unit/core/test_multimodal.py::TestMultimodalMessage::test_from_legacy_message PASSED [ 51%]
tests/unit/core/test_multimodal.py::TestMultimodalMessage::test_backend_format_selection PASSED [ 51%]
tests/unit/core/test_oneoff_command.py::test_oneoff_handler_initialization PASSED [ 51%]
tests/unit/core/test_oneoff_command.py::test_oneoff_handler_can_handle PASSED [ 51%]
tests/unit/core/test_oneoff_command.py::test_oneoff_handler_with_empty_value PASSED [ 51%]
tests/unit/core/test_oneoff_command.py::test_oneoff_handler_with_empty_dict PASSED [ 51%]
tests/unit/core/test_oneoff_command.py::test_oneoff_handler_with_invalid_format PASSED [ 51%]
tests/unit/core/test_oneoff_command.py::test_oneoff_handler_with_empty_backend PASSED [ 51%]
tests/unit/core/test_oneoff_command.py::test_oneoff_handler_with_empty_model PASSED [ 52%]
tests/unit/core/test_oneoff_command.py::test_oneoff_handler_with_valid_value_slash_format PASSED [ 52%]
tests/unit/core/test_oneoff_command.py::test_oneoff_handler_with_valid_value_colon_format PASSED [ 52%]
tests/unit/core/test_oneoff_command.py::test_oneoff_handler_with_valid_value_dict_format PASSED [ 52%]
tests/unit/core/test_pwd_command.py::test_pwd_handler_initialization PASSED [ 52%]
tests/unit/core/test_pwd_command.py::test_pwd_handler_can_handle PASSED  [ 52%]
tests/unit/core/test_pwd_command.py::test_pwd_handler_with_project_dir_set PASSED [ 52%]
tests/unit/core/test_pwd_command.py::test_pwd_handler_without_project_dir PASSED [ 52%]
tests/unit/core/test_request_processor.py::test_process_request_basic SKIPPED [ 53%]
tests/unit/core/test_request_processor.py::test_process_request_with_commands SKIPPED [ 53%]
tests/unit/core/test_request_processor.py::test_process_command_only_request SKIPPED [ 53%]
tests/unit/core/test_request_processor.py::test_process_streaming_request SKIPPED [ 53%]
tests/unit/core/test_request_processor.py::test_backend_error_handling SKIPPED [ 53%]
tests/unit/core/test_response_processor.py::test_response_processor_initialization PASSED [ 53%]
tests/unit/core/test_response_processor.py::test_process_response PASSED [ 53%]
tests/unit/core/test_response_processor.py::test_loop_detection PASSED   [ 53%]
tests/unit/core/test_response_processor.py::test_streaming_response_processing PASSED [ 54%]
tests/unit/core/test_response_processor.py::test_middleware_registration PASSED [ 54%]
tests/unit/core/test_session_service.py::test_session_creation PASSED    [ 54%]
tests/unit/core/test_session_service.py::test_session_retrieval PASSED   [ 54%]
tests/unit/core/test_session_service.py::test_session_update PASSED      [ 54%]
tests/unit/core/test_session_service.py::test_session_deletion PASSED    [ 54%]
tests/unit/core/test_session_service.py::test_get_all_sessions PASSED    [ 54%]
tests/unit/gemini_connector_tests/test_http_error_streaming.py::test_chat_completions_http_error_streaming PASSED [ 54%]
tests/unit/gemini_connector_tests/test_model_prefix_handling.py::test_chat_completions_model_prefix_handled PASSED [ 55%]
tests/unit/gemini_connector_tests/test_multimodal_payload.py::test_multimodal_data_url_converts_to_inline_data PASSED [ 55%]
tests/unit/gemini_connector_tests/test_multimodal_payload.py::test_multimodal_http_url_converts_to_file_data PASSED [ 55%]
tests/unit/gemini_connector_tests/test_part_conversion.py::test_text_part_type_removed PASSED [ 55%]
tests/unit/gemini_connector_tests/test_part_conversion.py::test_system_message_filtered PASSED [ 55%]
tests/unit/gemini_connector_tests/test_streaming_success.py::test_chat_completions_streaming_success PASSED [ 55%]
tests/unit/gemini_connector_tests/test_temperature_handling.py::TestGeminiTemperatureHandling::test_temperature_added_to_generation_config PASSED [ 55%]
tests/unit/gemini_connector_tests/test_temperature_handling.py::TestGeminiTemperatureHandling::test_temperature_clamping_above_one PASSED [ 55%]
tests/unit/gemini_connector_tests/test_temperature_handling.py::TestGeminiTemperatureHandling::test_temperature_zero_value PASSED [ 56%]
tests/unit/gemini_connector_tests/test_temperature_handling.py::TestGeminiTemperatureHandling::test_temperature_with_existing_generation_config PASSED [ 56%]
tests/unit/gemini_connector_tests/test_temperature_handling.py::TestGeminiTemperatureHandling::test_temperature_with_thinking_budget PASSED [ 56%]
tests/unit/gemini_connector_tests/test_temperature_handling.py::TestGeminiTemperatureHandling::test_no_temperature_no_generation_config PASSED [ 56%]
tests/unit/gemini_connector_tests/test_temperature_handling.py::TestGeminiTemperatureHandling::test_temperature_with_extra_params_override PASSED [ 56%]
tests/unit/gemini_connector_tests/test_temperature_handling.py::TestGeminiTemperatureHandling::test_temperature_streaming_request PASSED [ 56%]
tests/unit/loop_detection/test_detector.py::TestLoopDetector::test_detector_initialization PASSED [ 56%]
tests/unit/loop_detection/test_detector.py::TestLoopDetector::test_detector_disabled PASSED [ 56%]
tests/unit/loop_detection/test_detector.py::TestLoopDetector::test_simple_loop_detection PASSED [ 57%]
tests/unit/loop_detection/test_detector.py::TestLoopDetector::test_no_false_positive_normal_text PASSED [ 57%]
tests/unit/loop_detection/test_detector.py::TestLoopDetector::test_detector_reset PASSED [ 57%]
tests/unit/loop_detection/test_detector.py::TestLoopDetector::test_detector_enable_disable PASSED [ 57%]
tests/unit/loop_detection/test_detector.py::TestLoopDetector::test_detector_stats PASSED [ 57%]
tests/unit/loop_detection/test_detector.py::TestLoopDetector::test_minimum_content_threshold PASSED [ 57%]
tests/unit/loop_detection/test_detector.py::TestLoopDetector::test_config_validation PASSED [ 57%]
tests/unit/loop_detection/test_detector.py::TestLoopDetectionEvent::test_event_creation PASSED [ 57%]
tests/unit/loop_detection/test_streaming_wrapper.py::test_stream_cancellation_on_loop PASSED [ 58%]
tests/unit/loop_detection/test_tool_call_tracker.py::TestToolCallSignature::test_from_tool_call_valid_json PASSED [ 58%]
tests/unit/loop_detection/test_tool_call_tracker.py::TestToolCallSignature::test_from_tool_call_invalid_json PASSED [ 58%]
tests/unit/loop_detection/test_tool_call_tracker.py::TestToolCallSignature::test_get_full_signature PASSED [ 58%]
tests/unit/loop_detection/test_tool_call_tracker.py::TestToolCallSignature::test_is_expired PASSED [ 58%]
tests/unit/loop_detection/test_tool_call_tracker.py::TestToolCallTracker::test_init PASSED [ 58%]
tests/unit/loop_detection/test_tool_call_tracker.py::TestToolCallTracker::test_prune_expired_no_signatures PASSED [ 58%]
tests/unit/loop_detection/test_tool_call_tracker.py::TestToolCallTracker::test_prune_expired_with_expired PASSED [ 58%]
tests/unit/loop_detection/test_tool_call_tracker.py::TestToolCallTracker::test_track_tool_call_disabled PASSED [ 59%]
tests/unit/loop_detection/test_tool_call_tracker.py::TestToolCallTracker::test_track_tool_call_first_call PASSED [ 59%]
tests/unit/loop_detection/test_tool_call_tracker.py::TestToolCallTracker::test_track_tool_call_different_calls PASSED [ 59%]
tests/unit/loop_detection/test_tool_call_tracker.py::TestToolCallTracker::test_track_tool_call_repeated_below_threshold PASSED [ 59%]
tests/unit/loop_detection/test_tool_call_tracker.py::TestToolCallTracker::test_track_tool_call_repeated_at_threshold_break_mode PASSED [ 59%]
tests/unit/loop_detection/test_tool_call_tracker.py::TestToolCallTracker::test_track_tool_call_repeated_at_threshold_chance_mode PASSED [ 59%]
tests/unit/loop_detection/test_tool_call_tracker.py::TestToolCallTracker::test_track_tool_call_after_chance_different_call PASSED [ 59%]
tests/unit/loop_detection/test_tool_call_tracker.py::TestToolCallTracker::test_track_tool_call_after_chance_same_call PASSED [ 59%]
tests/unit/loop_detection/test_tool_call_tracker.py::TestToolCallTracker::test_track_tool_call_reset_after_different PASSED [ 60%]
tests/unit/loop_detection/test_tool_call_tracker.py::TestToolCallTracker::test_track_tool_call_with_ttl_expiry PASSED [ 60%]
tests/unit/openai_connector_tests/test_integration.py::test_set_openai_url_command FAILED [ 60%]
tests/unit/openai_connector_tests/test_url_override.py::test_chat_completions_uses_default_url PASSED [ 60%]
tests/unit/openai_connector_tests/test_url_override.py::test_chat_completions_uses_custom_url PASSED [ 60%]
tests/unit/openai_connector_tests/test_url_override.py::test_initialize_with_custom_url FAILED [ 60%]
tests/unit/openai_connector_tests/test_url_override.py::test_set_command_openai_url_integration FAILED [ 60%]
tests/unit/openrouter_connector_tests/test_headers_plumbing.py::test_headers_plumbing PASSED [ 60%]
tests/unit/openrouter_connector_tests/test_http_error_non_streaming.py::test_chat_completions_http_error_non_streaming PASSED [ 61%]
tests/unit/openrouter_connector_tests/test_http_error_streaming.py::test_chat_completions_http_error_streaming FAILED [ 61%]
tests/unit/openrouter_connector_tests/test_non_streaming_success.py::test_chat_completions_non_streaming_success PASSED [ 61%]
tests/unit/openrouter_connector_tests/test_payload_construction_and_headers.py::test_openrouter_headers_are_correct PASSED [ 61%]
tests/unit/openrouter_connector_tests/test_payload_construction_and_headers.py::test_openrouter_payload_basic_fields_and_model PASSED [ 61%]
tests/unit/openrouter_connector_tests/test_payload_construction_and_headers.py::test_openrouter_payload_message_count PASSED [ 61%]
tests/unit/openrouter_connector_tests/test_payload_construction_and_headers.py::test_openrouter_payload_first_message_structure PASSED [ 61%]
tests/unit/openrouter_connector_tests/test_payload_construction_and_headers.py::test_openrouter_payload_second_message_structure PASSED [ 61%]
tests/unit/openrouter_connector_tests/test_payload_construction_and_headers.py::test_openrouter_payload_third_message_multipart_structure PASSED [ 62%]
tests/unit/openrouter_connector_tests/test_payload_construction_and_headers.py::test_openrouter_payload_unset_fields_are_excluded PASSED [ 62%]
tests/unit/openrouter_connector_tests/test_payload_construction_and_headers.py::test_openrouter_original_request_data_unmodified PASSED [ 62%]
tests/unit/openrouter_connector_tests/test_payload_construction_and_headers.py::test_openrouter_processed_messages_remain_pydantic PASSED [ 62%]
tests/unit/openrouter_connector_tests/test_redaction.py::test_prompt_redaction PASSED [ 62%]
tests/unit/openrouter_connector_tests/test_request_error.py::test_chat_completions_request_error FAILED [ 62%]
tests/unit/openrouter_connector_tests/test_streaming_success.py::test_chat_completions_streaming_success PASSED [ 62%]
tests/unit/openrouter_connector_tests/test_temperature_handling.py::TestOpenRouterTemperatureHandling::test_temperature_added_to_payload PASSED [ 62%]
tests/unit/openrouter_connector_tests/test_temperature_handling.py::TestOpenRouterTemperatureHandling::test_temperature_zero_value PASSED [ 63%]
tests/unit/openrouter_connector_tests/test_temperature_handling.py::TestOpenRouterTemperatureHandling::test_temperature_max_value PASSED [ 63%]
tests/unit/openrouter_connector_tests/test_temperature_handling.py::TestOpenRouterTemperatureHandling::test_temperature_with_extra_params PASSED [ 63%]
tests/unit/openrouter_connector_tests/test_temperature_handling.py::TestOpenRouterTemperatureHandling::test_temperature_with_reasoning_effort PASSED [ 63%]
tests/unit/openrouter_connector_tests/test_temperature_handling.py::TestOpenRouterTemperatureHandling::test_temperature_with_reasoning_config PASSED [ 63%]
tests/unit/openrouter_connector_tests/test_temperature_handling.py::TestOpenRouterTemperatureHandling::test_no_temperature_not_in_payload PASSED [ 63%]
tests/unit/openrouter_connector_tests/test_temperature_handling.py::TestOpenRouterTemperatureHandling::test_temperature_with_extra_params_override PASSED [ 63%]
tests/unit/openrouter_connector_tests/test_temperature_handling.py::TestOpenRouterTemperatureHandling::test_temperature_streaming_request FAILED [ 63%]
tests/unit/openrouter_connector_tests/test_temperature_handling.py::TestOpenRouterTemperatureHandling::test_temperature_with_all_standard_params PASSED [ 64%]
tests/unit/proxy_logic_tests/test_parse_arguments.py::TestParseArguments::test_parse_valid_arguments PASSED [ 64%]
tests/unit/proxy_logic_tests/test_parse_arguments.py::TestParseArguments::test_parse_empty_arguments PASSED [ 64%]
tests/unit/proxy_logic_tests/test_parse_arguments.py::TestParseArguments::test_parse_arguments_with_slashes_in_model_name PASSED [ 64%]
tests/unit/proxy_logic_tests/test_parse_arguments.py::TestParseArguments::test_parse_arguments_single_argument PASSED [ 64%]
tests/unit/proxy_logic_tests/test_parse_arguments.py::TestParseArguments::test_parse_arguments_with_spaces PASSED [ 64%]
tests/unit/proxy_logic_tests/test_parse_arguments.py::TestParseArguments::test_parse_flag_argument PASSED [ 64%]
tests/unit/proxy_logic_tests/test_parse_arguments.py::TestParseArguments::test_parse_mixed_arguments PASSED [ 64%]
tests/unit/proxy_logic_tests/test_parse_arguments.py::TestParseArguments::test_parse_project_with_spaces_and_quotes PASSED [ 65%]
tests/unit/proxy_logic_tests/test_parse_arguments.py::TestParseArguments::test_parse_project_with_double_quotes PASSED [ 65%]
tests/unit/proxy_logic_tests/test_parse_arguments.py::TestParseArguments::test_parse_project_without_quotes PASSED [ 65%]
tests/unit/proxy_logic_tests/test_parse_arguments.py::TestParseArguments::test_parse_project_name_alias_quotes PASSED [ 65%]
tests/unit/proxy_logic_tests/test_parse_arguments.py::TestParseArguments::test_parse_project_name_alias_no_quotes PASSED [ 65%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_string_content_with_set_command PASSED [ 65%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_multimodal_content_with_command PASSED [ 65%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_command_strips_text_part_empty_in_multimodal PASSED [ 65%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_command_strips_message_to_empty_multimodal PASSED [ 66%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_command_in_earlier_message_not_processed_if_later_has_command PASSED [ 66%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_command_in_earlier_message_processed_if_later_has_no_command PASSED [ 66%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_no_commands_in_any_message PASSED [ 66%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_process_empty_messages_list PASSED [ 66%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_message_with_only_command_string_content PASSED [ 66%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_multimodal_text_part_preserved_if_empty_but_no_command_found PASSED [ 66%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_unknown_command_in_last_message PASSED [ 67%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_custom_command_prefix PASSED [ 67%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_multiline_command_detection PASSED [ 67%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_set_project_in_messages PASSED [ 67%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_unset_model_and_project_in_message FAILED [ 67%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_set_command_prefix_variants[$/] PASSED [ 67%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_set_command_prefix_variants['$/'] PASSED [ 67%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_set_command_prefix_variants["$/"] PASSED [ 67%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_unset_command_prefix PASSED [ 68%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_command_with_agent_environment_details PASSED [ 68%]
tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_set_command_with_multiple_parameters_and_prefix PASSED [ 68%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_no_commands PASSED [ 68%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_set_model_command PASSED [ 68%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_set_model_command_with_slash PASSED [ 68%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_unset_model_command FAILED [ 68%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_multiple_commands_in_one_string PASSED [ 68%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_unknown_commands_are_preserved PASSED [ 69%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_command_at_start_of_string PASSED [ 69%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_command_at_end_of_string PASSED [ 69%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_command_only_string PASSED [ 69%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_malformed_set_command PASSED [ 69%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_malformed_unset_command PASSED [ 69%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_set_and_unset_project FAILED [ 69%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_unset_model_and_project_together FAILED [ 69%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_set_interactive_mode FAILED [ 70%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_unset_interactive_mode FAILED [ 70%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_hello_command PASSED [ 70%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_hello_command_with_text PASSED [ 70%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_unknown_command_removed_interactive FAILED [ 70%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_set_invalid_model_interactive FAILED [ 70%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_set_invalid_model_noninteractive FAILED [ 70%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_set_backend PASSED [ 70%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_unset_backend PASSED [ 71%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_set_redact_api_keys_flag PASSED [ 71%]
tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_unset_redact_api_keys_flag PASSED [ 71%]
tests/unit/test_agent_utils.py::test_detect_agent_cline PASSED           [ 71%]
tests/unit/test_agent_utils.py::test_detect_agent_roocode PASSED         [ 71%]
tests/unit/test_agent_utils.py::test_detect_agent_aider PASSED           [ 71%]
tests/unit/test_agent_utils.py::test_wrap_proxy_message_cline PASSED     [ 71%]
tests/unit/test_agent_utils.py::test_wrap_proxy_message_aider PASSED     [ 71%]
tests/unit/test_api_redaction_middleware.py::TestRequestRedactionMiddleware::test_middleware_initialization PASSED [ 72%]
tests/unit/test_api_redaction_middleware.py::TestRequestRedactionMiddleware::test_add_processor PASSED [ 72%]
tests/unit/test_api_redaction_middleware.py::TestRequestRedactionMiddleware::test_remove_processor PASSED [ 72%]
tests/unit/test_api_redaction_middleware.py::TestRequestRedactionMiddleware::test_process_request_no_processors PASSED [ 72%]
tests/unit/test_api_redaction_middleware.py::TestRequestRedactionMiddleware::test_process_request_with_redaction_disabled PASSED [ 72%]
tests/unit/test_api_redaction_middleware.py::TestRequestRedactionMiddleware::test_process_request_with_redaction_enabled PASSED [ 72%]
tests/unit/test_api_redaction_middleware.py::TestRequestRedactionMiddleware::test_process_request_with_command_filtering PASSED [ 72%]
tests/unit/test_api_redaction_middleware.py::TestRequestRedactionMiddleware::test_process_request_with_both_redaction_and_filtering PASSED [ 72%]
tests/unit/test_api_redaction_middleware.py::TestRequestRedactionMiddleware::test_process_request_with_multiple_messages PASSED [ 73%]
tests/unit/test_api_redaction_middleware.py::TestRequestRedactionMiddleware::test_process_request_with_message_parts PASSED [ 73%]
tests/unit/test_api_redaction_middleware.py::TestResponseRedactionMiddleware::test_response_middleware_initialization PASSED [ 73%]
tests/unit/test_api_redaction_middleware.py::TestResponseRedactionMiddleware::test_process_response_no_processors PASSED [ 73%]
tests/unit/test_api_redaction_middleware.py::TestResponseRedactionMiddleware::test_process_non_streaming_response_with_redaction PASSED [ 73%]
tests/unit/test_api_redaction_middleware.py::TestResponseRedactionMiddleware::test_process_streaming_response_with_redaction PASSED [ 73%]
tests/unit/test_api_redaction_middleware.py::TestGlobalMiddlewareConfiguration::test_configure_redaction_middleware PASSED [ 73%]
tests/unit/test_api_redaction_middleware.py::TestGlobalMiddlewareConfiguration::test_reconfigure_redaction_middleware PASSED [ 73%]
tests/unit/test_auth.py::test_auth_required PASSED                       [ 74%]
tests/unit/test_auth.py::test_auth_wrong_key PASSED                      [ 74%]
tests/unit/test_auth.py::test_disable_auth PASSED                        [ 74%]
tests/unit/test_auth.py::test_disable_auth_no_key_generated PASSED       [ 74%]
tests/unit/test_cli.py::test_apply_cli_args_sets_env PASSED              [ 74%]
tests/unit/test_cli.py::test_cli_interactive_mode PASSED                 [ 74%]
tests/unit/test_cli.py::test_cli_redaction_flag PASSED                   [ 74%]
tests/unit/test_cli.py::test_cli_force_set_project PASSED                [ 74%]
tests/unit/test_cli.py::test_cli_disable_interactive_commands PASSED     [ 75%]
tests/unit/test_cli.py::test_cli_log_argument PASSED                     [ 75%]
tests/unit/test_cli.py::test_main_log_file PASSED                        [ 75%]
tests/unit/test_cli.py::test_build_app_uses_env FAILED                   [ 75%]
tests/unit/test_cli.py::test_build_app_uses_interactive_env PASSED       [ 75%]
tests/unit/test_cli.py::test_default_command_prefix_from_env PASSED      [ 75%]
tests/unit/test_cli.py::test_invalid_command_prefix_cli[!] PASSED        [ 75%]
tests/unit/test_cli.py::test_invalid_command_prefix_cli[!!] PASSED       [ 75%]
tests/unit/test_cli.py::test_invalid_command_prefix_cli[prefix with space] PASSED [ 76%]
tests/unit/test_cli.py::test_invalid_command_prefix_cli[12345678901] PASSED [ 76%]
tests/unit/test_cli.py::test_check_privileges_root SKIPPED (Test for...) [ 76%]
tests/unit/test_cli.py::test_check_privileges_non_root SKIPPED (Test...) [ 76%]
tests/unit/test_cli.py::test_check_privileges_admin_windows PASSED       [ 76%]
tests/unit/test_cli.py::test_check_privileges_non_admin_windows PASSED   [ 76%]
tests/unit/test_cli.py::test_parse_cli_args_basic PASSED                 [ 76%]
tests/unit/test_cli.py::test_parse_cli_args_disable_auth PASSED          [ 76%]
tests/unit/test_cli.py::test_apply_cli_args_basic PASSED                 [ 77%]
tests/unit/test_cli.py::test_apply_cli_args_disable_auth_forces_localhost PASSED [ 77%]
tests/unit/test_cli.py::test_apply_cli_args_disable_auth_with_localhost_no_warning PASSED [ 77%]
tests/unit/test_cli.py::test_main_disable_auth_forces_localhost PASSED   [ 77%]
tests/unit/test_cli.py::test_main_disable_auth_with_localhost_no_force PASSED [ 77%]
tests/unit/test_cli.py::test_main_auth_enabled_allows_custom_host PASSED [ 77%]
tests/unit/test_command_parser.py::test_parse_arguments_empty PASSED     [ 77%]
tests/unit/test_command_parser.py::test_parse_arguments_simple_key_value PASSED [ 77%]
tests/unit/test_command_parser.py::test_parse_arguments_multiple_key_values PASSED [ 78%]
tests/unit/test_command_parser.py::test_parse_arguments_boolean_true PASSED [ 78%]
tests/unit/test_command_parser.py::test_parse_arguments_mixed_values PASSED [ 78%]
tests/unit/test_command_parser.py::test_parse_arguments_quotes_stripping PASSED [ 78%]
tests/unit/test_command_parser.py::test_get_command_pattern_default_prefix PASSED [ 78%]
tests/unit/test_command_parser.py::test_get_command_pattern_custom_prefix PASSED [ 78%]
tests/unit/test_command_parser.py::test_process_text_single_command[preserve_unknown_True] PASSED [ 78%]
tests/unit/test_command_parser.py::test_process_text_single_command[preserve_unknown_False] PASSED [ 78%]
tests/unit/test_command_parser.py::test_process_text_command_with_prefix_text[preserve_unknown_True] PASSED [ 79%]
tests/unit/test_command_parser.py::test_process_text_command_with_prefix_text[preserve_unknown_False] PASSED [ 79%]
tests/unit/test_command_parser.py::test_process_text_command_with_suffix_text[preserve_unknown_True] PASSED [ 79%]
tests/unit/test_command_parser.py::test_process_text_command_with_suffix_text[preserve_unknown_False] PASSED [ 79%]
tests/unit/test_command_parser.py::test_process_text_command_with_prefix_and_suffix_text[preserve_unknown_True] PASSED [ 79%]
tests/unit/test_command_parser.py::test_process_text_command_with_prefix_and_suffix_text[preserve_unknown_False] PASSED [ 79%]
tests/unit/test_command_parser.py::test_process_text_multiple_commands_only_first_processed[preserve_unknown_True] PASSED [ 79%]
tests/unit/test_command_parser.py::test_process_text_multiple_commands_only_first_processed[preserve_unknown_False] PASSED [ 79%]
tests/unit/test_command_parser.py::test_process_text_no_command[preserve_unknown_True] PASSED [ 80%]
tests/unit/test_command_parser.py::test_process_text_no_command[preserve_unknown_False] PASSED [ 80%]
tests/unit/test_command_parser.py::test_process_text_unknown_command[preserve_unknown_True] PASSED [ 80%]
tests/unit/test_command_parser.py::test_process_text_unknown_command[preserve_unknown_False] PASSED [ 80%]
tests/unit/test_command_parser.py::test_process_messages_single_message_with_command[preserve_unknown_True] PASSED [ 80%]
tests/unit/test_command_parser.py::test_process_messages_single_message_with_command[preserve_unknown_False] PASSED [ 80%]
tests/unit/test_command_parser.py::test_process_messages_stops_after_first_command_in_message_content_list[preserve_unknown_True] PASSED [ 80%]
tests/unit/test_command_parser.py::test_process_messages_stops_after_first_command_in_message_content_list[preserve_unknown_False] PASSED [ 80%]
tests/unit/test_command_parser.py::test_process_messages_processes_command_in_last_message_and_stops[preserve_unknown_True] PASSED [ 81%]
tests/unit/test_command_parser.py::test_process_messages_processes_command_in_last_message_and_stops[preserve_unknown_False] PASSED [ 81%]
tests/unit/test_config.py::test_collect_api_keys_single PASSED           [ 81%]
tests/unit/test_config.py::test_collect_api_keys_numbered PASSED         [ 81%]
tests/unit/test_config.py::test_collect_api_keys_prioritizes_numbered PASSED [ 81%]
tests/unit/test_config.py::test_load_config_basic PASSED                 [ 81%]
tests/unit/test_config.py::test_load_config_custom_values PASSED         [ 81%]
tests/unit/test_config.py::test_load_config_disable_auth_forces_localhost PASSED [ 81%]
tests/unit/test_config.py::test_load_config_disable_auth_with_localhost_no_warning PASSED [ 82%]
tests/unit/test_config.py::test_load_config_auth_enabled_allows_custom_host PASSED [ 82%]
tests/unit/test_config.py::test_load_config_str_to_bool_variations PASSED [ 82%]
tests/unit/test_config_persistence.py::test_save_and_load_persistent_config FAILED [ 82%]
tests/unit/test_config_persistence.py::test_invalid_persisted_backend FAILED [ 82%]
tests/unit/test_emergency_command_filter.py::TestProxyCommandFilter::test_filter_initialization PASSED [ 82%]
tests/unit/test_emergency_command_filter.py::TestProxyCommandFilter::test_basic_command_detection_and_removal PASSED [ 82%]
tests/unit/test_emergency_command_filter.py::TestProxyCommandFilter::test_command_only_text PASSED [ 82%]
tests/unit/test_emergency_command_filter.py::TestProxyCommandFilter::test_no_commands_present PASSED [ 83%]
tests/unit/test_emergency_command_filter.py::TestProxyCommandFilter::test_different_command_prefixes PASSED [ 83%]
tests/unit/test_emergency_command_filter.py::TestProxyCommandFilter::test_prefix_update PASSED [ 83%]
tests/unit/test_emergency_command_filter.py::TestProxyCommandFilter::test_edge_cases PASSED [ 83%]
tests/unit/test_emergency_command_filter.py::TestProxyCommandFilter::test_complex_command_patterns PASSED [ 83%]
tests/unit/test_failover_routes.py::TestFailoverRoutes::test_create_route_enables_interactive FAILED [ 83%]
tests/unit/test_failover_routes.py::TestFailoverRoutes::test_route_append_and_list FAILED [ 83%]
tests/unit/test_failover_routes.py::TestFailoverRoutes::test_routes_are_server_wide FAILED [ 83%]
tests/unit/test_gemini_converters.py::TestMessageConversion::test_gemini_to_openai_simple_message PASSED [ 84%]
tests/unit/test_gemini_converters.py::TestMessageConversion::test_gemini_to_openai_model_role PASSED [ 84%]
tests/unit/test_gemini_converters.py::TestMessageConversion::test_gemini_to_openai_multiple_parts PASSED [ 84%]
tests/unit/test_gemini_converters.py::TestMessageConversion::test_openai_to_gemini_simple_message PASSED [ 84%]
tests/unit/test_gemini_converters.py::TestMessageConversion::test_openai_to_gemini_assistant_role PASSED [ 84%]
tests/unit/test_gemini_converters.py::TestMessageConversion::test_openai_to_gemini_system_role PASSED [ 84%]
tests/unit/test_model_discovery.py::test_openrouter_models_cached FAILED [ 84%]
tests/unit/test_model_discovery.py::test_gemini_models_cached FAILED     [ 84%]
tests/unit/test_model_discovery.py::test_auto_default_backend FAILED     [ 85%]
tests/unit/test_model_discovery.py::test_multiple_backends_requires_arg FAILED [ 85%]
tests/unit/test_models_endpoint.py::test_models_endpoint_lists_all FAILED [ 85%]
tests/unit/test_models_endpoint.py::test_v1_models_endpoint_lists_all FAILED [ 85%]
tests/unit/test_no_prints.py::test_no_print_statements FAILED            [ 85%]
tests/unit/test_prompt_redaction.py::test_redactor_replaces_keys_and_logs PASSED [ 85%]
tests/unit/test_proxy_logic.py::TestParseArguments::test_parse_valid_arguments PASSED [ 85%]
tests/unit/test_proxy_logic.py::TestParseArguments::test_parse_empty_arguments PASSED [ 85%]
tests/unit/test_proxy_logic.py::TestParseArguments::test_parse_arguments_with_slashes_in_model_name PASSED [ 86%]
tests/unit/test_proxy_logic.py::TestParseArguments::test_parse_arguments_single_argument PASSED [ 86%]
tests/unit/test_proxy_logic.py::TestParseArguments::test_parse_arguments_with_spaces PASSED [ 86%]
tests/unit/test_proxy_logic.py::TestParseArguments::test_parse_flag_argument PASSED [ 86%]
tests/unit/test_proxy_logic.py::TestParseArguments::test_parse_mixed_arguments PASSED [ 86%]
tests/unit/test_qwen_oauth_authentication.py::TestQwenOAuthAuthentication::test_token_refresh_exact_expiry PASSED [ 86%]
tests/unit/test_qwen_oauth_authentication.py::TestQwenOAuthAuthentication::test_token_refresh_before_expiry PASSED [ 86%]
tests/unit/test_qwen_oauth_authentication.py::TestQwenOAuthAuthentication::test_token_refresh_flow_success PASSED [ 86%]
tests/unit/test_qwen_oauth_authentication.py::TestQwenOAuthAuthentication::test_token_refresh_no_refresh_token PASSED [ 87%]
tests/unit/test_qwen_oauth_authentication.py::TestQwenOAuthAuthentication::test_token_refresh_http_error PASSED [ 87%]
tests/unit/test_qwen_oauth_authentication.py::TestQwenOAuthAuthentication::test_token_refresh_network_error PASSED [ 87%]
tests/unit/test_qwen_oauth_authentication.py::TestQwenOAuthAuthentication::test_token_refresh_malformed_response PASSED [ 87%]
tests/unit/test_qwen_oauth_authentication.py::TestQwenOAuthAuthentication::test_chat_completion_token_refresh_check PASSED [ 87%]
tests/unit/test_qwen_oauth_authentication.py::TestQwenOAuthAuthentication::test_chat_completion_token_refresh_failure PASSED [ 87%]
tests/unit/test_qwen_oauth_authentication.py::TestQwenOAuthAuthentication::test_credential_persistence PASSED [ 87%]
tests/unit/test_qwen_oauth_authentication.py::TestQwenOAuthAuthentication::test_credential_persistence_error PASSED [ 87%]
tests/unit/test_qwen_oauth_authentication.py::TestQwenOAuthAuthentication::test_credential_loading_success PASSED [ 88%]
tests/unit/test_qwen_oauth_authentication.py::TestQwenOAuthAuthentication::test_credential_loading_file_not_found PASSED [ 88%]
tests/unit/test_qwen_oauth_authentication.py::TestQwenOAuthAuthentication::test_credential_loading_permission_error PASSED [ 88%]
tests/unit/test_qwen_oauth_authentication.py::TestQwenOAuthAuthentication::test_credential_loading_malformed_json PASSED [ 88%]
tests/unit/test_qwen_oauth_authentication.py::TestQwenOAuthAuthentication::test_credential_loading_missing_access_token PASSED [ 88%]
tests/unit/test_qwen_oauth_authentication.py::TestQwenOAuthAuthentication::test_get_headers_valid_token PASSED [ 88%]
tests/unit/test_qwen_oauth_authentication.py::TestQwenOAuthAuthentication::test_get_headers_no_token PASSED [ 88%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_connector_initialization PASSED [ 88%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_initialize_with_valid_credentials PASSED [ 89%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_initialize_without_credentials PASSED [ 89%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_initialize_with_invalid_credentials PASSED [ 89%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_get_access_token PASSED [ 89%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_get_access_token_no_credentials PASSED [ 89%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_get_endpoint_url_with_resource_url PASSED [ 89%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_get_endpoint_url_without_protocol PASSED [ 89%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_get_endpoint_url_default PASSED [ 89%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_is_token_expired_valid_token PASSED [ 90%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_is_token_expired_expired_token PASSED [ 90%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_is_token_expired_no_expiry PASSED [ 90%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_get_headers PASSED [ 90%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_get_headers_no_token PASSED [ 90%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_refresh_token_success PASSED [ 90%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_refresh_token_no_refresh_token PASSED [ 90%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_refresh_token_http_error PASSED [ 90%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_get_available_models_functional PASSED [ 91%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_get_available_models_not_functional PASSED [ 91%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_chat_completions_success PASSED [ 91%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_chat_completions_with_prefix PASSED [ 91%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_chat_completions_streaming PASSED [ 91%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_chat_completions_token_refresh_failure PASSED [ 91%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_chat_completions_exception_handling PASSED [ 91%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorUnit::test_save_oauth_credentials PASSED [ 91%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorEdgeCases::test_malformed_credentials_file PASSED [ 92%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorEdgeCases::test_credentials_file_permission_error PASSED [ 92%]
tests/unit/test_qwen_oauth_connector.py::TestQwenOAuthConnectorEdgeCases::test_network_error_during_refresh PASSED [ 92%]
tests/unit/test_qwen_oauth_enhanced_error_handling.py::TestQwenOAuthEnhancedErrorHandling::test_initialize_success PASSED [ 92%]
tests/unit/test_qwen_oauth_enhanced_error_handling.py::TestQwenOAuthEnhancedErrorHandling::test_initialize_failure PASSED [ 92%]
tests/unit/test_qwen_oauth_enhanced_error_handling.py::TestQwenOAuthEnhancedErrorHandling::test_get_endpoint_url_no_credentials PASSED [ 92%]
tests/unit/test_qwen_oauth_enhanced_error_handling.py::TestQwenOAuthEnhancedErrorHandling::test_is_token_expired_no_credentials PASSED [ 92%]
tests/unit/test_qwen_oauth_enhanced_error_handling.py::TestQwenOAuthEnhancedErrorHandling::test_is_token_expired_no_expiry_date PASSED [ 92%]
tests/unit/test_qwen_oauth_enhanced_error_handling.py::TestQwenOAuthEnhancedErrorHandling::test_token_refresh_check_not_expired PASSED [ 93%]
tests/unit/test_qwen_oauth_enhanced_error_handling.py::TestQwenOAuthEnhancedErrorHandling::test_chat_completions_generic_error_handling PASSED [ 93%]
tests/unit/test_qwen_oauth_enhanced_error_handling.py::TestQwenOAuthEnhancedErrorHandling::test_model_prefix_stripping PASSED [ 93%]
tests/unit/test_qwen_oauth_enhanced_error_handling.py::TestQwenOAuthEnhancedErrorHandling::test_http_exception_passthrough PASSED [ 93%]
tests/unit/test_qwen_oauth_enhanced_error_handling.py::TestQwenOAuthEnhancedErrorHandling::test_chat_completions_refresh_token_failure PASSED [ 93%]
tests/unit/test_qwen_oauth_interactive_commands.py::TestQwenOAuthInteractiveCommands::test_environment_variable_support PASSED [ 93%]
tests/unit/test_qwen_oauth_interactive_commands.py::TestQwenOAuthInteractiveCommands::test_cli_argument_support SKIPPED [ 93%]
tests/unit/test_qwen_oauth_interactive_commands.py::TestQwenOAuthInteractiveCommands::test_interactive_backend_setting PASSED [ 93%]
tests/unit/test_qwen_oauth_interactive_commands.py::TestQwenOAuthInteractiveCommands::test_interactive_model_setting_issue PASSED [ 94%]
tests/unit/test_qwen_oauth_interactive_commands.py::TestQwenOAuthInteractiveCommands::test_backend_attribute_name_conversion SKIPPED [ 94%]
tests/unit/test_qwen_oauth_interactive_commands.py::TestQwenOAuthInteractiveCommands::test_backend_object_accessibility SKIPPED [ 94%]
tests/unit/test_qwen_oauth_interactive_commands.py::TestQwenOAuthInteractiveCommands::test_fixed_interactive_model_setting PASSED [ 94%]
tests/unit/test_qwen_oauth_interactive_commands.py::TestQwenOAuthInteractiveCommands::test_functional_backends_includes_qwen_oauth FAILED [ 94%]
tests/unit/test_qwen_oauth_interactive_commands.py::TestQwenOAuthInteractiveCommands::test_backend_routing_with_qwen_oauth PASSED [ 94%]
tests/unit/test_qwen_oauth_interactive_commands.py::TestQwenOAuthConfigurationMethods::test_dotenv_file_support PASSED [ 94%]
tests/unit/test_qwen_oauth_interactive_commands.py::TestQwenOAuthConfigurationMethods::test_config_file_backend_persistence PASSED [ 94%]
tests/unit/test_qwen_oauth_interactive_commands.py::TestQwenOAuthConfigurationMethods::test_all_backend_access_methods SKIPPED [ 95%]
tests/unit/test_qwen_oauth_tool_calling.py::TestQwenOAuthToolCallingUnit::test_chat_completions_with_tools PASSED [ 95%]
tests/unit/test_qwen_oauth_tool_calling.py::TestQwenOAuthToolCallingUnit::test_chat_completions_tool_choice_none PASSED [ 95%]
tests/unit/test_qwen_oauth_tool_calling.py::TestQwenOAuthToolCallingUnit::test_chat_completions_specific_tool_choice PASSED [ 95%]
tests/unit/test_qwen_oauth_tool_calling.py::TestQwenOAuthToolCallingUnit::test_streaming_with_tool_calls PASSED [ 95%]
tests/unit/test_qwen_oauth_tool_calling.py::TestQwenOAuthToolCallingUnit::test_multi_turn_tool_conversation PASSED [ 95%]
tests/unit/test_qwen_oauth_tool_calling.py::TestQwenOAuthToolCallingUnit::test_tool_calling_error_handling PASSED [ 95%]
tests/unit/test_qwen_oauth_tool_calling.py::TestQwenOAuthToolCallingUnit::test_tool_call_serialization PASSED [ 95%]
tests/unit/test_qwen_oauth_tool_calling_enhanced.py::TestQwenOAuthToolCallingEnhanced::test_chat_completions_with_tools PASSED [ 96%]
tests/unit/test_qwen_oauth_tool_calling_enhanced.py::TestQwenOAuthToolCallingEnhanced::test_chat_completions_tool_choice_none PASSED [ 96%]
tests/unit/test_qwen_oauth_tool_calling_enhanced.py::TestQwenOAuthToolCallingEnhanced::test_chat_completions_specific_tool_choice PASSED [ 96%]
tests/unit/test_qwen_oauth_tool_calling_enhanced.py::TestQwenOAuthToolCallingEnhanced::test_streaming_with_tool_calls PASSED [ 96%]
tests/unit/test_qwen_oauth_tool_calling_enhanced.py::TestQwenOAuthToolCallingEnhanced::test_multi_turn_tool_conversation PASSED [ 96%]
tests/unit/test_qwen_oauth_tool_calling_enhanced.py::TestQwenOAuthToolCallingEnhanced::test_tool_calling_error_handling PASSED [ 96%]
tests/unit/test_qwen_oauth_tool_calling_enhanced.py::TestQwenOAuthToolCallingEnhanced::test_model_prefix_stripping PASSED [ 96%]
tests/unit/test_rate_limit.py::test_parse_retry_delay_with_prefixed_string PASSED [ 96%]
tests/unit/test_rate_limit.py::test_rate_limit_registry_earliest PASSED  [ 97%]
tests/unit/test_response_middleware.py::TestResponseMiddleware::test_middleware_initialization PASSED [ 97%]
tests/unit/test_response_middleware.py::TestResponseMiddleware::test_add_processor PASSED [ 97%]
tests/unit/test_response_middleware.py::TestResponseMiddleware::test_remove_processor PASSED [ 97%]
tests/unit/test_response_middleware.py::TestResponseMiddleware::test_process_response_no_processors PASSED [ 97%]
tests/unit/test_response_middleware.py::TestResponseMiddleware::test_process_response_with_processors PASSED [ 97%]
tests/unit/test_response_middleware.py::TestRequestContext::test_context_creation PASSED [ 97%]
tests/unit/test_response_middleware.py::TestLoopDetectionProcessor::test_processor_initialization PASSED [ 97%]
tests/unit/test_response_middleware.py::TestLoopDetectionProcessor::test_should_process_enabled PASSED [ 98%]
tests/unit/test_response_middleware.py::TestLoopDetectionProcessor::test_should_process_disabled PASSED [ 98%]
tests/unit/test_response_middleware.py::TestLoopDetectionProcessor::test_get_or_create_detector PASSED [ 98%]
tests/unit/test_response_middleware.py::TestLoopDetectionProcessor::test_process_non_streaming_response PASSED [ 98%]
tests/unit/test_response_middleware.py::TestLoopDetectionProcessor::test_cleanup_session PASSED [ 98%]
tests/unit/test_response_middleware.py::TestGlobalMiddleware::test_configure_loop_detection_middleware PASSED [ 98%]
tests/unit/test_response_middleware.py::TestGlobalMiddleware::test_configure_disabled_loop_detection PASSED [ 98%]
tests/unit/test_response_middleware.py::TestGlobalMiddleware::test_reconfigure_middleware PASSED [ 98%]
tests/unit/test_response_shape.py::test_extract_response_content_with_dict PASSED [ 99%]
tests/unit/test_response_shape.py::test_extract_response_content_with_object_choices PASSED [ 99%]
tests/unit/test_response_shape.py::test_extract_response_content_with_tuple_is_invalid PASSED [ 99%]
tests/unit/test_security.py::test_cli_disable_auth_forces_localhost PASSED [ 99%]
tests/unit/test_security.py::test_env_disable_auth_forces_localhost PASSED [ 99%]
tests/unit/test_security.py::test_auth_enabled_allows_custom_host PASSED [ 99%]
tests/unit/test_security.py::test_config_disable_auth_forces_localhost PASSED [ 99%]
tests/unit/test_security.py::test_security_documentation PASSED          [100%]

=================================== ERRORS ====================================
___________ ERROR at setup of test_anthropic_messages_non_streaming ___________

fixturedef = <FixtureDef argname='test_config' scope='session' baseid='tests'>
request = <SubRequest 'test_config' for <Coroutine test_anthropic_messages_non_streaming>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
                return (yield)
            if not _is_coroutine_or_asyncgen(fixturedef.func):
>               return (yield)

.venv\lib\site-packages\pytest_asyncio\plugin.py:683: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of test_anthropic_messages_with_tool_use_from_openai_tool_calls _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
______ ERROR at setup of test_anthropic_messages_non_streaming_frontend _______

    @pytest.fixture()
    def anthropic_client():
        """Create TestClient with config patched for Anthropic."""
        with (
            patch("src.core.config.app_config.load_config") as mock_cfg,
            patch(
                "src.connectors.anthropic.AnthropicBackend.get_available_models",
                return_value=["claude-3-haiku-20240229"],
            ),
        ):
            # Create a proper AppConfig object
            config = AppConfig()
            config.auth = AuthConfig(disable_auth=False, api_keys=["test-proxy-key"])
            config.proxy_timeout = 10
            config.session.default_interactive_mode = False
            config.command_prefix = "!/"
            config.backends = BackendSettings()
>           config.backends.anthropic = BackendConfig(
                api_key="ant-key", api_url="https://api.anthropic.com/v1"
            )
E           pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E           api_key
E             Input should be a valid list [type=list_type, input_value='ant-key', input_type=str]
E               For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\chat_completions_tests\test_anthropic_frontend.py:38: ValidationError
________ ERROR at setup of test_anthropic_messages_streaming_frontend _________

    @pytest.fixture()
    def anthropic_client():
        """Create TestClient with config patched for Anthropic."""
        with (
            patch("src.core.config.app_config.load_config") as mock_cfg,
            patch(
                "src.connectors.anthropic.AnthropicBackend.get_available_models",
                return_value=["claude-3-haiku-20240229"],
            ),
        ):
            # Create a proper AppConfig object
            config = AppConfig()
            config.auth = AuthConfig(disable_auth=False, api_keys=["test-proxy-key"])
            config.proxy_timeout = 10
            config.session.default_interactive_mode = False
            config.command_prefix = "!/"
            config.backends = BackendSettings()
>           config.backends.anthropic = BackendConfig(
                api_key="ant-key", api_url="https://api.anthropic.com/v1"
            )
E           pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E           api_key
E             Input should be a valid list [type=list_type, input_value='ant-key', input_type=str]
E               For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\chat_completions_tests\test_anthropic_frontend.py:38: ValidationError
___________ ERROR at setup of test_anthropic_messages_auth_failure ____________

    @pytest.fixture()
    def anthropic_client():
        """Create TestClient with config patched for Anthropic."""
        with (
            patch("src.core.config.app_config.load_config") as mock_cfg,
            patch(
                "src.connectors.anthropic.AnthropicBackend.get_available_models",
                return_value=["claude-3-haiku-20240229"],
            ),
        ):
            # Create a proper AppConfig object
            config = AppConfig()
            config.auth = AuthConfig(disable_auth=False, api_keys=["test-proxy-key"])
            config.proxy_timeout = 10
            config.session.default_interactive_mode = False
            config.command_prefix = "!/"
            config.backends = BackendSettings()
>           config.backends.anthropic = BackendConfig(
                api_key="ant-key", api_url="https://api.anthropic.com/v1"
            )
E           pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E           api_key
E             Input should be a valid list [type=list_type, input_value='ant-key', input_type=str]
E               For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\chat_completions_tests\test_anthropic_frontend.py:38: ValidationError
__________ ERROR at setup of test_models_endpoint_includes_anthropic __________

    @pytest.fixture()
    def anthropic_client():
        """Create TestClient with config patched for Anthropic."""
        with (
            patch("src.core.config.app_config.load_config") as mock_cfg,
            patch(
                "src.connectors.anthropic.AnthropicBackend.get_available_models",
                return_value=["claude-3-haiku-20240229"],
            ),
        ):
            # Create a proper AppConfig object
            config = AppConfig()
            config.auth = AuthConfig(disable_auth=False, api_keys=["test-proxy-key"])
            config.proxy_timeout = 10
            config.session.default_interactive_mode = False
            config.command_prefix = "!/"
            config.backends = BackendSettings()
>           config.backends.anthropic = BackendConfig(
                api_key="ant-key", api_url="https://api.anthropic.com/v1"
            )
E           pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E           api_key
E             Input should be a valid list [type=list_type, input_value='ant-key', input_type=str]
E               For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\chat_completions_tests\test_anthropic_frontend.py:38: ValidationError
_ ERROR at setup of TestMultipleOneoffCommands.test_multiple_oneoff_commands_sequence _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of TestMultipleOneoffCommands.test_oneoff_commands_different_sessions _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of TestMultipleOneoffCommands.test_oneoff_command_with_prompt_in_same_message _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
______________ ERROR at setup of test_oneoff_command_integration ______________

    @pytest.fixture
    def app():
        """Create a test app with oneoff commands enabled."""
        # Create app with test config
>       config = AppConfig()
E       NameError: name 'AppConfig' is not defined

tests\integration\test_oneoff_command_integration.py:18: NameError
_________ ERROR at setup of test_basic_request_proxying_non_streaming _________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
___________ ERROR at setup of test_basic_request_proxying_streaming ___________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_________ ERROR at setup of test_cline_xml_wrapping_for_all_commands __________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
__________ ERROR at setup of test_cline_xml_wrapping_error_commands ___________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
________ ERROR at setup of test_cline_xml_wrapping_pure_error_commands ________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
__________ ERROR at setup of test_non_cline_commands_no_xml_wrapping __________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
____________ ERROR at setup of test_cline_hello_command_tool_calls ____________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
___________ ERROR at setup of test_cline_hello_command_same_request ___________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_______ ERROR at setup of test_cline_hello_with_attempt_completion_only _______

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
__________ ERROR at setup of test_cline_hello_command_first_message ___________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_______ ERROR at setup of test_non_cline_hello_command_no_xml_wrapping ________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
__________ ERROR at setup of test_non_cline_clients_no_xml_wrapping ___________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
________ ERROR at setup of test_remote_llm_responses_never_xml_wrapped ________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
__________ ERROR at setup of test_mixed_cline_command_and_llm_prompt __________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
________ ERROR at setup of test_streaming_responses_never_xml_wrapped _________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
______ ERROR at setup of test_error_responses_from_llm_never_xml_wrapped ______

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
______________ ERROR at setup of test_cline_output_format_exact _______________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
__________ ERROR at setup of test_cline_output_format_other_commands __________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_________ ERROR at setup of test_command_only_request_direct_response _________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
__________ ERROR at setup of test_command_plus_text_direct_response ___________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
______ ERROR at setup of test_command_with_agent_prefix_direct_response _______

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
__ ERROR at setup of test_command_only_request_direct_response_explicit_mock __

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
___________ ERROR at setup of test_hello_command_with_agent_prefix ____________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
____________ ERROR at setup of test_hello_command_followed_by_text ____________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_________ ERROR at setup of test_hello_command_with_prefix_and_suffix _________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
___________________ ERROR at setup of test_commands_ignored ___________________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of test_empty_messages_after_processing_no_commands_bad_request _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
__________ ERROR at setup of test_get_openrouter_headers_no_api_key ___________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_____________ ERROR at setup of test_invalid_model_noninteractive _____________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_______________ ERROR at teardown of test_failover_key_rotation _______________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x00000150F8A5FEB0>
request = <SubRequest 'httpx_mock' for <Function test_failover_key_rotation>>

    @pytest.fixture
    def httpx_mock(
        monkeypatch: MonkeyPatch,
        request: FixtureRequest,
    ) -> Generator[HTTPXMock, None, None]:
        options = {}
        for marker in request.node.iter_markers("httpx_mock"):
            options = marker.kwargs | options
        __tracebackhide__ = methodcaller("errisinstance", TypeError)
        options = _HTTPXMockOptions(**options)
    
        mock = HTTPXMock(options)
    
        # Mock synchronous requests
        real_handle_request = httpx.HTTPTransport.handle_request
    
        def mocked_handle_request(
            transport: httpx.HTTPTransport, request: httpx.Request
        ) -> httpx.Response:
            if options.should_mock(request):
                return mock._handle_request(transport, request)
            return real_handle_request(transport, request)
    
        monkeypatch.setattr(
            httpx.HTTPTransport,
            "handle_request",
            mocked_handle_request,
        )
    
        # Mock asynchronous requests
        real_handle_async_request = httpx.AsyncHTTPTransport.handle_async_request
    
        async def mocked_handle_async_request(
            transport: httpx.AsyncHTTPTransport, request: httpx.Request
        ) -> httpx.Response:
            if options.should_mock(request):
                return await mock._handle_async_request(transport, request)
            return await real_handle_async_request(transport, request)
    
        monkeypatch.setattr(
            httpx.AsyncHTTPTransport,
            "handle_async_request",
            mocked_handle_async_request,
        )
    
        yield mock
        try:
>           mock._assert_options()

.venv\lib\site-packages\pytest_httpx\__init__.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <pytest_httpx._httpx_mock.HTTPXMock object at 0x00000150F9CA3FD0>

    def _assert_options(self) -> None:
        callbacks_not_executed = [
            matcher for matcher, _ in self._callbacks if matcher.should_have_matched()
        ]
        matchers_description = "\n".join(
            [f"- {matcher}" for matcher in callbacks_not_executed]
        )
    
>       assert not callbacks_not_executed, (
            "The following responses are mocked but not requested:\n"
            f"{matchers_description}\n"
            "\n"
            "If this is on purpose, refer to https://github.com/Colin-b/pytest_httpx/blob/master/README.md#allow-to-register-more-responses-than-what-will-be-requested"
        )
E       AssertionError: The following responses are mocked but not requested:
E         - Match POST request on https://openrouter.ai/api/v1/chat/completions
E         - Match POST request on https://openrouter.ai/api/v1/chat/completions
E         
E         If this is on purpose, refer to https://github.com/Colin-b/pytest_httpx/blob/master/README.md#allow-to-register-more-responses-than-what-will-be-requested
E       assert not [<pytest_httpx._request_matcher._RequestMatcher object at 0x00000150F8AE8520>, <pytest_httpx._request_matcher._RequestMatcher object at 0x00000150F8AEAAA0>]

.venv\lib\site-packages\pytest_httpx\_httpx_mock.py:319: AssertionError
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:37.820971Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:37.821479Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:37.828960Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:37.841443Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:354 2025-08-17T00:00:37.842945Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: help
INFO     src.core.services.command_service:command_service.py:78 Registered command: backend
INFO     src.core.services.command_service:command_service.py:78 Registered command: model
INFO     src.core.services.command_service:command_service.py:78 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:78 Registered command: project
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:78 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:78 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:78 Registered command: set
INFO     src.core.services.command_service:command_service.py:78 Registered command: unset
INFO     src.core.services.command_service:command_service.py:78 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:78 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:78 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:78 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:78 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:269 Registered 26 command handlers with registry
INFO     src.core.services.command_service:command_service.py:183 Executing command: create-failover-route with session: e9608c38-23d8-4fdd-920e-dceddf425f62
INFO     src.core.services.command_service:command_service.py:183 Executing command: route-append with session: 1e8b541e-6385-4e62-8bd0-0b630a44addd
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openai: {}
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
ERROR    src.core.services.request_processor:request_processor.py:259 Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 79, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 85, in _handle_non_streaming_response
    raise HTTPException(
fastapi.exceptions.HTTPException: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 215, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
INFO     src.core.app.application_factory:application_factory.py:368 2025-08-17T00:00:37.858445Z [info     ] Shutting down application      [src.core.app.application_factory]
__________________ ERROR at setup of test_help_list_commands __________________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
________________ ERROR at setup of test_help_specific_command _________________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
___________ ERROR at setup of test_first_reply_no_automatic_banner ____________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_____________ ERROR at setup of test_hello_command_returns_banner _____________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
___ ERROR at setup of test_hello_command_returns_xml_banner_for_cline_agent ___

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_______ ERROR at setup of test_set_command_returns_xml_for_cline_agent ________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
________________ ERROR at setup of test_unknown_command_error _________________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_______________ ERROR at setup of test_set_command_confirmation _______________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_______________ ERROR at setup of test_set_backend_confirmation _______________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
______________ ERROR at setup of test_set_backend_nonfunctional _______________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
__________________ ERROR at setup of test_set_redaction_flag __________________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_________________ ERROR at setup of test_unset_redaction_flag _________________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
____________ ERROR at setup of test_set_model_command_integration _____________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
___________ ERROR at setup of test_unset_model_command_integration ____________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of test_oneoff_command[oneoff_with_prompt-request_payload0] __

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of test_oneoff_command[oneoff_alias_with_prompt-request_payload1] _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of test_oneoff_command[oneoff_without_prompt-request_payload2] _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
___________ ERROR at setup of test_set_project_command_integration ____________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
__________ ERROR at setup of test_unset_project_command_integration ___________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
__________ ERROR at setup of test_set_project_name_alias_integration __________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_________ ERROR at setup of test_unset_project_name_alias_integration _________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
__________ ERROR at setup of test_force_set_project_blocks_requests ___________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
__________ ERROR at setup of test_force_set_project_allows_after_set __________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
___________ ERROR at setup of test_set_project_dir_command_invalid ____________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
________ ERROR at setup of test_unset_project_dir_command[project-dir] ________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
____________ ERROR at setup of test_unset_project_dir_command[dir] ____________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_____ ERROR at setup of test_unset_project_dir_command[project-directory] _____

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
___________ ERROR at setup of test_pwd_command_with_project_dir_set ___________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_________ ERROR at setup of test_pwd_command_without_project_dir_set __________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
____________ ERROR at setup of test_wait_for_rate_limited_backends ____________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
__________________ ERROR at setup of test_rate_limit_memory ___________________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
______________ ERROR at setup of test_real_cline_hello_response _______________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_______________ ERROR at setup of test_cline_pure_hello_command _______________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_________________ ERROR at setup of test_cline_no_session_id __________________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
______________ ERROR at setup of test_cline_non_command_message _______________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
______________ ERROR at setup of test_cline_first_message_hello _______________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
__________ ERROR at setup of test_cline_first_message_with_detection __________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
____________ ERROR at setup of test_realistic_cline_hello_request _____________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
____ ERROR at setup of test_session_records_proxy_and_backend_interactions ____

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
________ ERROR at setup of test_session_records_streaming_placeholder _________

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of TestTemperatureCommands.test_set_temperature_command_valid_float _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of TestTemperatureCommands.test_set_temperature_command_valid_int _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of TestTemperatureCommands.test_set_temperature_command_valid_string_number _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of TestTemperatureCommands.test_set_temperature_command_zero_value _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of TestTemperatureCommands.test_set_temperature_command_max_openai_value _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of TestTemperatureCommands.test_set_temperature_command_invalid_negative _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of TestTemperatureCommands.test_set_temperature_command_invalid_too_high _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of TestTemperatureCommands.test_set_temperature_command_invalid_string _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
__ ERROR at setup of TestTemperatureCommands.test_unset_temperature_command ___

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of TestTemperatureCommands.test_temperature_persistence_across_requests _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of TestTemperatureCommands.test_temperature_with_message_content _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of TestTemperatureAPIParameters.test_direct_api_temperature_parameter _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of TestTemperatureAPIParameters.test_api_temperature_overrides_session_setting _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of TestTemperatureModelDefaults.test_temperature_model_defaults_applied _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
_ ERROR at setup of TestTemperatureModelDefaults.test_session_temperature_overrides_model_defaults _

    @pytest.fixture(scope="session")
    def test_config() -> AppConfig:
        """Create a test configuration."""
        # Create a test configuration
        return AppConfig(
            host="localhost",
            port=9000,
            proxy_timeout=10,
            command_prefix="!/",
            backends=AppConfig.BackendSettings(
                default_backend="openrouter",
>               openrouter=AppConfig.BackendConfig(api_key="test_openrouter_key"),
            ),
            auth=AppConfig.AuthConfig(
                disable_auth=True,
                api_keys=["test_api_key"],
                client_api_key="test_api_key",
            ),
            session=AppConfig.SessionConfig(
                cleanup_enabled=False,  # Disable cleanup for testing
                default_interactive_mode=True,
            ),
            logging=AppConfig.LoggingConfig(
                level=AppConfig.LogLevel.WARNING  # Silence logging during tests
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_openrouter_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\conftest.py:49: ValidationError
================================== FAILURES ===================================
_______ TestAnthropicFrontendIntegration.test_models_endpoint_via_http ________

self = <tests.integration.test_anthropic_frontend_integration.TestAnthropicFrontendIntegration object at 0x00000150F5148C40>

    def test_models_endpoint_via_http(self):
        """Test models endpoint via direct HTTP call."""
        response = self.client.get("/anthropic/v1/models")
    
>       assert response.status_code == 200
E       assert 401 == 200
E        +  where 401 = <Response [401 Unauthorized]>.status_code

tests\integration\test_anthropic_frontend_integration.py:63: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:33.545234Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:33.545234Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:33.552233Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:33.565233Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for GET /anthropic/v1/models from client testclient
_ TestAnthropicFrontendIntegration.test_messages_endpoint_validation_via_http _

self = <tests.integration.test_anthropic_frontend_integration.TestAnthropicFrontendIntegration object at 0x00000150F5148E20>

    def test_messages_endpoint_validation_via_http(self):
        """Test messages endpoint validation via direct HTTP call."""
        # Valid request structure
        request_data = {
            "model": "claude-3-sonnet-20240229",
            "messages": [{"role": "user", "content": "Hello, how are you?"}],
            "max_tokens": 100,
            "temperature": 0.7,
        }
    
        response = self.client.post("/anthropic/v1/messages", json=request_data)
    
        # Currently returns 501 but should validate the request
>       assert response.status_code == 501
E       assert 401 == 501
E        +  where 401 = <Response [401 Unauthorized]>.status_code

tests\integration\test_anthropic_frontend_integration.py:88: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:33.576758Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:33.576758Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:33.584735Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:33.600234Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for POST /anthropic/v1/messages from client testclient
_ TestAnthropicFrontendIntegration.test_messages_endpoint_with_system_message _

self = <tests.integration.test_anthropic_frontend_integration.TestAnthropicFrontendIntegration object at 0x00000150F5149000>

    def test_messages_endpoint_with_system_message(self):
        """Test messages endpoint with system message."""
        request_data = {
            "model": "claude-3-sonnet-20240229",
            "messages": [{"role": "user", "content": "What is 2+2?"}],
            "max_tokens": 50,
            "system": "You are a helpful math tutor.",
            "temperature": 0.3,
        }
    
        response = self.client.post("/anthropic/v1/messages", json=request_data)
>       assert response.status_code == 501  # Not yet implemented
E       assert 401 == 501
E        +  where 401 = <Response [401 Unauthorized]>.status_code

tests\integration\test_anthropic_frontend_integration.py:102: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:33.610748Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:33.610748Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:33.618733Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:33.638232Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for POST /anthropic/v1/messages from client testclient
__ TestAnthropicFrontendIntegration.test_messages_endpoint_streaming_request __

self = <tests.integration.test_anthropic_frontend_integration.TestAnthropicFrontendIntegration object at 0x00000150F5148F10>

    def test_messages_endpoint_streaming_request(self):
        """Test messages endpoint with streaming enabled."""
        request_data = {
            "model": "claude-3-haiku-20240307",
            "messages": [{"role": "user", "content": "Tell me a short story."}],
            "max_tokens": 200,
            "stream": True,
        }
    
        response = self.client.post("/anthropic/v1/messages", json=request_data)
>       assert response.status_code == 501  # Not yet implemented
E       assert 401 == 501
E        +  where 401 = <Response [401 Unauthorized]>.status_code

tests\integration\test_anthropic_frontend_integration.py:114: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:33.649232Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:33.649232Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:33.656733Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:33.669233Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for POST /anthropic/v1/messages from client testclient
_ TestAnthropicFrontendIntegration.test_messages_endpoint_with_stop_sequences _

self = <tests.integration.test_anthropic_frontend_integration.TestAnthropicFrontendIntegration object at 0x00000150F5148910>

    def test_messages_endpoint_with_stop_sequences(self):
        """Test messages endpoint with stop sequences."""
        request_data = {
            "model": "claude-3-sonnet-20240229",
            "messages": [{"role": "user", "content": "Count from 1 to 10"}],
            "max_tokens": 100,
            "stop_sequences": ["5", "STOP"],
        }
    
        response = self.client.post("/anthropic/v1/messages", json=request_data)
>       assert response.status_code == 501  # Not yet implemented
E       assert 401 == 501
E        +  where 401 = <Response [401 Unauthorized]>.status_code

tests\integration\test_anthropic_frontend_integration.py:126: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:33.679234Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:33.679234Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:33.686736Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:33.704234Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for POST /anthropic/v1/messages from client testclient
______ TestAnthropicFrontendIntegration.test_conversation_flow_via_http _______

self = <tests.integration.test_anthropic_frontend_integration.TestAnthropicFrontendIntegration object at 0x00000150F5149240>

    def test_conversation_flow_via_http(self):
        """Test multi-turn conversation via HTTP."""
        request_data = {
            "model": "claude-3-sonnet-20240229",
            "messages": [
                {"role": "user", "content": "What is the capital of France?"},
                {"role": "assistant", "content": "The capital of France is Paris."},
                {"role": "user", "content": "What about Italy?"},
            ],
            "max_tokens": 50,
        }
    
        response = self.client.post("/anthropic/v1/messages", json=request_data)
>       assert response.status_code == 501  # Not yet implemented
E       assert 401 == 501
E        +  where 401 = <Response [401 Unauthorized]>.status_code

tests\integration\test_anthropic_frontend_integration.py:141: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:33.714734Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:33.715234Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:33.723734Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:33.802234Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for POST /anthropic/v1/messages from client testclient
_____ TestAnthropicFrontendIntegration.test_error_handling_invalid_model ______

self = <tests.integration.test_anthropic_frontend_integration.TestAnthropicFrontendIntegration object at 0x00000150F5149420>

    def test_error_handling_invalid_model(self):
        """Test error handling for invalid model."""
        request_data = {
            "model": "invalid-model-name",
            "messages": [{"role": "user", "content": "Hello"}],
            "max_tokens": 100,
        }
    
        response = self.client.post("/anthropic/v1/messages", json=request_data)
        # Should validate but still return 501 for now
>       assert response.status_code == 501
E       assert 401 == 501
E        +  where 401 = <Response [401 Unauthorized]>.status_code

tests\integration\test_anthropic_frontend_integration.py:153: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:33.813236Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:33.813236Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:33.820735Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:33.835731Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for POST /anthropic/v1/messages from client testclient
_ TestAnthropicFrontendIntegration.test_error_handling_missing_required_fields _

self = <tests.integration.test_anthropic_frontend_integration.TestAnthropicFrontendIntegration object at 0x00000150F5149600>

    def test_error_handling_missing_required_fields(self):
        """Test error handling for missing required fields."""
        # Missing messages
        response = self.client.post(
            "/anthropic/v1/messages",
            json={"model": "claude-3-sonnet-20240229", "max_tokens": 100},
        )
>       assert response.status_code == 422  # Validation error
E       assert 401 == 422
E        +  where 401 = <Response [401 Unauthorized]>.status_code

tests\integration\test_anthropic_frontend_integration.py:162: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:33.845748Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:33.845748Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:33.854732Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:33.872736Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for POST /anthropic/v1/messages from client testclient
______ TestAnthropicFrontendIntegration.test_parameter_validation_ranges ______

self = <tests.integration.test_anthropic_frontend_integration.TestAnthropicFrontendIntegration object at 0x00000150F51497E0>

    def test_parameter_validation_ranges(self):
        """Test parameter validation for ranges."""
        # Temperature out of range
        request_data = {
            "model": "claude-3-sonnet-20240229",
            "messages": [{"role": "user", "content": "Hello"}],
            "max_tokens": 100,
            "temperature": 2.5,  # Should be 0-1
        }
    
        response = self.client.post("/anthropic/v1/messages", json=request_data)
        # Pydantic should validate this, but let's see current behavior
>       assert response.status_code in [422, 501]
E       assert 401 in [422, 501]
E        +  where 401 = <Response [401 Unauthorized]>.status_code

tests\integration\test_anthropic_frontend_integration.py:186: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:33.891735Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:33.892234Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:33.903239Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:33.928235Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for POST /anthropic/v1/messages from client testclient
_______ TestAnthropicFrontendIntegration.test_health_and_info_endpoints _______

self = <tests.integration.test_anthropic_frontend_integration.TestAnthropicFrontendIntegration object at 0x00000150F51499C0>

    def test_health_and_info_endpoints(self):
        """Test health and info endpoints."""
        # Health check
        response = self.client.get("/anthropic/health")
>       assert response.status_code == 200
E       assert 401 == 200
E        +  where 401 = <Response [401 Unauthorized]>.status_code

tests\integration\test_anthropic_frontend_integration.py:192: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:33.941243Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:33.941741Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:33.952236Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:33.972236Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for GET /anthropic/health from client testclient
__________ TestAnthropicFrontendIntegration.test_concurrent_requests __________

self = <tests.integration.test_anthropic_frontend_integration.TestAnthropicFrontendIntegration object at 0x00000150F5149E10>

    def test_concurrent_requests(self):
        """Test handling of concurrent requests."""
        import threading
    
        results = []
    
        def make_request():
            response = self.client.get("/anthropic/v1/models")
            results.append(response.status_code)
    
        # Create multiple threads
        threads = []
        for _ in range(5):
            thread = threading.Thread(target=make_request)
            threads.append(thread)
            thread.start()
    
        # Wait for all threads to complete
        for thread in threads:
            thread.join()
    
        # All requests should succeed
>       assert all(status == 200 for status in results)
E       assert False
E        +  where False = all(<generator object TestAnthropicFrontendIntegration.test_concurrent_requests.<locals>.<genexpr> at 0x00000150F70D5AF0>)

tests\integration\test_anthropic_frontend_integration.py:235: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:34.047735Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:34.048234Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:34.058739Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:34.078241Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for GET /anthropic/v1/models from client testclient
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for GET /anthropic/v1/models from client testclient
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for GET /anthropic/v1/models from client testclient
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for GET /anthropic/v1/models from client testclient
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for GET /anthropic/v1/models from client testclient
________ TestAnthropicFrontendIntegration.test_large_payload_handling _________

self = <tests.integration.test_anthropic_frontend_integration.TestAnthropicFrontendIntegration object at 0x00000150F5149FF0>

    def test_large_payload_handling(self):
        """Test handling of large payloads."""
        # Large message content
        large_content = "This is a test message. " * 1000  # ~24KB
    
        request_data = {
            "model": "claude-3-sonnet-20240229",
            "messages": [{"role": "user", "content": large_content}],
            "max_tokens": 100,
        }
    
        response = self.client.post("/anthropic/v1/messages", json=request_data)
        # Should handle large payloads and still return 501
>       assert response.status_code == 501
E       assert 401 == 501
E        +  where 401 = <Response [401 Unauthorized]>.status_code

tests\integration\test_anthropic_frontend_integration.py:251: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:34.102734Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:34.103234Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:34.111233Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:34.123233Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for POST /anthropic/v1/messages from client testclient
____ TestAnthropicFrontendIntegration.test_unicode_and_special_characters _____

self = <tests.integration.test_anthropic_frontend_integration.TestAnthropicFrontendIntegration object at 0x00000150F514A1D0>

    def test_unicode_and_special_characters(self):
        """Test handling of Unicode and special characters."""
        request_data = {
            "model": "claude-3-sonnet-20240229",
            "messages": [
                {"role": "user", "content": "Hello \u4e16\u754c! \U0001f30d Caf\xe9 na\xefve r\xe9sum\xe9"}
            ],
            "max_tokens": 100,
        }
    
        response = self.client.post("/anthropic/v1/messages", json=request_data)
        # Should handle Unicode properly
>       assert response.status_code == 501
E       assert 401 == 501
E        +  where 401 = <Response [401 Unauthorized]>.status_code

tests\integration\test_anthropic_frontend_integration.py:265: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:34.134733Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:34.134733Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:34.142233Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:34.155233Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for POST /anthropic/v1/messages from client testclient
_________ TestAnthropicFrontendIntegration.test_content_type_headers __________

self = <tests.integration.test_anthropic_frontend_integration.TestAnthropicFrontendIntegration object at 0x00000150F514A3B0>

    def test_content_type_headers(self):
        """Test proper content type headers."""
        response = self.client.get("/anthropic/v1/models")
>       assert response.status_code == 200
E       assert 401 == 200
E        +  where 401 = <Response [401 Unauthorized]>.status_code

tests\integration\test_anthropic_frontend_integration.py:270: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:34.166733Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:34.166733Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:34.174731Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:34.191735Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for GET /anthropic/v1/models from client testclient
____ TestAnthropicFrontendIntegration.test_anthropic_specific_model_names _____

self = <tests.integration.test_anthropic_frontend_integration.TestAnthropicFrontendIntegration object at 0x00000150F514A590>

    def test_anthropic_specific_model_names(self):
        """Test that Anthropic-specific model names are handled."""
        models_response = self.client.get("/anthropic/v1/models")
>       assert models_response.status_code == 200
E       assert 401 == 200
E        +  where 401 = <Response [401 Unauthorized]>.status_code

tests\integration\test_anthropic_frontend_integration.py:276: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:34.205258Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:34.205736Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:34.216233Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:34.233737Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for GET /anthropic/v1/models from client testclient
__________ TestAnthropicFrontendIntegration.test_endpoint_not_found ___________

self = <tests.integration.test_anthropic_frontend_integration.TestAnthropicFrontendIntegration object at 0x00000150F514A770>

    def test_endpoint_not_found(self):
        """Test that non-existent endpoints return 404."""
        response = self.client.get("/anthropic/v1/nonexistent")
>       assert response.status_code == 404
E       assert 401 == 404
E        +  where 401 = <Response [401 Unauthorized]>.status_code

tests\integration\test_anthropic_frontend_integration.py:295: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:34.247233Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:34.247233Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:34.257733Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:34.277733Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for GET /anthropic/v1/nonexistent from client testclient
__________ TestAnthropicFrontendIntegration.test_method_not_allowed ___________

self = <tests.integration.test_anthropic_frontend_integration.TestAnthropicFrontendIntegration object at 0x00000150F514A950>

    def test_method_not_allowed(self):
        """Test that wrong HTTP methods return 405."""
        # GET on messages endpoint (should be POST)
        response = self.client.get("/anthropic/v1/messages")
>       assert response.status_code == 405
E       assert 401 == 405
E        +  where 401 = <Response [401 Unauthorized]>.status_code

tests\integration\test_anthropic_frontend_integration.py:304: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:34.290244Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:34.290244Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:34.300237Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:34.319741Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for GET /anthropic/v1/messages from client testclient
____ TestClineCommandResponses.test_cline_hello_command_returns_tool_calls ____

self = <tests.integration.test_cline_tool_call_implementation.TestClineCommandResponses object at 0x00000150F5174130>
client = <starlette.testclient.TestClient object at 0x00000150F9B96260>

    def test_cline_hello_command_returns_tool_calls(self, client):
        """Test that !/hello command returns tool calls for Cline agents."""
    
        # Step 1: Establish Cline agent
        response1 = client.post(
            "/v1/chat/completions",
            json={
                "model": "gpt-4",
                "messages": [
                    {
                        "role": "user",
                        "content": "I am a Cline agent. <attempt_completion>test</attempt_completion>",
                    }
                ],
            },
            headers={"Authorization": "Bearer test-proxy-key"},
        )
    
>       assert response1.status_code == 200
E       assert 500 == 200
E        +  where 500 = <Response [500 Internal Server Error]>.status_code

tests\integration\test_cline_tool_call_implementation.py:60: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:34.465233Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:34.465233Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:34.472732Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:34.485733Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openai: {}
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
ERROR    src.core.services.request_processor:request_processor.py:259 Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 79, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 85, in _handle_non_streaming_response
    raise HTTPException(
fastapi.exceptions.HTTPException: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 215, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
_____ TestClineCommandResponses.test_cline_set_command_returns_tool_calls _____

self = <tests.integration.test_cline_tool_call_implementation.TestClineCommandResponses object at 0x00000150F5174310>
client = <starlette.testclient.TestClient object at 0x00000150F8B1A0E0>

    def test_cline_set_command_returns_tool_calls(self, client):
        """Test that !/set command returns tool calls for Cline agents."""
    
        # Establish Cline agent
        response1 = client.post(
            "/v1/chat/completions",
            json={
                "model": "gpt-4",
                "messages": [
                    {
                        "role": "user",
                        "content": "I am a Cline agent. <attempt_completion>test</attempt_completion>",
                    }
                ],
            },
            headers={"Authorization": "Bearer test-proxy-key"},
        )
    
>       assert response1.status_code == 200
E       assert 500 == 200
E        +  where 500 = <Response [500 Internal Server Error]>.status_code

tests\integration\test_cline_tool_call_implementation.py:116: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:34.499744Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:34.499744Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:34.506733Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:34.519733Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openai: {}
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
ERROR    src.core.services.request_processor:request_processor.py:259 Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 79, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 85, in _handle_non_streaming_response
    raise HTTPException(
fastapi.exceptions.HTTPException: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 215, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
___________________ test_loop_detection_with_mocked_backend ___________________

    @pytest.mark.asyncio
    async def test_loop_detection_with_mocked_backend():
        """Test loop detection with a mocked backend."""
    
        from src.core.app.application_factory import build_app
    
        # Import the config loader to patch it before building the app
        from src.core.config.config_loader import _load_config
    
        # Mock the config loader to include disable_auth during app build
        def mock_load_config():
            config = _load_config()
            config["disable_auth"] = True
            return config
    
>       with patch(
            "src.core.app.application_factory._load_config", side_effect=mock_load_config
        ):

tests\integration\test_end_to_end_loop_detection.py:58: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x00000150F9C726B0>

    def __enter__(self):
        """Perform the patch."""
        new, spec, spec_set = self.new, self.spec, self.spec_set
        autospec, kwargs = self.autospec, self.kwargs
        new_callable = self.new_callable
        self.target = self.getter()
    
        # normalise False to None
        if spec is False:
            spec = None
        if spec_set is False:
            spec_set = None
        if autospec is False:
            autospec = None
    
        if spec is not None and autospec is not None:
            raise TypeError("Can't specify spec and autospec")
        if ((spec is not None or autospec is not None) and
            spec_set not in (True, None)):
            raise TypeError("Can't provide explicit spec_set *and* spec or autospec")
    
>       original, local = self.get_original()

C:\Program Files\Python310\lib\unittest\mock.py:1447: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x00000150F9C726B0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'src.core.app.application_factory' from 'C:\\Users\\Mateusz\\source\\repos\\llm-interactive-proxy\\src\\core\\app\\application_factory.py'> does not have the attribute '_load_config'

C:\Program Files\Python310\lib\unittest\mock.py:1420: AttributeError
__________________ test_loop_detection_in_streaming_response __________________

    @pytest.mark.asyncio
    async def test_loop_detection_in_streaming_response():
        """Test loop detection in a streaming response."""
        from src.core.app.application_factory import build_app
    
        # Import the config loader to patch it before building the app
        from src.core.config.config_loader import _load_config
        from src.core.domain.chat import StreamingChatResponse
    
        # Mock the config loader to include disable_auth during app build
        def mock_load_config():
            config = _load_config()
            config["disable_auth"] = True
            return config
    
>       with patch(
            "src.core.app.application_factory._load_config", side_effect=mock_load_config
        ):

tests\integration\test_end_to_end_loop_detection.py:152: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x00000150F9BEB730>

    def __enter__(self):
        """Perform the patch."""
        new, spec, spec_set = self.new, self.spec, self.spec_set
        autospec, kwargs = self.autospec, self.kwargs
        new_callable = self.new_callable
        self.target = self.getter()
    
        # normalise False to None
        if spec is False:
            spec = None
        if spec_set is False:
            spec_set = None
        if autospec is False:
            autospec = None
    
        if spec is not None and autospec is not None:
            raise TypeError("Can't specify spec and autospec")
        if ((spec is not None or autospec is not None) and
            spec_set not in (True, None)):
            raise TypeError("Can't provide explicit spec_set *and* spec or autospec")
    
>       original, local = self.get_original()

C:\Program Files\Python310\lib\unittest\mock.py:1447: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x00000150F9BEB730>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'src.core.app.application_factory' from 'C:\\Users\\Mateusz\\source\\repos\\llm-interactive-proxy\\src\\core\\app\\application_factory.py'> does not have the attribute '_load_config'

C:\Program Files\Python310\lib\unittest\mock.py:1420: AttributeError
________________________ test_failover_route_commands _________________________

app = <fastapi.applications.FastAPI object at 0x00000150F9C0B100>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x00000150F9C0A410>

    def test_failover_route_commands(app, monkeypatch):
        """Test failover route commands in the new architecture."""
        # Mock the APIKeyMiddleware's dispatch method to always return the next response
    
        # Patch the get_integration_bridge function to return the bridge from app.state
        def mock_get_integration_bridge(app_param=None):
            return app.state.integration_bridge
    
        async def mock_dispatch(self, request, call_next):
            return await call_next(request)
    
        with (
            patch(
                "src.core.integration.bridge.get_integration_bridge",
                new=mock_get_integration_bridge,
            ),
            patch(
                "src.core.security.middleware.APIKeyMiddleware.dispatch", new=mock_dispatch
            ),
        ):
            # Create a test client
            client = TestClient(app)
    
            # Create a new failover route
            response = client.post(
                "/v2/chat/completions",
                json={
                    "model": "gpt-3.5-turbo",
                    "messages": [
                        {
                            "role": "user",
                            "content": "!/create-failover-route(name=test-route,policy=k)",
                        }
                    ],
                    "session_id": "test-failover-session",
                },
            )
    
>           assert response.status_code == 200
E           assert 500 == 200
E            +  where 500 = <Response [500 Internal Server Error]>.status_code

tests\integration\test_failover_routes.py:66: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:34.919734Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:34.919734Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:40 2025-08-17T00:00:34.927734Z [info     ] API Key authentication is disabled [src.core.app.middleware_config]
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:34.940734Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
INFO     src.core.app.controllers.chat_controller:chat_controller.py:59 Handling chat completion request: model=gpt-3.5-turbo
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openai: {}
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
ERROR    src.core.services.request_processor:request_processor.py:259 Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 79, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 85, in _handle_non_streaming_response
    raise HTTPException(
fastapi.exceptions.HTTPException: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 215, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
_ TestLoopDetectionIntegration.test_loop_detection_initialization_on_startup __

self = <tests.integration.test_loop_detection_integration.TestLoopDetectionIntegration object at 0x00000150F7000100>

    def test_loop_detection_initialization_on_startup(self):
        """Test that loop detection is properly initialized during app startup."""
        # Set environment variables for loop detection
        import os
    
        os.environ["LOOP_DETECTION_ENABLED"] = "true"
        os.environ["LOOP_DETECTION_BUFFER_SIZE"] = "1024"
        os.environ["LOOP_DETECTION_MAX_PATTERN_LENGTH"] = "100"
    
        # Build app with loop detection enabled
        app = build_app()
    
        # Use TestClient to trigger lifespan events
        with TestClient(app):
            # Check that middleware was configured
            middleware = get_response_middleware()
>           assert len(middleware.middleware_stack) > 0
E           assert 0 > 0
E            +  where 0 = len([])
E            +    where [] = <src.response_middleware.ResponseMiddleware object at 0x00000150F7002AD0>.middleware_stack

tests\integration\test_loop_detection_integration.py:39: AssertionError
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:35.003737Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:35.003737Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:35.011233Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:35.023734Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:354 2025-08-17T00:00:35.025736Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: help
INFO     src.core.services.command_service:command_service.py:78 Registered command: backend
INFO     src.core.services.command_service:command_service.py:78 Registered command: model
INFO     src.core.services.command_service:command_service.py:78 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:78 Registered command: project
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:78 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:78 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:78 Registered command: set
INFO     src.core.services.command_service:command_service.py:78 Registered command: unset
INFO     src.core.services.command_service:command_service.py:78 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:78 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:78 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:78 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:78 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:269 Registered 26 command handlers with registry
INFO     src.core.app.application_factory:application_factory.py:368 2025-08-17T00:00:35.035734Z [info     ] Shutting down application      [src.core.app.application_factory]
____ TestLoopDetectionIntegration.test_environment_variable_configuration _____

self = <tests.integration.test_loop_detection_integration.TestLoopDetectionIntegration object at 0x00000150F7003310>

    def test_environment_variable_configuration(self):
        """Test that loop detection respects environment variables."""
        import os
    
        # Set environment variables
        env_vars = {
            "LOOP_DETECTION_ENABLED": "false",
            "LOOP_DETECTION_BUFFER_SIZE": "512",
            "LOOP_DETECTION_MAX_PATTERN_LENGTH": "100",
        }
    
        with patch.dict(os.environ, env_vars):
            from src.core.config.config_loader import _load_config
    
            base_config = _load_config()
            config = {
                **base_config,
                "backend": "gemini",  # Use gemini backend
                "gemini_api_keys": {"test_key": "test_value"},  # Add dummy API key
            }
            app = build_app(config=config)
    
            with TestClient(app):
                middleware = get_response_middleware()
    
                # Should be disabled
                from src.response_middleware import LoopDetectionProcessor
    
                loop_processors = [
                    p
                    for p in middleware.middleware_stack
                    if isinstance(p, LoopDetectionProcessor)
                ]
>               assert len(loop_processors) == 0
E               assert 1 == 0
E                +  where 1 = len([<src.response_middleware.LoopDetectionProcessor object at 0x00000150F9BFA4A0>])

tests\integration\test_loop_detection_integration.py:209: AssertionError
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:35.082733Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:35.082733Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:35.089733Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:35.102734Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:354 2025-08-17T00:00:35.104238Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: help
INFO     src.core.services.command_service:command_service.py:78 Registered command: backend
INFO     src.core.services.command_service:command_service.py:78 Registered command: model
INFO     src.core.services.command_service:command_service.py:78 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:78 Registered command: project
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:78 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:78 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:78 Registered command: set
INFO     src.core.services.command_service:command_service.py:78 Registered command: unset
INFO     src.core.services.command_service:command_service.py:78 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:78 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:78 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:78 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:78 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:269 Registered 26 command handlers with registry
INFO     src.core.app.application_factory:application_factory.py:368 2025-08-17T00:00:35.113735Z [info     ] Shutting down application      [src.core.app.application_factory]
________________________ test_end_to_end_with_real_app ________________________

    @pytest.mark.asyncio
    async def test_end_to_end_with_real_app():
        """Test the complete end-to-end flow with a real FastAPI app."""
        from fastapi.testclient import TestClient
        from src.core.app.application_factory import build_app
        from src.core.interfaces.backend_service import IBackendService
    
        # Store original _load_config before patching
        original_load_config = src.core.config.config_loader._load_config
    
        # Set environment variables to use the new architecture
        os.environ["USE_NEW_BACKEND_SERVICE"] = "true"
        os.environ["USE_NEW_SESSION_SERVICE"] = "true"
        os.environ["USE_NEW_COMMAND_SERVICE"] = "true"
        os.environ["USE_NEW_REQUEST_PROCESSOR"] = "true"
    
        try:
            # Set proxy API key for the test
            os.environ["LLM_INTERACTIVE_PROXY_API_KEY"] = "test-proxy-key"
    
            # Patch _load_config to include the API key in the config
>           with patch("src.core.app.application_factory._load_config") as mock_load_config:

tests\integration\test_loop_detection_middleware.py:283: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x00000150F9B948B0>

    def __enter__(self):
        """Perform the patch."""
        new, spec, spec_set = self.new, self.spec, self.spec_set
        autospec, kwargs = self.autospec, self.kwargs
        new_callable = self.new_callable
        self.target = self.getter()
    
        # normalise False to None
        if spec is False:
            spec = None
        if spec_set is False:
            spec_set = None
        if autospec is False:
            autospec = None
    
        if spec is not None and autospec is not None:
            raise TypeError("Can't specify spec and autospec")
        if ((spec is not None or autospec is not None) and
            spec_set not in (True, None)):
            raise TypeError("Can't provide explicit spec_set *and* spec or autospec")
    
>       original, local = self.get_original()

C:\Program Files\Python310\lib\unittest\mock.py:1447: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x00000150F9B948B0>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'src.core.app.application_factory' from 'C:\\Users\\Mateusz\\source\\repos\\llm-interactive-proxy\\src\\core\\app\\application_factory.py'> does not have the attribute '_load_config'

C:\Program Files\Python310\lib\unittest\mock.py:1420: AttributeError
_____________ TestModelsEndpoints.test_models_endpoint_with_auth ______________

self = <tests.integration.test_models_endpoints.TestModelsEndpoints object at 0x00000150F7031630>
app_with_auth_enabled = <fastapi.applications.FastAPI object at 0x00000150F8AA8BE0>

    def test_models_endpoint_with_auth(self, app_with_auth_enabled):
        """Test /models endpoint with authentication."""
        with TestClient(app_with_auth_enabled) as client:
            # Without auth - should fail
            response = client.get("/models")
            assert response.status_code == 401
    
            # With valid auth
            response = client.get(
                "/models", headers={"Authorization": "Bearer test-key-123"}
            )
>           assert response.status_code == 200
E           assert 401 == 200
E            +  where 401 = <Response [401 Unauthorized]>.status_code

tests\integration\test_models_endpoints.py:69: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:35.331234Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:35.331234Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:35.338733Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:35.352233Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
INFO     src.core.app.application_factory:application_factory.py:354 2025-08-17T00:00:35.354234Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: help
INFO     src.core.services.command_service:command_service.py:78 Registered command: backend
INFO     src.core.services.command_service:command_service.py:78 Registered command: model
INFO     src.core.services.command_service:command_service.py:78 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:78 Registered command: project
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:78 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:78 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:78 Registered command: set
INFO     src.core.services.command_service:command_service.py:78 Registered command: unset
INFO     src.core.services.command_service:command_service.py:78 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:78 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:78 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:78 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:78 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:269 Registered 26 command handlers with registry
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for GET /models from client testclient
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for GET /models from client testclient
INFO     src.core.app.application_factory:application_factory.py:368 2025-08-17T00:00:35.366235Z [info     ] Shutting down application      [src.core.app.application_factory]
__________ TestModelsEndpoints.test_models_with_configured_backends ___________
  + Exception Group Traceback (most recent call last):
  |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 77, in collapse_excgroups
  |     yield
  |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 183, in __call__
  |     async with anyio.create_task_group() as task_group:
  |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\anyio\_backends\_asyncio.py", line 772, in __aexit__
  |     raise BaseExceptionGroup(
  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)
  +-+---------------- 1 ----------------
    | Traceback (most recent call last):
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\_pytest\runner.py", line 344, in from_call
    |     result: TResult | None = func()
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\_pytest\runner.py", line 246, in <lambda>
    |     lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_hooks.py", line 512, in __call__
    |     return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_manager.py", line 120, in _hookexec
    |     return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 167, in _multicall
    |     raise exception
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 139, in _multicall
    |     teardown.throw(exception)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\_pytest\logging.py", line 850, in pytest_runtest_call
    |     yield
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 139, in _multicall
    |     teardown.throw(exception)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\_pytest\capture.py", line 900, in pytest_runtest_call
    |     return (yield)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 139, in _multicall
    |     teardown.throw(exception)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\_pytest\skipping.py", line 263, in pytest_runtest_call
    |     return (yield)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 121, in _multicall
    |     res = hook_impl.function(*args)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\_pytest\runner.py", line 178, in pytest_runtest_call
    |     item.runtest()
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\_pytest\python.py", line 1671, in runtest
    |     self.ihook.pytest_pyfunc_call(pyfuncitem=self)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_hooks.py", line 512, in __call__
    |     return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_manager.py", line 120, in _hookexec
    |     return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 167, in _multicall
    |     raise exception
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 139, in _multicall
    |     teardown.throw(exception)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 53, in run_old_style_hookwrapper
    |     return result.get_result()
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_result.py", line 103, in get_result
    |     raise exc.with_traceback(tb)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 38, in run_old_style_hookwrapper
    |     res = yield
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\pluggy\_callers.py", line 121, in _multicall
    |     res = hook_impl.function(*args)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\_pytest\python.py", line 157, in pytest_pyfunc_call
    |     result = testfunction(**testargs)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\tests\integration\test_models_endpoints.py", line 127, in test_models_with_configured_backends
    |     response = client.get("/models")
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\testclient.py", line 479, in get
    |     return super().get(
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1053, in get
    |     return self.request(
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\testclient.py", line 451, in request
    |     return super().request(
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 825, in request
    |     return self.send(request, auth=auth, follow_redirects=follow_redirects)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 914, in send
    |     response = self._send_handling_auth(
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    |     response = self._send_handling_redirects(
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    |     response = self._send_single_request(request)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1014, in _send_single_request
    |     response = transport.handle_request(request)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\testclient.py", line 354, in handle_request
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\testclient.py", line 351, in handle_request
    |     portal.call(self.app, scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\anyio\from_thread.py", line 291, in call
    |     return cast(T_Retval, self.start_task_soon(func, *args).result())
    |   File "C:\Program Files\Python310\lib\concurrent\futures\_base.py", line 458, in result
    |     return self.__get_result()
    |   File "C:\Program Files\Python310\lib\concurrent\futures\_base.py", line 403, in __get_result
    |     raise self._exception
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\anyio\from_thread.py", line 222, in _call_func
    |     retval = await retval_or_awaitable
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\applications.py", line 1054, in __call__
    |     await super().__call__(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\applications.py", line 113, in __call__
    |     await self.middleware_stack(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\errors.py", line 186, in __call__
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\errors.py", line 164, in __call__
    |     await self.app(scope, receive, _send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 182, in __call__
    |     with recv_stream, send_stream, collapse_excgroups():
    |   File "C:\Program Files\Python310\lib\contextlib.py", line 153, in __exit__
    |     self.gen.throw(typ, value, traceback)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 83, in collapse_excgroups
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 184, in __call__
    |     response = await self.dispatch_func(request, call_next)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\response_middleware.py", line 437, in dispatch
    |     response = await call_next(request)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 159, in call_next
    |     raise app_exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 144, in coro
    |     await self.app(scope, receive_or_disconnect, send_no_error)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 182, in __call__
    |     with recv_stream, send_stream, collapse_excgroups():
    |   File "C:\Program Files\Python310\lib\contextlib.py", line 153, in __exit__
    |     self.gen.throw(typ, value, traceback)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 83, in collapse_excgroups
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 184, in __call__
    |     response = await self.dispatch_func(request, call_next)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\request_middleware.py", line 210, in dispatch
    |     response = await call_next(request)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 159, in call_next
    |     raise app_exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 144, in coro
    |     await self.app(scope, receive_or_disconnect, send_no_error)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\cors.py", line 85, in __call__
    |     await self.app(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\exceptions.py", line 63, in __call__
    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    |     await app(scope, receive, sender)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 716, in __call__
    |     await self.middleware_stack(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 736, in app
    |     await route.handle(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 290, in handle
    |     await self.app(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 78, in app
    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    |     await app(scope, receive, sender)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 75, in app
    |     response = await f(request)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\routing.py", line 292, in app
    |     solved_result = await solve_dependencies(
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\dependencies\utils.py", line 638, in solve_dependencies
    |     solved = await call(**solved_result.values)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\app\controllers\models_controller.py", line 32, in get_backend_service
    |     return service_provider.get_required_service(IBackendService)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\di\container.py", line 140, in get_required_service
    |     raise KeyError(f"No service registered for {service_type.__name__}")
    |   File "C:\Program Files\Python310\lib\unittest\mock.py", line 645, in __getattr__
    |     raise AttributeError(name)
    | AttributeError: __name__. Did you mean: '__hash__'?
    +------------------------------------

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\tests\integration\test_models_endpoints.py", line 127, in test_models_with_configured_backends
    response = client.get("/models")
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\testclient.py", line 479, in get
    return super().get(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1053, in get
    return self.request(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\testclient.py", line 451, in request
    return super().request(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 825, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\httpx\_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\testclient.py", line 354, in handle_request
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\testclient.py", line 351, in handle_request
    portal.call(self.app, scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\anyio\from_thread.py", line 291, in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
  File "C:\Program Files\Python310\lib\concurrent\futures\_base.py", line 458, in result
    return self.__get_result()
  File "C:\Program Files\Python310\lib\concurrent\futures\_base.py", line 403, in __get_result
    raise self._exception
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\anyio\from_thread.py", line 222, in _call_func
    retval = await retval_or_awaitable
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\applications.py", line 1054, in __call__
    await super().__call__(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\applications.py", line 113, in __call__
    await self.middleware_stack(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\errors.py", line 186, in __call__
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 182, in __call__
    with recv_stream, send_stream, collapse_excgroups():
  File "C:\Program Files\Python310\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 83, in collapse_excgroups
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 184, in __call__
    response = await self.dispatch_func(request, call_next)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\response_middleware.py", line 437, in dispatch
    response = await call_next(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 159, in call_next
    raise app_exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 144, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 182, in __call__
    with recv_stream, send_stream, collapse_excgroups():
  File "C:\Program Files\Python310\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 83, in collapse_excgroups
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 184, in __call__
    response = await self.dispatch_func(request, call_next)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\request_middleware.py", line 210, in dispatch
    response = await call_next(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 159, in call_next
    raise app_exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 144, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\cors.py", line 85, in __call__
    await self.app(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 78, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 75, in app
    response = await f(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\routing.py", line 292, in app
    solved_result = await solve_dependencies(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\dependencies\utils.py", line 638, in solve_dependencies
    solved = await call(**solved_result.values)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\app\controllers\models_controller.py", line 32, in get_backend_service
    return service_provider.get_required_service(IBackendService)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\di\container.py", line 140, in get_required_service
    raise KeyError(f"No service registered for {service_type.__name__}")
  File "C:\Program Files\Python310\lib\unittest\mock.py", line 645, in __getattr__
    raise AttributeError(name)
AttributeError: __name__. Did you mean: '__hash__'?

During handling of the above exception, another exception occurred:

self = <tests.integration.test_models_endpoints.TestModelsEndpoints object at 0x00000150F7032EF0>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x00000150F8B19D50>

    def test_models_with_configured_backends(self, monkeypatch):
        """Test models discovery with configured backends."""
        # Set up environment with multiple backends
        monkeypatch.setenv("DISABLE_AUTH", "true")
        monkeypatch.setenv("OPENAI_API_KEY", "test-openai-key")
        monkeypatch.setenv("ANTHROPIC_API_KEY", "test-anthropic-key")
    
        app = build_app()
    
        with (
            TestClient(app) as client,
            patch(
                "src.core.app.controllers.models_controller.IBackendService"
            ) as mock_service_class,
        ):
            mock_backend_service = MagicMock()
    
            # Mock OpenAI backend
            mock_openai_backend = MagicMock()
            mock_openai_backend.get_available_models.return_value = [
                "gpt-4",
                "gpt-3.5-turbo",
            ]
    
            # Mock Anthropic backend
            mock_anthropic_backend = MagicMock()
            mock_anthropic_backend.get_available_models.return_value = [
                "claude-3-opus",
                "claude-3-sonnet",
            ]
    
            # Setup mock to return different backends
            async def mock_get_backend(backend_type):
                if backend_type == BackendType.OPENAI:
                    return mock_openai_backend
                elif backend_type == BackendType.ANTHROPIC:
                    return mock_anthropic_backend
                raise ValueError(f"Unknown backend: {backend_type}")
    
            mock_backend_service._get_or_create_backend = AsyncMock(
                side_effect=mock_get_backend
            )
    
            # This won't work directly, but shows the intent
            # In reality, the backends would be discovered through the actual service
>           response = client.get("/models")

tests\integration\test_models_endpoints.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient.TestClient object at 0x00000150F9CB71C0>
url = '/models'

    def get(  # type: ignore[override]
        self,
        url: httpx._types.URLTypes,
        *,
        params: httpx._types.QueryParamTypes | None = None,
        headers: httpx._types.HeaderTypes | None = None,
        cookies: httpx._types.CookieTypes | None = None,
        auth: httpx._types.AuthTypes | httpx._client.UseClientDefault = httpx._client.USE_CLIENT_DEFAULT,
        follow_redirects: bool | httpx._client.UseClientDefault = httpx._client.USE_CLIENT_DEFAULT,
        timeout: httpx._types.TimeoutTypes | httpx._client.UseClientDefault = httpx._client.USE_CLIENT_DEFAULT,
        extensions: dict[str, Any] | None = None,
    ) -> httpx.Response:
>       return super().get(
            url,
            params=params,
            headers=headers,
            cookies=cookies,
            auth=auth,
            follow_redirects=follow_redirects,
            timeout=timeout,
            extensions=extensions,
        )

.venv\lib\site-packages\starlette\testclient.py:479: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient.TestClient object at 0x00000150F9CB71C0>
url = '/models'

    def get(
        self,
        url: URL | str,
        *,
        params: QueryParamTypes | None = None,
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault | None = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> Response:
        """
        Send a `GET` request.
    
        **Parameters**: See `httpx.request`.
        """
>       return self.request(
            "GET",
            url,
            params=params,
            headers=headers,
            cookies=cookies,
            auth=auth,
            follow_redirects=follow_redirects,
            timeout=timeout,
            extensions=extensions,
        )

.venv\lib\site-packages\httpx\_client.py:1053: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient.TestClient object at 0x00000150F9CB71C0>
method = 'GET', url = URL('http://testserver/models')

    def request(  # type: ignore[override]
        self,
        method: str,
        url: httpx._types.URLTypes,
        *,
        content: httpx._types.RequestContent | None = None,
        data: _RequestData | None = None,
        files: httpx._types.RequestFiles | None = None,
        json: Any = None,
        params: httpx._types.QueryParamTypes | None = None,
        headers: httpx._types.HeaderTypes | None = None,
        cookies: httpx._types.CookieTypes | None = None,
        auth: httpx._types.AuthTypes | httpx._client.UseClientDefault = httpx._client.USE_CLIENT_DEFAULT,
        follow_redirects: bool | httpx._client.UseClientDefault = httpx._client.USE_CLIENT_DEFAULT,
        timeout: httpx._types.TimeoutTypes | httpx._client.UseClientDefault = httpx._client.USE_CLIENT_DEFAULT,
        extensions: dict[str, Any] | None = None,
    ) -> httpx.Response:
        if timeout is not httpx.USE_CLIENT_DEFAULT:
            warnings.warn(
                "You should not use the 'timeout' argument with the TestClient. "
                "See https://github.com/encode/starlette/issues/1108 for more information.",
                DeprecationWarning,
            )
        url = self._merge_url(url)
>       return super().request(
            method,
            url,
            content=content,
            data=data,
            files=files,
            json=json,
            params=params,
            headers=headers,
            cookies=cookies,
            auth=auth,
            follow_redirects=follow_redirects,
            timeout=timeout,
            extensions=extensions,
        )

.venv\lib\site-packages\starlette\testclient.py:451: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient.TestClient object at 0x00000150F9CB71C0>
method = 'GET', url = URL('http://testserver/models')

    def request(
        self,
        method: str,
        url: URL | str,
        *,
        content: RequestContent | None = None,
        data: RequestData | None = None,
        files: RequestFiles | None = None,
        json: typing.Any | None = None,
        params: QueryParamTypes | None = None,
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault | None = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> Response:
        """
        Build and send a request.
    
        Equivalent to:
    
        ```python
        request = client.build_request(...)
        response = client.send(request, ...)
        ```
    
        See `Client.build_request()`, `Client.send()` and
        [Merging of configuration][0] for how the various parameters
        are merged with client-level configuration.
    
        [0]: /advanced/clients/#merging-of-configuration
        """
        if cookies is not None:
            message = (
                "Setting per-request cookies=<...> is being deprecated, because "
                "the expected behaviour on cookie persistence is ambiguous. Set "
                "cookies directly on the client instance instead."
            )
            warnings.warn(message, DeprecationWarning, stacklevel=2)
    
        request = self.build_request(
            method=method,
            url=url,
            content=content,
            data=data,
            files=files,
            json=json,
            params=params,
            headers=headers,
            cookies=cookies,
            timeout=timeout,
            extensions=extensions,
        )
>       return self.send(request, auth=auth, follow_redirects=follow_redirects)

.venv\lib\site-packages\httpx\_client.py:825: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient.TestClient object at 0x00000150F9CB71C0>
request = <Request('GET', 'http://testserver/models')>

    def send(
        self,
        request: Request,
        *,
        stream: bool = False,
        auth: AuthTypes | UseClientDefault | None = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
    ) -> Response:
        """
        Send a request.
    
        The request is sent as-is, unmodified.
    
        Typically you'll want to build one with `Client.build_request()`
        so that any client-level configuration is merged into the request,
        but passing an explicit `httpx.Request()` is supported as well.
    
        See also: [Request instances][0]
    
        [0]: /advanced/clients/#request-instances
        """
        if self._state == ClientState.CLOSED:
            raise RuntimeError("Cannot send a request, as the client has been closed.")
    
        self._state = ClientState.OPENED
        follow_redirects = (
            self.follow_redirects
            if isinstance(follow_redirects, UseClientDefault)
            else follow_redirects
        )
    
        self._set_timeout(request)
    
        auth = self._build_request_auth(request, auth)
    
>       response = self._send_handling_auth(
            request,
            auth=auth,
            follow_redirects=follow_redirects,
            history=[],
        )

.venv\lib\site-packages\httpx\_client.py:914: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient.TestClient object at 0x00000150F9CB71C0>
request = <Request('GET', 'http://testserver/models')>
auth = <httpx.Auth object at 0x00000150F9B95EA0>, follow_redirects = True
history = []

    def _send_handling_auth(
        self,
        request: Request,
        auth: Auth,
        follow_redirects: bool,
        history: list[Response],
    ) -> Response:
        auth_flow = auth.sync_auth_flow(request)
        try:
            request = next(auth_flow)
    
            while True:
>               response = self._send_handling_redirects(
                    request,
                    follow_redirects=follow_redirects,
                    history=history,
                )

.venv\lib\site-packages\httpx\_client.py:942: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient.TestClient object at 0x00000150F9CB71C0>
request = <Request('GET', 'http://testserver/models')>, follow_redirects = True
history = []

    def _send_handling_redirects(
        self,
        request: Request,
        follow_redirects: bool,
        history: list[Response],
    ) -> Response:
        while True:
            if len(history) > self.max_redirects:
                raise TooManyRedirects(
                    "Exceeded maximum allowed redirects.", request=request
                )
    
            for hook in self._event_hooks["request"]:
                hook(request)
    
>           response = self._send_single_request(request)

.venv\lib\site-packages\httpx\_client.py:979: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient.TestClient object at 0x00000150F9CB71C0>
request = <Request('GET', 'http://testserver/models')>

    def _send_single_request(self, request: Request) -> Response:
        """
        Sends a single request, without handling any redirections.
        """
        transport = self._transport_for_url(request.url)
        start = time.perf_counter()
    
        if not isinstance(request.stream, SyncByteStream):
            raise RuntimeError(
                "Attempted to send an async request with a sync Client instance."
            )
    
        with request_context(request=request):
>           response = transport.handle_request(request)

.venv\lib\site-packages\httpx\_client.py:1014: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient._TestClientTransport object at 0x00000150F9CB46D0>
request = <Request('GET', 'http://testserver/models')>

    def handle_request(self, request: httpx.Request) -> httpx.Response:
        scheme = request.url.scheme
        netloc = request.url.netloc.decode(encoding="ascii")
        path = request.url.path
        raw_path = request.url.raw_path
        query = request.url.query.decode(encoding="ascii")
    
        default_port = {"http": 80, "ws": 80, "https": 443, "wss": 443}[scheme]
    
        if ":" in netloc:
            host, port_string = netloc.split(":", 1)
            port = int(port_string)
        else:
            host = netloc
            port = default_port
    
        # Include the 'host' header.
        if "host" in request.headers:
            headers: list[tuple[bytes, bytes]] = []
        elif port == default_port:  # pragma: no cover
            headers = [(b"host", host.encode())]
        else:  # pragma: no cover
            headers = [(b"host", (f"{host}:{port}").encode())]
    
        # Include other request headers.
        headers += [(key.lower().encode(), value.encode()) for key, value in request.headers.multi_items()]
    
        scope: dict[str, Any]
    
        if scheme in {"ws", "wss"}:
            subprotocol = request.headers.get("sec-websocket-protocol", None)
            if subprotocol is None:
                subprotocols: Sequence[str] = []
            else:
                subprotocols = [value.strip() for value in subprotocol.split(",")]
            scope = {
                "type": "websocket",
                "path": unquote(path),
                "raw_path": raw_path.split(b"?", 1)[0],
                "root_path": self.root_path,
                "scheme": scheme,
                "query_string": query.encode(),
                "headers": headers,
                "client": self.client,
                "server": [host, port],
                "subprotocols": subprotocols,
                "state": self.app_state.copy(),
                "extensions": {"websocket.http.response": {}},
            }
            session = WebSocketTestSession(self.app, scope, self.portal_factory)
            raise _Upgrade(session)
    
        scope = {
            "type": "http",
            "http_version": "1.1",
            "method": request.method,
            "path": unquote(path),
            "raw_path": raw_path.split(b"?", 1)[0],
            "root_path": self.root_path,
            "scheme": scheme,
            "query_string": query.encode(),
            "headers": headers,
            "client": self.client,
            "server": [host, port],
            "extensions": {"http.response.debug": {}},
            "state": self.app_state.copy(),
        }
    
        request_complete = False
        response_started = False
        response_complete: anyio.Event
        raw_kwargs: dict[str, Any] = {"stream": io.BytesIO()}
        template = None
        context = None
    
        async def receive() -> Message:
            nonlocal request_complete
    
            if request_complete:
                if not response_complete.is_set():
                    await response_complete.wait()
                return {"type": "http.disconnect"}
    
            body = request.read()
            if isinstance(body, str):
                body_bytes: bytes = body.encode("utf-8")  # pragma: no cover
            elif body is None:
                body_bytes = b""  # pragma: no cover
            elif isinstance(body, GeneratorType):
                try:  # pragma: no cover
                    chunk = body.send(None)
                    if isinstance(chunk, str):
                        chunk = chunk.encode("utf-8")
                    return {"type": "http.request", "body": chunk, "more_body": True}
                except StopIteration:  # pragma: no cover
                    request_complete = True
                    return {"type": "http.request", "body": b""}
            else:
                body_bytes = body
    
            request_complete = True
            return {"type": "http.request", "body": body_bytes}
    
        async def send(message: Message) -> None:
            nonlocal raw_kwargs, response_started, template, context
    
            if message["type"] == "http.response.start":
                assert not response_started, 'Received multiple "http.response.start" messages.'
                raw_kwargs["status_code"] = message["status"]
                raw_kwargs["headers"] = [(key.decode(), value.decode()) for key, value in message.get("headers", [])]
                response_started = True
            elif message["type"] == "http.response.body":
                assert response_started, 'Received "http.response.body" without "http.response.start".'
                assert not response_complete.is_set(), 'Received "http.response.body" after response completed.'
                body = message.get("body", b"")
                more_body = message.get("more_body", False)
                if request.method != "HEAD":
                    raw_kwargs["stream"].write(body)
                if not more_body:
                    raw_kwargs["stream"].seek(0)
                    response_complete.set()
            elif message["type"] == "http.response.debug":
                template = message["info"]["template"]
                context = message["info"]["context"]
    
        try:
            with self.portal_factory() as portal:
                response_complete = portal.call(anyio.Event)
                portal.call(self.app, scope, receive, send)
        except BaseException as exc:
            if self.raise_server_exceptions:
>               raise exc

.venv\lib\site-packages\starlette\testclient.py:354: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient._TestClientTransport object at 0x00000150F9CB46D0>
request = <Request('GET', 'http://testserver/models')>

    def handle_request(self, request: httpx.Request) -> httpx.Response:
        scheme = request.url.scheme
        netloc = request.url.netloc.decode(encoding="ascii")
        path = request.url.path
        raw_path = request.url.raw_path
        query = request.url.query.decode(encoding="ascii")
    
        default_port = {"http": 80, "ws": 80, "https": 443, "wss": 443}[scheme]
    
        if ":" in netloc:
            host, port_string = netloc.split(":", 1)
            port = int(port_string)
        else:
            host = netloc
            port = default_port
    
        # Include the 'host' header.
        if "host" in request.headers:
            headers: list[tuple[bytes, bytes]] = []
        elif port == default_port:  # pragma: no cover
            headers = [(b"host", host.encode())]
        else:  # pragma: no cover
            headers = [(b"host", (f"{host}:{port}").encode())]
    
        # Include other request headers.
        headers += [(key.lower().encode(), value.encode()) for key, value in request.headers.multi_items()]
    
        scope: dict[str, Any]
    
        if scheme in {"ws", "wss"}:
            subprotocol = request.headers.get("sec-websocket-protocol", None)
            if subprotocol is None:
                subprotocols: Sequence[str] = []
            else:
                subprotocols = [value.strip() for value in subprotocol.split(",")]
            scope = {
                "type": "websocket",
                "path": unquote(path),
                "raw_path": raw_path.split(b"?", 1)[0],
                "root_path": self.root_path,
                "scheme": scheme,
                "query_string": query.encode(),
                "headers": headers,
                "client": self.client,
                "server": [host, port],
                "subprotocols": subprotocols,
                "state": self.app_state.copy(),
                "extensions": {"websocket.http.response": {}},
            }
            session = WebSocketTestSession(self.app, scope, self.portal_factory)
            raise _Upgrade(session)
    
        scope = {
            "type": "http",
            "http_version": "1.1",
            "method": request.method,
            "path": unquote(path),
            "raw_path": raw_path.split(b"?", 1)[0],
            "root_path": self.root_path,
            "scheme": scheme,
            "query_string": query.encode(),
            "headers": headers,
            "client": self.client,
            "server": [host, port],
            "extensions": {"http.response.debug": {}},
            "state": self.app_state.copy(),
        }
    
        request_complete = False
        response_started = False
        response_complete: anyio.Event
        raw_kwargs: dict[str, Any] = {"stream": io.BytesIO()}
        template = None
        context = None
    
        async def receive() -> Message:
            nonlocal request_complete
    
            if request_complete:
                if not response_complete.is_set():
                    await response_complete.wait()
                return {"type": "http.disconnect"}
    
            body = request.read()
            if isinstance(body, str):
                body_bytes: bytes = body.encode("utf-8")  # pragma: no cover
            elif body is None:
                body_bytes = b""  # pragma: no cover
            elif isinstance(body, GeneratorType):
                try:  # pragma: no cover
                    chunk = body.send(None)
                    if isinstance(chunk, str):
                        chunk = chunk.encode("utf-8")
                    return {"type": "http.request", "body": chunk, "more_body": True}
                except StopIteration:  # pragma: no cover
                    request_complete = True
                    return {"type": "http.request", "body": b""}
            else:
                body_bytes = body
    
            request_complete = True
            return {"type": "http.request", "body": body_bytes}
    
        async def send(message: Message) -> None:
            nonlocal raw_kwargs, response_started, template, context
    
            if message["type"] == "http.response.start":
                assert not response_started, 'Received multiple "http.response.start" messages.'
                raw_kwargs["status_code"] = message["status"]
                raw_kwargs["headers"] = [(key.decode(), value.decode()) for key, value in message.get("headers", [])]
                response_started = True
            elif message["type"] == "http.response.body":
                assert response_started, 'Received "http.response.body" without "http.response.start".'
                assert not response_complete.is_set(), 'Received "http.response.body" after response completed.'
                body = message.get("body", b"")
                more_body = message.get("more_body", False)
                if request.method != "HEAD":
                    raw_kwargs["stream"].write(body)
                if not more_body:
                    raw_kwargs["stream"].seek(0)
                    response_complete.set()
            elif message["type"] == "http.response.debug":
                template = message["info"]["template"]
                context = message["info"]["context"]
    
        try:
            with self.portal_factory() as portal:
                response_complete = portal.call(anyio.Event)
>               portal.call(self.app, scope, receive, send)

.venv\lib\site-packages\starlette\testclient.py:351: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <anyio._backends._asyncio.BlockingPortal object at 0x00000150F9CB7610>
func = <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>
args = ({'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <fu...ls>.receive at 0x00000150F9B1C3A0>, <function _TestClientTransport.handle_request.<locals>.send at 0x00000150F9B1D120>)

    def call(
        self,
        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval] | T_Retval],
        *args: Unpack[PosArgsT],
    ) -> T_Retval:
        """
        Call the given function in the event loop thread.
    
        If the callable returns a coroutine object, it is awaited on.
    
        :param func: any callable
        :raises RuntimeError: if the portal is not running or if this method is called
            from within the event loop thread
    
        """
>       return cast(T_Retval, self.start_task_soon(func, *args).result())

.venv\lib\site-packages\anyio\from_thread.py:291: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = None, timeout = None

    def result(self, timeout=None):
        """Return the result of the call that the future represents.
    
        Args:
            timeout: The number of seconds to wait for the result if the future
                isn't done. If None, then there is no limit on the wait time.
    
        Returns:
            The result of the call that the future represents.
    
        Raises:
            CancelledError: If the future was cancelled.
            TimeoutError: If the future didn't finish executing before the given
                timeout.
            Exception: If the call raised then that exception will be raised.
        """
        try:
            with self._condition:
                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
                    raise CancelledError()
                elif self._state == FINISHED:
                    return self.__get_result()
    
                self._condition.wait(timeout)
    
                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
                    raise CancelledError()
                elif self._state == FINISHED:
>                   return self.__get_result()

C:\Program Files\Python310\lib\concurrent\futures\_base.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = None

    def __get_result(self):
        if self._exception:
            try:
>               raise self._exception

C:\Program Files\Python310\lib\concurrent\futures\_base.py:403: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <anyio._backends._asyncio.BlockingPortal object at 0x00000150F9CB7610>
func = <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>
args = ({'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <fu...ls>.receive at 0x00000150F9B1C3A0>, <function _TestClientTransport.handle_request.<locals>.send at 0x00000150F9B1D120>)
kwargs = {}
future = <Future at 0x150f9bf8f10 state=finished raised AttributeError>

    async def _call_func(
        self,
        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval] | T_Retval],
        args: tuple[Unpack[PosArgsT]],
        kwargs: dict[str, Any],
        future: Future[T_Retval],
    ) -> None:
        def callback(f: Future[T_Retval]) -> None:
            if f.cancelled() and self._event_loop_thread_id not in (
                None,
                get_ident(),
            ):
                self.call(scope.cancel)
    
        try:
            retval_or_awaitable = func(*args, **kwargs)
            if isawaitable(retval_or_awaitable):
                with CancelScope() as scope:
                    if future.cancelled():
                        scope.cancel()
                    else:
                        future.add_done_callback(callback)
    
>                   retval = await retval_or_awaitable

.venv\lib\site-packages\anyio\from_thread.py:222: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <function list_models at 0x00000150F77917E0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150F9B1C3A0>
send = <function _TestClientTransport.handle_request.<locals>.send at 0x00000150F9B1D120>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if self.root_path:
            scope["root_path"] = self.root_path
>       await super().__call__(scope, receive, send)

.venv\lib\site-packages\fastapi\applications.py:1054: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <function list_models at 0x00000150F77917E0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150F9B1C3A0>
send = <function _TestClientTransport.handle_request.<locals>.send at 0x00000150F9B1D120>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        scope["app"] = self
        if self.middleware_stack is None:
            self.middleware_stack = self.build_middleware_stack()
>       await self.middleware_stack(scope, receive, send)

.venv\lib\site-packages\starlette\applications.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.middleware.errors.ServerErrorMiddleware object at 0x00000150F9CB5240>
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <function list_models at 0x00000150F77917E0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150F9B1C3A0>
send = <function _TestClientTransport.handle_request.<locals>.send at 0x00000150F9B1D120>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        response_started = False
    
        async def _send(message: Message) -> None:
            nonlocal response_started, send
    
            if message["type"] == "http.response.start":
                response_started = True
            await send(message)
    
        try:
            await self.app(scope, receive, _send)
        except Exception as exc:
            request = Request(scope)
            if self.debug:
                # In debug mode, return traceback responses.
                response = self.debug_response(request, exc)
            elif self.handler is None:
                # Use our default 500 error handler.
                response = self.error_response(request, exc)
            else:
                # Use an installed 500 error handler.
                if is_async_callable(self.handler):
                    response = await self.handler(request, exc)
                else:
                    response = await run_in_threadpool(self.handler, request, exc)
    
            if not response_started:
                await response(scope, receive, send)
    
            # We always continue to raise the exception.
            # This allows servers to log the error, or allows test clients
            # to optionally raise the error within the test case.
>           raise exc

.venv\lib\site-packages\starlette\middleware\errors.py:186: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.middleware.errors.ServerErrorMiddleware object at 0x00000150F9CB5240>
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <function list_models at 0x00000150F77917E0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150F9B1C3A0>
send = <function _TestClientTransport.handle_request.<locals>.send at 0x00000150F9B1D120>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        response_started = False
    
        async def _send(message: Message) -> None:
            nonlocal response_started, send
    
            if message["type"] == "http.response.start":
                response_started = True
            await send(message)
    
        try:
>           await self.app(scope, receive, _send)

.venv\lib\site-packages\starlette\middleware\errors.py:164: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.response_middleware.RetryAfterMiddleware object at 0x00000150F9CB51E0>
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <function list_models at 0x00000150F77917E0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150F9B1C3A0>
send = <function ServerErrorMiddleware.__call__.<locals>._send at 0x00000150F8AC2050>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        request = _CachedRequest(scope, receive)
        wrapped_receive = request.wrapped_receive
        response_sent = anyio.Event()
        app_exc: Exception | None = None
        exception_already_raised = False
    
        async def call_next(request: Request) -> Response:
            async def receive_or_disconnect() -> Message:
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                async with anyio.create_task_group() as task_group:
    
                    async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                        result = await func()
                        task_group.cancel_scope.cancel()
                        return result
    
                    task_group.start_soon(wrap, response_sent.wait)
                    message = await wrap(wrapped_receive)
    
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                return message
    
            async def send_no_error(message: Message) -> None:
                try:
                    await send_stream.send(message)
                except anyio.BrokenResourceError:
                    # recv_stream has been closed, i.e. response_sent has been set.
                    return
    
            async def coro() -> None:
                nonlocal app_exc
    
                with send_stream:
                    try:
                        await self.app(scope, receive_or_disconnect, send_no_error)
                    except Exception as exc:
                        app_exc = exc
    
            task_group.start_soon(coro)
    
            try:
                message = await recv_stream.receive()
                info = message.get("info", None)
                if message["type"] == "http.response.debug" and info is not None:
                    message = await recv_stream.receive()
            except anyio.EndOfStream:
                if app_exc is not None:
                    nonlocal exception_already_raised
                    exception_already_raised = True
                    raise app_exc
                raise RuntimeError("No response returned.")
    
            assert message["type"] == "http.response.start"
    
            async def body_stream() -> BodyStreamGenerator:
                async for message in recv_stream:
                    if message["type"] == "http.response.pathsend":
                        yield message
                        break
                    assert message["type"] == "http.response.body", f"Unexpected message: {message}"
                    body = message.get("body", b"")
                    if body:
                        yield body
                    if not message.get("more_body", False):
                        break
    
            response = _StreamingResponse(status_code=message["status"], content=body_stream(), info=info)
            response.raw_headers = message["headers"]
            return response
    
        streams: anyio.create_memory_object_stream[Message] = anyio.create_memory_object_stream()
        send_stream, recv_stream = streams
>       with recv_stream, send_stream, collapse_excgroups():

.venv\lib\site-packages\starlette\middleware\base.py:182: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <contextlib._GeneratorContextManager object at 0x00000150F9BF9000>
typ = <class 'exceptiongroup.ExceptionGroup'>
value = ExceptionGroup('unhandled errors in a TaskGroup', [AttributeError('__name__')])
traceback = <traceback object at 0x00000150F9C68280>

    def __exit__(self, typ, value, traceback):
        if typ is None:
            try:
                next(self.gen)
            except StopIteration:
                return False
            else:
                raise RuntimeError("generator didn't stop")
        else:
            if value is None:
                # Need to force instantiation so we can reliably
                # tell if we get the same exception back
                value = typ()
            try:
>               self.gen.throw(typ, value, traceback)

C:\Program Files\Python310\lib\contextlib.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    @contextmanager
    def collapse_excgroups() -> Generator[None, None, None]:
        try:
            yield
        except BaseException as exc:
            if has_exceptiongroups:  # pragma: no cover
                while isinstance(exc, BaseExceptionGroup) and len(exc.exceptions) == 1:
                    exc = exc.exceptions[0]
    
>           raise exc

.venv\lib\site-packages\starlette\_utils.py:83: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.response_middleware.RetryAfterMiddleware object at 0x00000150F9CB51E0>
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <function list_models at 0x00000150F77917E0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150F9B1C3A0>
send = <function ServerErrorMiddleware.__call__.<locals>._send at 0x00000150F8AC2050>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        request = _CachedRequest(scope, receive)
        wrapped_receive = request.wrapped_receive
        response_sent = anyio.Event()
        app_exc: Exception | None = None
        exception_already_raised = False
    
        async def call_next(request: Request) -> Response:
            async def receive_or_disconnect() -> Message:
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                async with anyio.create_task_group() as task_group:
    
                    async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                        result = await func()
                        task_group.cancel_scope.cancel()
                        return result
    
                    task_group.start_soon(wrap, response_sent.wait)
                    message = await wrap(wrapped_receive)
    
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                return message
    
            async def send_no_error(message: Message) -> None:
                try:
                    await send_stream.send(message)
                except anyio.BrokenResourceError:
                    # recv_stream has been closed, i.e. response_sent has been set.
                    return
    
            async def coro() -> None:
                nonlocal app_exc
    
                with send_stream:
                    try:
                        await self.app(scope, receive_or_disconnect, send_no_error)
                    except Exception as exc:
                        app_exc = exc
    
            task_group.start_soon(coro)
    
            try:
                message = await recv_stream.receive()
                info = message.get("info", None)
                if message["type"] == "http.response.debug" and info is not None:
                    message = await recv_stream.receive()
            except anyio.EndOfStream:
                if app_exc is not None:
                    nonlocal exception_already_raised
                    exception_already_raised = True
                    raise app_exc
                raise RuntimeError("No response returned.")
    
            assert message["type"] == "http.response.start"
    
            async def body_stream() -> BodyStreamGenerator:
                async for message in recv_stream:
                    if message["type"] == "http.response.pathsend":
                        yield message
                        break
                    assert message["type"] == "http.response.body", f"Unexpected message: {message}"
                    body = message.get("body", b"")
                    if body:
                        yield body
                    if not message.get("more_body", False):
                        break
    
            response = _StreamingResponse(status_code=message["status"], content=body_stream(), info=info)
            response.raw_headers = message["headers"]
            return response
    
        streams: anyio.create_memory_object_stream[Message] = anyio.create_memory_object_stream()
        send_stream, recv_stream = streams
        with recv_stream, send_stream, collapse_excgroups():
            async with anyio.create_task_group() as task_group:
>               response = await self.dispatch_func(request, call_next)

.venv\lib\site-packages\starlette\middleware\base.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.response_middleware.RetryAfterMiddleware object at 0x00000150F9CB51E0>
request = <starlette.middleware.base._CachedRequest object at 0x00000150F9BF9AB0>
call_next = <function BaseHTTPMiddleware.__call__.<locals>.call_next at 0x00000150F9C35480>

    async def dispatch(self, request: Request, call_next) -> Response:
>       response = await call_next(request)

src\response_middleware.py:437: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

request = <starlette.middleware.base._CachedRequest object at 0x00000150F9BF9AB0>

    async def call_next(request: Request) -> Response:
        async def receive_or_disconnect() -> Message:
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            async with anyio.create_task_group() as task_group:
    
                async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                    result = await func()
                    task_group.cancel_scope.cancel()
                    return result
    
                task_group.start_soon(wrap, response_sent.wait)
                message = await wrap(wrapped_receive)
    
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            return message
    
        async def send_no_error(message: Message) -> None:
            try:
                await send_stream.send(message)
            except anyio.BrokenResourceError:
                # recv_stream has been closed, i.e. response_sent has been set.
                return
    
        async def coro() -> None:
            nonlocal app_exc
    
            with send_stream:
                try:
                    await self.app(scope, receive_or_disconnect, send_no_error)
                except Exception as exc:
                    app_exc = exc
    
        task_group.start_soon(coro)
    
        try:
            message = await recv_stream.receive()
            info = message.get("info", None)
            if message["type"] == "http.response.debug" and info is not None:
                message = await recv_stream.receive()
        except anyio.EndOfStream:
            if app_exc is not None:
                nonlocal exception_already_raised
                exception_already_raised = True
>               raise app_exc

.venv\lib\site-packages\starlette\middleware\base.py:159: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    async def coro() -> None:
        nonlocal app_exc
    
        with send_stream:
            try:
>               await self.app(scope, receive_or_disconnect, send_no_error)

.venv\lib\site-packages\starlette\middleware\base.py:144: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.request_middleware.CustomHeaderMiddleware object at 0x00000150F9CB6EC0>
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <function list_models at 0x00000150F77917E0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x00000150F9C35000>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x00000150F9C36950>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        request = _CachedRequest(scope, receive)
        wrapped_receive = request.wrapped_receive
        response_sent = anyio.Event()
        app_exc: Exception | None = None
        exception_already_raised = False
    
        async def call_next(request: Request) -> Response:
            async def receive_or_disconnect() -> Message:
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                async with anyio.create_task_group() as task_group:
    
                    async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                        result = await func()
                        task_group.cancel_scope.cancel()
                        return result
    
                    task_group.start_soon(wrap, response_sent.wait)
                    message = await wrap(wrapped_receive)
    
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                return message
    
            async def send_no_error(message: Message) -> None:
                try:
                    await send_stream.send(message)
                except anyio.BrokenResourceError:
                    # recv_stream has been closed, i.e. response_sent has been set.
                    return
    
            async def coro() -> None:
                nonlocal app_exc
    
                with send_stream:
                    try:
                        await self.app(scope, receive_or_disconnect, send_no_error)
                    except Exception as exc:
                        app_exc = exc
    
            task_group.start_soon(coro)
    
            try:
                message = await recv_stream.receive()
                info = message.get("info", None)
                if message["type"] == "http.response.debug" and info is not None:
                    message = await recv_stream.receive()
            except anyio.EndOfStream:
                if app_exc is not None:
                    nonlocal exception_already_raised
                    exception_already_raised = True
                    raise app_exc
                raise RuntimeError("No response returned.")
    
            assert message["type"] == "http.response.start"
    
            async def body_stream() -> BodyStreamGenerator:
                async for message in recv_stream:
                    if message["type"] == "http.response.pathsend":
                        yield message
                        break
                    assert message["type"] == "http.response.body", f"Unexpected message: {message}"
                    body = message.get("body", b"")
                    if body:
                        yield body
                    if not message.get("more_body", False):
                        break
    
            response = _StreamingResponse(status_code=message["status"], content=body_stream(), info=info)
            response.raw_headers = message["headers"]
            return response
    
        streams: anyio.create_memory_object_stream[Message] = anyio.create_memory_object_stream()
        send_stream, recv_stream = streams
>       with recv_stream, send_stream, collapse_excgroups():

.venv\lib\site-packages\starlette\middleware\base.py:182: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <contextlib._GeneratorContextManager object at 0x00000150F9BF91E0>
typ = <class 'exceptiongroup.ExceptionGroup'>
value = ExceptionGroup('unhandled errors in a TaskGroup', [AttributeError('__name__')])
traceback = <traceback object at 0x00000150F9C68A80>

    def __exit__(self, typ, value, traceback):
        if typ is None:
            try:
                next(self.gen)
            except StopIteration:
                return False
            else:
                raise RuntimeError("generator didn't stop")
        else:
            if value is None:
                # Need to force instantiation so we can reliably
                # tell if we get the same exception back
                value = typ()
            try:
>               self.gen.throw(typ, value, traceback)

C:\Program Files\Python310\lib\contextlib.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    @contextmanager
    def collapse_excgroups() -> Generator[None, None, None]:
        try:
            yield
        except BaseException as exc:
            if has_exceptiongroups:  # pragma: no cover
                while isinstance(exc, BaseExceptionGroup) and len(exc.exceptions) == 1:
                    exc = exc.exceptions[0]
    
>           raise exc

.venv\lib\site-packages\starlette\_utils.py:83: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.request_middleware.CustomHeaderMiddleware object at 0x00000150F9CB6EC0>
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <function list_models at 0x00000150F77917E0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x00000150F9C35000>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x00000150F9C36950>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        request = _CachedRequest(scope, receive)
        wrapped_receive = request.wrapped_receive
        response_sent = anyio.Event()
        app_exc: Exception | None = None
        exception_already_raised = False
    
        async def call_next(request: Request) -> Response:
            async def receive_or_disconnect() -> Message:
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                async with anyio.create_task_group() as task_group:
    
                    async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                        result = await func()
                        task_group.cancel_scope.cancel()
                        return result
    
                    task_group.start_soon(wrap, response_sent.wait)
                    message = await wrap(wrapped_receive)
    
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                return message
    
            async def send_no_error(message: Message) -> None:
                try:
                    await send_stream.send(message)
                except anyio.BrokenResourceError:
                    # recv_stream has been closed, i.e. response_sent has been set.
                    return
    
            async def coro() -> None:
                nonlocal app_exc
    
                with send_stream:
                    try:
                        await self.app(scope, receive_or_disconnect, send_no_error)
                    except Exception as exc:
                        app_exc = exc
    
            task_group.start_soon(coro)
    
            try:
                message = await recv_stream.receive()
                info = message.get("info", None)
                if message["type"] == "http.response.debug" and info is not None:
                    message = await recv_stream.receive()
            except anyio.EndOfStream:
                if app_exc is not None:
                    nonlocal exception_already_raised
                    exception_already_raised = True
                    raise app_exc
                raise RuntimeError("No response returned.")
    
            assert message["type"] == "http.response.start"
    
            async def body_stream() -> BodyStreamGenerator:
                async for message in recv_stream:
                    if message["type"] == "http.response.pathsend":
                        yield message
                        break
                    assert message["type"] == "http.response.body", f"Unexpected message: {message}"
                    body = message.get("body", b"")
                    if body:
                        yield body
                    if not message.get("more_body", False):
                        break
    
            response = _StreamingResponse(status_code=message["status"], content=body_stream(), info=info)
            response.raw_headers = message["headers"]
            return response
    
        streams: anyio.create_memory_object_stream[Message] = anyio.create_memory_object_stream()
        send_stream, recv_stream = streams
        with recv_stream, send_stream, collapse_excgroups():
            async with anyio.create_task_group() as task_group:
>               response = await self.dispatch_func(request, call_next)

.venv\lib\site-packages\starlette\middleware\base.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.request_middleware.CustomHeaderMiddleware object at 0x00000150F9CB6EC0>
request = <starlette.middleware.base._CachedRequest object at 0x00000150F9BFA530>
call_next = <function BaseHTTPMiddleware.__call__.<locals>.call_next at 0x00000150F9C35B40>

    async def dispatch(self, request: Request, call_next) -> Response:
>       response = await call_next(request)

src\request_middleware.py:210: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

request = <starlette.middleware.base._CachedRequest object at 0x00000150F9BFA530>

    async def call_next(request: Request) -> Response:
        async def receive_or_disconnect() -> Message:
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            async with anyio.create_task_group() as task_group:
    
                async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                    result = await func()
                    task_group.cancel_scope.cancel()
                    return result
    
                task_group.start_soon(wrap, response_sent.wait)
                message = await wrap(wrapped_receive)
    
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            return message
    
        async def send_no_error(message: Message) -> None:
            try:
                await send_stream.send(message)
            except anyio.BrokenResourceError:
                # recv_stream has been closed, i.e. response_sent has been set.
                return
    
        async def coro() -> None:
            nonlocal app_exc
    
            with send_stream:
                try:
                    await self.app(scope, receive_or_disconnect, send_no_error)
                except Exception as exc:
                    app_exc = exc
    
        task_group.start_soon(coro)
    
        try:
            message = await recv_stream.receive()
            info = message.get("info", None)
            if message["type"] == "http.response.debug" and info is not None:
                message = await recv_stream.receive()
        except anyio.EndOfStream:
            if app_exc is not None:
                nonlocal exception_already_raised
                exception_already_raised = True
>               raise app_exc

.venv\lib\site-packages\starlette\middleware\base.py:159: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    async def coro() -> None:
        nonlocal app_exc
    
        with send_stream:
            try:
>               await self.app(scope, receive_or_disconnect, send_no_error)

.venv\lib\site-packages\starlette\middleware\base.py:144: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.middleware.cors.CORSMiddleware object at 0x00000150F9CB5030>
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <function list_models at 0x00000150F77917E0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x00000150F9C35FC0>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x00000150F9C34AF0>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":  # pragma: no cover
            await self.app(scope, receive, send)
            return
    
        method = scope["method"]
        headers = Headers(scope=scope)
        origin = headers.get("origin")
    
        if origin is None:
>           await self.app(scope, receive, send)

.venv\lib\site-packages\starlette\middleware\cors.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.middleware.exceptions.ExceptionMiddleware object at 0x00000150F9CB58D0>
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <function list_models at 0x00000150F77917E0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x00000150F9C35FC0>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x00000150F9C34AF0>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] not in ("http", "websocket"):
            await self.app(scope, receive, send)
            return
    
        scope["starlette.exception_handlers"] = (
            self._exception_handlers,
            self._status_handlers,
        )
    
        conn: Request | WebSocket
        if scope["type"] == "http":
            conn = Request(scope, receive, send)
        else:
            conn = WebSocket(scope, receive, send)
    
>       await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)

.venv\lib\site-packages\starlette\middleware\exceptions.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

scope = {'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <function list_models at 0x00000150F77917E0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x00000150F9C35FC0>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x00000150F9C34AF0>

    async def wrapped_app(scope: Scope, receive: Receive, send: Send) -> None:
        response_started = False
    
        async def sender(message: Message) -> None:
            nonlocal response_started
    
            if message["type"] == "http.response.start":
                response_started = True
            await send(message)
    
        try:
            await app(scope, receive, sender)
        except Exception as exc:
            handler = None
    
            if isinstance(exc, HTTPException):
                handler = status_handlers.get(exc.status_code)
    
            if handler is None:
                handler = _lookup_exception_handler(exception_handlers, exc)
    
            if handler is None:
>               raise exc

.venv\lib\site-packages\starlette\_exception_handler.py:53: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

scope = {'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <function list_models at 0x00000150F77917E0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x00000150F9C35FC0>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x00000150F9C34AF0>

    async def wrapped_app(scope: Scope, receive: Receive, send: Send) -> None:
        response_started = False
    
        async def sender(message: Message) -> None:
            nonlocal response_started
    
            if message["type"] == "http.response.start":
                response_started = True
            await send(message)
    
        try:
>           await app(scope, receive, sender)

.venv\lib\site-packages\starlette\_exception_handler.py:42: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <fastapi.routing.APIRouter object at 0x00000150F9D04160>
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <function list_models at 0x00000150F77917E0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x00000150F9C35FC0>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x00000150F9C363B0>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        """
        The main entry point to the Router class.
        """
>       await self.middleware_stack(scope, receive, send)

.venv\lib\site-packages\starlette\routing.py:716: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <fastapi.routing.APIRouter object at 0x00000150F9D04160>
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <function list_models at 0x00000150F77917E0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x00000150F9C35FC0>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x00000150F9C363B0>

    async def app(self, scope: Scope, receive: Receive, send: Send) -> None:
        assert scope["type"] in ("http", "websocket", "lifespan")
    
        if "router" not in scope:
            scope["router"] = self
    
        if scope["type"] == "lifespan":
            await self.lifespan(scope, receive, send)
            return
    
        partial = None
    
        for route in self.routes:
            # Determine if any route matches the incoming scope,
            # and hand over to the matching route if found.
            match, child_scope = route.matches(scope)
            if match == Match.FULL:
                scope.update(child_scope)
>               await route.handle(scope, receive, send)

.venv\lib\site-packages\starlette\routing.py:736: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = APIRoute(path='/models', name='list_models', methods=['GET'])
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <function list_models at 0x00000150F77917E0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x00000150F9C35FC0>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x00000150F9C363B0>

    async def handle(self, scope: Scope, receive: Receive, send: Send) -> None:
        if self.methods and scope["method"] not in self.methods:
            headers = {"Allow": ", ".join(self.methods)}
            if "app" in scope:
                raise HTTPException(status_code=405, headers=headers)
            else:
                response = PlainTextResponse("Method Not Allowed", status_code=405, headers=headers)
            await response(scope, receive, send)
        else:
>           await self.app(scope, receive, send)

.venv\lib\site-packages\starlette\routing.py:290: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

scope = {'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <function list_models at 0x00000150F77917E0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x00000150F9C35FC0>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x00000150F9C363B0>

    async def app(scope: Scope, receive: Receive, send: Send) -> None:
        request = Request(scope, receive, send)
    
        async def app(scope: Scope, receive: Receive, send: Send) -> None:
            response = await f(request)
            await response(scope, receive, send)
    
>       await wrap_app_handling_exceptions(app, request)(scope, receive, send)

.venv\lib\site-packages\starlette\routing.py:78: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

scope = {'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <function list_models at 0x00000150F77917E0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x00000150F9C35FC0>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x00000150F9C363B0>

    async def wrapped_app(scope: Scope, receive: Receive, send: Send) -> None:
        response_started = False
    
        async def sender(message: Message) -> None:
            nonlocal response_started
    
            if message["type"] == "http.response.start":
                response_started = True
            await send(message)
    
        try:
            await app(scope, receive, sender)
        except Exception as exc:
            handler = None
    
            if isinstance(exc, HTTPException):
                handler = status_handlers.get(exc.status_code)
    
            if handler is None:
                handler = _lookup_exception_handler(exception_handlers, exc)
    
            if handler is None:
>               raise exc

.venv\lib\site-packages\starlette\_exception_handler.py:53: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

scope = {'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <function list_models at 0x00000150F77917E0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x00000150F9C35FC0>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x00000150F9C363B0>

    async def wrapped_app(scope: Scope, receive: Receive, send: Send) -> None:
        response_started = False
    
        async def sender(message: Message) -> None:
            nonlocal response_started
    
            if message["type"] == "http.response.start":
                response_started = True
            await send(message)
    
        try:
>           await app(scope, receive, sender)

.venv\lib\site-packages\starlette\_exception_handler.py:42: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

scope = {'app': <fastapi.applications.FastAPI object at 0x00000150F9CB4D30>, 'client': ('testclient', 50000), 'endpoint': <function list_models at 0x00000150F77917E0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x00000150F9C35FC0>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x00000150F9C35F30>

    async def app(scope: Scope, receive: Receive, send: Send) -> None:
>       response = await f(request)

.venv\lib\site-packages\starlette\routing.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

request = <starlette.requests.Request object at 0x00000150F9BF8A30>

    async def app(request: Request) -> Response:
        response: Union[Response, None] = None
        async with AsyncExitStack() as file_stack:
            try:
                body: Any = None
                if body_field:
                    if is_body_form:
                        body = await request.form()
                        file_stack.push_async_callback(body.close)
                    else:
                        body_bytes = await request.body()
                        if body_bytes:
                            json_body: Any = Undefined
                            content_type_value = request.headers.get("content-type")
                            if not content_type_value:
                                json_body = await request.json()
                            else:
                                message = email.message.Message()
                                message["content-type"] = content_type_value
                                if message.get_content_maintype() == "application":
                                    subtype = message.get_content_subtype()
                                    if subtype == "json" or subtype.endswith("+json"):
                                        json_body = await request.json()
                            if json_body != Undefined:
                                body = json_body
                            else:
                                body = body_bytes
            except json.JSONDecodeError as e:
                validation_error = RequestValidationError(
                    [
                        {
                            "type": "json_invalid",
                            "loc": ("body", e.pos),
                            "msg": "JSON decode error",
                            "input": {},
                            "ctx": {"error": e.msg},
                        }
                    ],
                    body=e.doc,
                )
                raise validation_error from e
            except HTTPException:
                # If a middleware raises an HTTPException, it should be raised again
                raise
            except Exception as e:
                http_error = HTTPException(
                    status_code=400, detail="There was an error parsing the body"
                )
                raise http_error from e
            errors: List[Any] = []
            async with AsyncExitStack() as async_exit_stack:
>               solved_result = await solve_dependencies(
                    request=request,
                    dependant=dependant,
                    body=body,
                    dependency_overrides_provider=dependency_overrides_provider,
                    async_exit_stack=async_exit_stack,
                    embed_body_fields=embed_body_fields,
                )

.venv\lib\site-packages\fastapi\routing.py:292: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    async def solve_dependencies(
        *,
        request: Union[Request, WebSocket],
        dependant: Dependant,
        body: Optional[Union[Dict[str, Any], FormData]] = None,
        background_tasks: Optional[StarletteBackgroundTasks] = None,
        response: Optional[Response] = None,
        dependency_overrides_provider: Optional[Any] = None,
        dependency_cache: Optional[Dict[Tuple[Callable[..., Any], Tuple[str]], Any]] = None,
        async_exit_stack: AsyncExitStack,
        embed_body_fields: bool,
    ) -> SolvedDependency:
        values: Dict[str, Any] = {}
        errors: List[Any] = []
        if response is None:
            response = Response()
            del response.headers["content-length"]
            response.status_code = None  # type: ignore
        dependency_cache = dependency_cache or {}
        sub_dependant: Dependant
        for sub_dependant in dependant.dependencies:
            sub_dependant.call = cast(Callable[..., Any], sub_dependant.call)
            sub_dependant.cache_key = cast(
                Tuple[Callable[..., Any], Tuple[str]], sub_dependant.cache_key
            )
            call = sub_dependant.call
            use_sub_dependant = sub_dependant
            if (
                dependency_overrides_provider
                and dependency_overrides_provider.dependency_overrides
            ):
                original_call = sub_dependant.call
                call = getattr(
                    dependency_overrides_provider, "dependency_overrides", {}
                ).get(original_call, original_call)
                use_path: str = sub_dependant.path  # type: ignore
                use_sub_dependant = get_dependant(
                    path=use_path,
                    call=call,
                    name=sub_dependant.name,
                    security_scopes=sub_dependant.security_scopes,
                )
    
            solved_result = await solve_dependencies(
                request=request,
                dependant=use_sub_dependant,
                body=body,
                background_tasks=background_tasks,
                response=response,
                dependency_overrides_provider=dependency_overrides_provider,
                dependency_cache=dependency_cache,
                async_exit_stack=async_exit_stack,
                embed_body_fields=embed_body_fields,
            )
            background_tasks = solved_result.background_tasks
            dependency_cache.update(solved_result.dependency_cache)
            if solved_result.errors:
                errors.extend(solved_result.errors)
                continue
            if sub_dependant.use_cache and sub_dependant.cache_key in dependency_cache:
                solved = dependency_cache[sub_dependant.cache_key]
            elif is_gen_callable(call) or is_async_gen_callable(call):
                solved = await solve_generator(
                    call=call, stack=async_exit_stack, sub_values=solved_result.values
                )
            elif is_coroutine_callable(call):
>               solved = await call(**solved_result.values)

.venv\lib\site-packages\fastapi\dependencies\utils.py:638: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

request = <starlette.requests.Request object at 0x00000150F9BF8A30>

    async def get_backend_service(request: Request) -> IBackendService:
        """Get the backend service from the service provider.
    
        Args:
            request: The FastAPI request
    
        Returns:
            The backend service
        """
        if hasattr(request.app.state, "service_provider"):
            service_provider = request.app.state.service_provider
>           return service_provider.get_required_service(IBackendService)

src\core\app\controllers\models_controller.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.core.di.container.ServiceProvider object at 0x00000150FADF5870>
service_type = <MagicMock name='IBackendService' id='1447299872224'>

    def get_required_service(self, service_type: type[T]) -> T:
        """Get a service of the given type, throwing if not found."""
        service = self.get_service(service_type)
        if service is None:
>           raise KeyError(f"No service registered for {service_type.__name__}")

src\core\di\container.py:140: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <MagicMock name='IBackendService' id='1447299872224'>, name = '__name__'

    def __getattr__(self, name):
        if name in {'_mock_methods', '_mock_unsafe'}:
            raise AttributeError(name)
        elif self._mock_methods is not None:
            if name not in self._mock_methods or name in _all_magics:
                raise AttributeError("Mock object has no attribute %r" % name)
        elif _is_magic(name):
>           raise AttributeError(name)
E           AttributeError: __name__. Did you mean: '__hash__'?

C:\Program Files\Python310\lib\unittest\mock.py:645: AttributeError
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:35.416733Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:35.416733Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:40 2025-08-17T00:00:35.424234Z [info     ] API Key authentication is disabled [src.core.app.middleware_config]
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:35.440733Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:354 2025-08-17T00:00:35.442743Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: help
INFO     src.core.services.command_service:command_service.py:78 Registered command: backend
INFO     src.core.services.command_service:command_service.py:78 Registered command: model
INFO     src.core.services.command_service:command_service.py:78 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:78 Registered command: project
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:78 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:78 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:78 Registered command: set
INFO     src.core.services.command_service:command_service.py:78 Registered command: unset
INFO     src.core.services.command_service:command_service.py:78 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:78 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:78 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:78 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:78 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:269 Registered 26 command handlers with registry
ERROR    src.core.app.error_handlers:error_handlers.py:110 Unhandled exception
  + Exception Group Traceback (most recent call last):
  |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 77, in collapse_excgroups
  |     yield
  |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 183, in __call__
  |     async with anyio.create_task_group() as task_group:
  |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\anyio\_backends\_asyncio.py", line 772, in __aexit__
  |     raise BaseExceptionGroup(
  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)
  +-+---------------- 1 ----------------
    | Traceback (most recent call last):
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\errors.py", line 164, in __call__
    |     await self.app(scope, receive, _send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 182, in __call__
    |     with recv_stream, send_stream, collapse_excgroups():
    |   File "C:\Program Files\Python310\lib\contextlib.py", line 153, in __exit__
    |     self.gen.throw(typ, value, traceback)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 83, in collapse_excgroups
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 184, in __call__
    |     response = await self.dispatch_func(request, call_next)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\response_middleware.py", line 437, in dispatch
    |     response = await call_next(request)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 159, in call_next
    |     raise app_exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 144, in coro
    |     await self.app(scope, receive_or_disconnect, send_no_error)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 182, in __call__
    |     with recv_stream, send_stream, collapse_excgroups():
    |   File "C:\Program Files\Python310\lib\contextlib.py", line 153, in __exit__
    |     self.gen.throw(typ, value, traceback)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 83, in collapse_excgroups
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 184, in __call__
    |     response = await self.dispatch_func(request, call_next)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\request_middleware.py", line 210, in dispatch
    |     response = await call_next(request)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 159, in call_next
    |     raise app_exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 144, in coro
    |     await self.app(scope, receive_or_disconnect, send_no_error)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\cors.py", line 85, in __call__
    |     await self.app(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\exceptions.py", line 63, in __call__
    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    |     await app(scope, receive, sender)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 716, in __call__
    |     await self.middleware_stack(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 736, in app
    |     await route.handle(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 290, in handle
    |     await self.app(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 78, in app
    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    |     raise exc
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    |     await app(scope, receive, sender)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 75, in app
    |     response = await f(request)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\routing.py", line 292, in app
    |     solved_result = await solve_dependencies(
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\dependencies\utils.py", line 638, in solve_dependencies
    |     solved = await call(**solved_result.values)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\app\controllers\models_controller.py", line 32, in get_backend_service
    |     return service_provider.get_required_service(IBackendService)
    |   File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\di\container.py", line 140, in get_required_service
    |     raise KeyError(f"No service registered for {service_type.__name__}")
    |   File "C:\Program Files\Python310\lib\unittest\mock.py", line 645, in __getattr__
    |     raise AttributeError(name)
    | AttributeError: __name__. Did you mean: '__hash__'?
    +------------------------------------

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 182, in __call__
    with recv_stream, send_stream, collapse_excgroups():
  File "C:\Program Files\Python310\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 83, in collapse_excgroups
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 184, in __call__
    response = await self.dispatch_func(request, call_next)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\response_middleware.py", line 437, in dispatch
    response = await call_next(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 159, in call_next
    raise app_exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 144, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 182, in __call__
    with recv_stream, send_stream, collapse_excgroups():
  File "C:\Program Files\Python310\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_utils.py", line 83, in collapse_excgroups
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 184, in __call__
    response = await self.dispatch_func(request, call_next)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\request_middleware.py", line 210, in dispatch
    response = await call_next(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 159, in call_next
    raise app_exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\base.py", line 144, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\cors.py", line 85, in __call__
    await self.app(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\middleware\exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 78, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\starlette\routing.py", line 75, in app
    response = await f(request)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\routing.py", line 292, in app
    solved_result = await solve_dependencies(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\.venv\lib\site-packages\fastapi\dependencies\utils.py", line 638, in solve_dependencies
    solved = await call(**solved_result.values)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\app\controllers\models_controller.py", line 32, in get_backend_service
    return service_provider.get_required_service(IBackendService)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\di\container.py", line 140, in get_required_service
    raise KeyError(f"No service registered for {service_type.__name__}")
  File "C:\Program Files\Python310\lib\unittest\mock.py", line 645, in __getattr__
    raise AttributeError(name)
AttributeError: __name__. Did you mean: '__hash__'?
INFO     src.core.app.application_factory:application_factory.py:368 2025-08-17T00:00:35.457733Z [info     ] Shutting down application      [src.core.app.application_factory]
___________ TestModelsEndpoints.test_models_endpoint_error_handling ___________

self = <tests.integration.test_models_endpoints.TestModelsEndpoints object at 0x00000150F70332E0>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x00000150FB76E860>

    def test_models_endpoint_error_handling(self, monkeypatch):
        """Test error handling in models endpoint."""
        monkeypatch.setenv("DISABLE_AUTH", "true")
        app = build_app()
    
        with (
            TestClient(app) as client,
            patch(
                "src.core.app.controllers.models_controller.get_backend_service"
            ) as mock_get_service,
        ):
            mock_get_service.side_effect = Exception("Service unavailable")
    
            response = client.get("/models")
    
            # Should handle the error gracefully
>           assert response.status_code == 500
E           assert 200 == 500
E            +  where 200 = <Response [200 OK]>.status_code

tests\integration\test_models_endpoints.py:170: AssertionError
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:35.806244Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:35.806244Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:40 2025-08-17T00:00:35.814234Z [info     ] API Key authentication is disabled [src.core.app.middleware_config]
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:35.827736Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:354 2025-08-17T00:00:35.829268Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: help
INFO     src.core.services.command_service:command_service.py:78 Registered command: backend
INFO     src.core.services.command_service:command_service.py:78 Registered command: model
INFO     src.core.services.command_service:command_service.py:78 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:78 Registered command: project
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:78 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:78 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:78 Registered command: set
INFO     src.core.services.command_service:command_service.py:78 Registered command: unset
INFO     src.core.services.command_service:command_service.py:78 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:78 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:78 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:78 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:78 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:269 Registered 26 command handlers with registry
INFO     src.core.app.controllers.models_controller:models_controller.py:47 Listing available models
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openrouter: {}
WARNING  src.core.app.controllers.models_controller:models_controller.py:122 Failed to get models from openrouter: Failed to create backend openrouter: api_key is required for OpenRouterBackend
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for gemini: {}
WARNING  src.core.app.controllers.models_controller:models_controller.py:122 Failed to get models from gemini: Failed to create backend gemini: gemini_api_base_url, key_name, and api_key are required for GeminiBackend
INFO     src.core.app.controllers.models_controller:models_controller.py:128 No models discovered from backends, using default models
INFO     src.core.app.controllers.models_controller:models_controller.py:146 Returning 6 models
INFO     src.core.app.application_factory:application_factory.py:368 2025-08-17T00:00:35.843735Z [info     ] Shutting down application      [src.core.app.application_factory]
________________ test_integration_bridge_async_initialization _________________

    @pytest.mark.asyncio
    async def test_integration_bridge_async_initialization():
        """Test that the integration bridge can be initialized asynchronously."""
        import httpx
        from fastapi import FastAPI
        from src.core.integration import IntegrationBridge
    
        app = FastAPI()
        app.state.config = {"command_prefix": "!/"}
        app.state.httpx_client = httpx.AsyncClient()
    
        bridge = IntegrationBridge(app)
    
        # Should be able to initialize the new architecture
        await bridge.initialize_new_architecture()
    
        # Verify initialization flags
>       assert bridge.initialized
E       AttributeError: 'IntegrationBridge' object has no attribute 'initialized'. Did you mean: 'initialize'?

tests\integration\test_phase1_integration.py:94: AttributeError
------------------------------ Captured log call ------------------------------
INFO     src.core.integration.bridge:bridge.py:236 Initializing new SOLID architecture
INFO     src.core.integration.bridge:bridge.py:277 Using new architecture services directly (no adapters)
INFO     src.core.integration.bridge:bridge.py:252 New SOLID architecture initialized
________________ test_pwd_command_integration_with_project_dir ________________

app = <fastapi.applications.FastAPI object at 0x00000150FAEB1480>

    def test_pwd_command_integration_with_project_dir(app):
        """Test that the PWD command works correctly in the integration environment with a project directory set."""
        # Mock the APIKeyMiddleware's dispatch method to always return the next response
    
        # Mock the get_integration_bridge function to return the bridge from app.state
        def mock_get_integration_bridge(app_param=None):
            return app.state.integration_bridge
    
        async def mock_dispatch(self, request, call_next):
            return await call_next(request)
    
        # Mock the session service to set a project directory
        async def mock_get_session(*args, **kwargs):
            """Mock the get_session method to return a session with a project directory."""
            from src.core.domain.session import Session, SessionState
    
            return Session(
                session_id="test-pwd-session",
                state=SessionState(project_dir="/test/project/dir"),
            )
    
        with (
            patch(
                "src.core.integration.bridge.get_integration_bridge",
                new=mock_get_integration_bridge,
            ),
            patch(
                "src.core.security.middleware.APIKeyMiddleware.dispatch", new=mock_dispatch
            ),
            patch(
                "src.core.services.session_service.SessionService.get_session",
                new=mock_get_session,
            ),
        ):
            # Create a test client
            client = TestClient(app)
    
            # Send a PWD command
            response = client.post(
                "/v2/chat/completions",
                json={
                    "model": "gpt-3.5-turbo",
                    "messages": [{"role": "user", "content": "!/pwd"}],
                    "session_id": "test-pwd-session",
                },
            )
    
            # Verify the response
>           assert response.status_code == 200
E           assert 500 == 200
E            +  where 500 = <Response [500 Internal Server Error]>.status_code

tests\integration\test_pwd_command_integration.py:99: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:36.112736Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:36.112736Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:36.120233Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:36.135236Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
------------------------------ Captured log call ------------------------------
INFO     src.core.app.controllers.chat_controller:chat_controller.py:59 Handling chat completion request: model=gpt-3.5-turbo
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openai: {}
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
ERROR    src.core.services.request_processor:request_processor.py:259 Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 79, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 85, in _handle_non_streaming_response
    raise HTTPException(
fastapi.exceptions.HTTPException: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 215, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
______________ test_pwd_command_integration_without_project_dir _______________

app = <fastapi.applications.FastAPI object at 0x00000150FB409FC0>

    def test_pwd_command_integration_without_project_dir(app):
        """Test that the PWD command works correctly in the integration environment without a project directory set."""
        # Mock the APIKeyMiddleware's dispatch method to always return the next response
    
        # Mock the get_integration_bridge function to return the bridge from app.state
        def mock_get_integration_bridge(app_param=None):
            return app.state.integration_bridge
    
        async def mock_dispatch(self, request, call_next):
            return await call_next(request)
    
        # Mock the session service to set a project directory
        async def mock_get_session(*args, **kwargs):
            """Mock the get_session method to return a session without a project directory."""
            from src.core.domain.session import Session, SessionState
    
            return Session(
                session_id="test-pwd-session", state=SessionState(project_dir=None)
            )
    
        with (
            patch(
                "src.core.integration.bridge.get_integration_bridge",
                new=mock_get_integration_bridge,
            ),
            patch(
                "src.core.security.middleware.APIKeyMiddleware.dispatch", new=mock_dispatch
            ),
            patch(
                "src.core.services.session_service.SessionService.get_session",
                new=mock_get_session,
            ),
        ):
            # Create a test client
            client = TestClient(app)
    
            # Send a PWD command
            response = client.post(
                "/v2/chat/completions",
                json={
                    "model": "gpt-3.5-turbo",
                    "messages": [{"role": "user", "content": "!/pwd"}],
                    "session_id": "test-pwd-session",
                },
            )
    
            # Verify the response
>           assert response.status_code == 200
E           assert 500 == 200
E            +  where 500 = <Response [500 Internal Server Error]>.status_code

tests\integration\test_pwd_command_integration.py:152: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:36.166235Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:36.166235Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:36.174236Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:36.187736Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
------------------------------ Captured log call ------------------------------
INFO     src.core.app.controllers.chat_controller:chat_controller.py:59 Handling chat completion request: model=gpt-3.5-turbo
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openai: {}
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
ERROR    src.core.services.request_processor:request_processor.py:259 Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 79, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 85, in _handle_non_streaming_response
    raise HTTPException(
fastapi.exceptions.HTTPException: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 215, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
____ TestToolCallLoopDetection.test_break_mode_blocks_repeated_tool_calls _____

self = <tests.integration.test_tool_call_loop_detection.TestToolCallLoopDetection object at 0x00000150F70C39A0>
test_client = <starlette.testclient.TestClient object at 0x00000150FB28CEB0>
mock_backend = <function chat_completions at 0x00000150F9B15F30>

    def test_break_mode_blocks_repeated_tool_calls(self, test_client, mock_backend):
        """Test that break mode blocks repeated tool calls."""
        # Configure the mock to return a response with tool calls
        tool_calls = [
            {
                "id": "call_abc123",
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "arguments": '{"location": "New York"}',
                },
            }
        ]
        # For the third call (after threshold), return an error response
        error_response = {
            "id": "chatcmpl-123",
            "object": "chat.completion",
            "created": 1677858242,
            "model": "gpt-4",
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": "Tool call loop detected: 'get_weather' invoked with identical parameters 3 times within 60s. Session stopped to prevent unintended looping. Try changing your inputs or approach.",
                    },
                    "finish_reason": "error",
                }
            ],
            "usage": {"prompt_tokens": 10, "completion_tokens": 10, "total_tokens": 20},
        }
        # Need to provide enough responses for all backend calls
        mock_backend.side_effect = [
            create_mock_response(tool_calls),
            create_mock_response(tool_calls),
            error_response,
        ]
    
        # Make multiple requests with the same tool call
        for _ in range(2):  # Below threshold
            response = test_client.post(
                "/v1/chat/completions",
                json=create_chat_completion_request(tool_calls=True),
            )
>           assert response.status_code == 200
E           assert 500 == 200
E            +  where 500 = <Response [500 Internal Server Error]>.status_code

tests\integration\test_tool_call_loop_detection.py:170: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:36.408234Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:36.408234Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:36.416233Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:36.429735Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openai: {}
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
ERROR    src.core.services.request_processor:request_processor.py:259 Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 79, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 85, in _handle_non_streaming_response
    raise HTTPException(
fastapi.exceptions.HTTPException: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 215, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
_ TestToolCallLoopDetection.test_chance_then_break_mode_transparent_retry_success _

self = <tests.integration.test_tool_call_loop_detection.TestToolCallLoopDetection object at 0x00000150F70C3B50>
test_client = <starlette.testclient.TestClient object at 0x00000150F9BF9060>
mock_backend = <function chat_completions at 0x00000150FB1AD750>

    def test_chance_then_break_mode_transparent_retry_success(
        self, test_client, mock_backend
    ):
        """Test chance_then_break performs a transparent retry that succeeds (different tool args)."""
        # Update the app config to use chance_then_break mode
        test_client.app.state.tool_loop_config = ToolCallLoopConfig(
            enabled=True,
            max_repeats=3,
            ttl_seconds=60,
            mode=ToolLoopMode.CHANCE_THEN_BREAK,
        )
    
        # Configure the mock to return a response with tool calls
        tool_calls = [
            {
                "id": "call_abc123",
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "arguments": '{"location": "New York"}',
                },
            }
        ]
        # Warm-up calls below threshold use the same repeating response
        mock_backend.return_value = create_mock_response(tool_calls)
    
        # Make multiple requests with the same tool call
        for _ in range(2):  # Below threshold
            response = test_client.post(
                "/v1/chat/completions",
                json=create_chat_completion_request(tool_calls=True),
            )
>           assert response.status_code == 200
E           assert 500 == 200
E            +  where 500 = <Response [500 Internal Server Error]>.status_code

tests\integration\test_tool_call_loop_detection.py:224: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:36.444735Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:36.445235Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:36.452736Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:36.466236Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openai: {}
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
ERROR    src.core.services.request_processor:request_processor.py:259 Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 79, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 85, in _handle_non_streaming_response
    raise HTTPException(
fastapi.exceptions.HTTPException: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 215, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
_ TestToolCallLoopDetection.test_chance_then_break_mode_transparent_retry_fail _

self = <tests.integration.test_tool_call_loop_detection.TestToolCallLoopDetection object at 0x00000150F70C3D00>
test_client = <starlette.testclient.TestClient object at 0x00000150FB1E06A0>
mock_backend = <function chat_completions at 0x00000150F9B14CA0>

    def test_chance_then_break_mode_transparent_retry_fail(
        self, test_client, mock_backend
    ):
        """Test chance_then_break performs a transparent retry that fails (same tool args again)."""
        test_client.app.state.tool_loop_config = ToolCallLoopConfig(
            enabled=True,
            max_repeats=3,
            ttl_seconds=60,
            mode=ToolLoopMode.CHANCE_THEN_BREAK,
        )
    
        tool_calls = [
            {
                "id": "call_abc123",
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "arguments": '{"location": "New York"}',
                },
            }
        ]
    
        # Below threshold warm-up
        mock_backend.return_value = create_mock_response(tool_calls)
        for _ in range(2):
            response = test_client.post(
                "/v1/chat/completions",
                json=create_chat_completion_request(tool_calls=True),
            )
>           assert response.status_code == 200
E           assert 500 == 200
E            +  where 500 = <Response [500 Internal Server Error]>.status_code

tests\integration\test_tool_call_loop_detection.py:292: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:36.481235Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:36.481235Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:36.488734Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:36.503235Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openai: {}
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
ERROR    src.core.services.request_processor:request_processor.py:259 Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 79, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 85, in _handle_non_streaming_response
    raise HTTPException(
fastapi.exceptions.HTTPException: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 215, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
_______ TestToolCallLoopDetection.test_different_tool_calls_not_blocked _______

self = <tests.integration.test_tool_call_loop_detection.TestToolCallLoopDetection object at 0x00000150F70C3EB0>
test_client = <starlette.testclient.TestClient object at 0x00000150FB5E8E50>
mock_backend = <function chat_completions at 0x00000150FB0F2A70>

    def test_different_tool_calls_not_blocked(self, test_client, mock_backend):
        """Test that different tool calls are not blocked."""
        # Configure the mock to return responses with different tool calls
        tool_calls1 = [
            {
                "id": "call_abc123",
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "arguments": '{"location": "New York"}',
                },
            }
        ]
        mock_response1 = create_mock_response(tool_calls1)
    
        tool_calls2 = [
            {
                "id": "call_def456",
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "arguments": '{"location": "London"}',
                },
            }
        ]
        mock_response2 = create_mock_response(tool_calls2)
    
        # Alternate between different tool calls
        mock_backend.side_effect = [mock_response1, mock_response2] * 3
    
        # Make multiple requests with alternating tool calls
        for _ in range(6):  # Well above threshold, but alternating
            response = test_client.post(
                "/v1/chat/completions",
                json=create_chat_completion_request(tool_calls=True),
            )
>           assert response.status_code == 200
E           assert 500 == 200
E            +  where 500 = <Response [500 Internal Server Error]>.status_code

tests\integration\test_tool_call_loop_detection.py:363: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:36.517235Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:36.517735Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:36.524743Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:36.538233Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openai: {}
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
ERROR    src.core.services.request_processor:request_processor.py:259 Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 79, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 85, in _handle_non_streaming_response
    raise HTTPException(
fastapi.exceptions.HTTPException: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 215, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
______ TestToolCallLoopDetection.test_disabled_tool_call_loop_detection _______

self = <tests.integration.test_tool_call_loop_detection.TestToolCallLoopDetection object at 0x00000150F70F80A0>
test_client = <starlette.testclient.TestClient object at 0x00000150FB7F0940>
mock_backend = <function chat_completions at 0x00000150FB11D2D0>

    def test_disabled_tool_call_loop_detection(self, test_client, mock_backend):
        """Test that disabled tool call loop detection doesn't block repeated tool calls."""
        # Update the app config to disable tool call loop detection
        test_client.app.state.tool_loop_config = ToolCallLoopConfig(
            enabled=False,
            max_repeats=3,
            ttl_seconds=60,
            mode=ToolLoopMode.BREAK,
        )
    
        # Configure the mock to return a response with tool calls
        tool_calls = [
            {
                "id": "call_abc123",
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "arguments": '{"location": "New York"}',
                },
            }
        ]
        # Need to provide enough responses for all backend calls
        mock_backend.side_effect = [create_mock_response(tool_calls)] * 10
    
        # Make multiple requests with the same tool call (well above threshold)
        for _ in range(6):
            response = test_client.post(
                "/v1/chat/completions",
                json=create_chat_completion_request(tool_calls=True),
            )
>           assert response.status_code == 200
E           assert 500 == 200
E            +  where 500 = <Response [500 Internal Server Error]>.status_code

tests\integration\test_tool_call_loop_detection.py:403: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:36.551736Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:36.551736Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:36.559234Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:36.572735Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openai: {}
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
ERROR    src.core.services.request_processor:request_processor.py:259 Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 79, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 85, in _handle_non_streaming_response
    raise HTTPException(
fastapi.exceptions.HTTPException: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 215, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
________________________ test_hybrid_chat_completions _________________________

setup_app = {'app': <fastapi.applications.FastAPI object at 0x00000150F8A802B0>, 'mock_provider': <MagicMock id='1447329121616'>, 'mock_request_processor': <AsyncMock name='mock.get_service()' id='1447329128528'>}

    def test_hybrid_chat_completions(setup_app):
        """Test that hybrid_chat_completions uses the request processor correctly."""
        # Create test client
        client = TestClient(setup_app["app"])
    
        # Make a request to the hybrid endpoint
        response = client.post(
            "/v2/chat/completions",
            json={
                "model": "test-model",
                "messages": [{"role": "user", "content": "Test message"}],
            },
        )
    
        # Verify that the request was processed by the mock
        setup_app["mock_request_processor"].process_request.assert_called_once()
    
        # Check response
        assert response.status_code == 200
>       assert response.json() == {"message": "processed"}
E       AssertionError: assert {} == {'message': 'processed'}
E         
E         Right contains 1 more item:
E         {'message': 'processed'}
E         
E         Full diff:
E         + {}
E         - {
E         -     'message': 'processed',
E         - }

tests\integration\test_updated_hybrid_controller.py:117: AssertionError
_______________________ test_hybrid_anthropic_messages ________________________

setup_app = {'app': <fastapi.applications.FastAPI object at 0x00000150FACF70D0>, 'mock_provider': <MagicMock id='1447317724496'>, 'mock_request_processor': <AsyncMock name='mock.get_service()' id='1447329268352'>}

    def test_hybrid_anthropic_messages(setup_app):
        """Test that hybrid_anthropic_messages uses the request processor correctly."""
        # Create test client
        client = TestClient(setup_app["app"])
    
        # Make a request to the hybrid endpoint
>       response = client.post(
            "/v2/anthropic/messages",
            json={
                "model": "claude-3-sonnet",
                "messages": [{"role": "user", "content": "Test message"}],
                "max_tokens": 100,
            },
        )

tests\integration\test_updated_hybrid_controller.py:147: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient.TestClient object at 0x00000150FACF4100>
url = '/v2/anthropic/messages'

    def post(  # type: ignore[override]
        self,
        url: httpx._types.URLTypes,
        *,
        content: httpx._types.RequestContent | None = None,
        data: _RequestData | None = None,
        files: httpx._types.RequestFiles | None = None,
        json: Any = None,
        params: httpx._types.QueryParamTypes | None = None,
        headers: httpx._types.HeaderTypes | None = None,
        cookies: httpx._types.CookieTypes | None = None,
        auth: httpx._types.AuthTypes | httpx._client.UseClientDefault = httpx._client.USE_CLIENT_DEFAULT,
        follow_redirects: bool | httpx._client.UseClientDefault = httpx._client.USE_CLIENT_DEFAULT,
        timeout: httpx._types.TimeoutTypes | httpx._client.UseClientDefault = httpx._client.USE_CLIENT_DEFAULT,
        extensions: dict[str, Any] | None = None,
    ) -> httpx.Response:
>       return super().post(
            url,
            content=content,
            data=data,
            files=files,
            json=json,
            params=params,
            headers=headers,
            cookies=cookies,
            auth=auth,
            follow_redirects=follow_redirects,
            timeout=timeout,
            extensions=extensions,
        )

.venv\lib\site-packages\starlette\testclient.py:552: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient.TestClient object at 0x00000150FACF4100>
url = '/v2/anthropic/messages'

    def post(
        self,
        url: URL | str,
        *,
        content: RequestContent | None = None,
        data: RequestData | None = None,
        files: RequestFiles | None = None,
        json: typing.Any | None = None,
        params: QueryParamTypes | None = None,
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> Response:
        """
        Send a `POST` request.
    
        **Parameters**: See `httpx.request`.
        """
>       return self.request(
            "POST",
            url,
            content=content,
            data=data,
            files=files,
            json=json,
            params=params,
            headers=headers,
            cookies=cookies,
            auth=auth,
            follow_redirects=follow_redirects,
            timeout=timeout,
            extensions=extensions,
        )

.venv\lib\site-packages\httpx\_client.py:1144: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient.TestClient object at 0x00000150FACF4100>
method = 'POST', url = URL('http://testserver/v2/anthropic/messages')

    def request(  # type: ignore[override]
        self,
        method: str,
        url: httpx._types.URLTypes,
        *,
        content: httpx._types.RequestContent | None = None,
        data: _RequestData | None = None,
        files: httpx._types.RequestFiles | None = None,
        json: Any = None,
        params: httpx._types.QueryParamTypes | None = None,
        headers: httpx._types.HeaderTypes | None = None,
        cookies: httpx._types.CookieTypes | None = None,
        auth: httpx._types.AuthTypes | httpx._client.UseClientDefault = httpx._client.USE_CLIENT_DEFAULT,
        follow_redirects: bool | httpx._client.UseClientDefault = httpx._client.USE_CLIENT_DEFAULT,
        timeout: httpx._types.TimeoutTypes | httpx._client.UseClientDefault = httpx._client.USE_CLIENT_DEFAULT,
        extensions: dict[str, Any] | None = None,
    ) -> httpx.Response:
        if timeout is not httpx.USE_CLIENT_DEFAULT:
            warnings.warn(
                "You should not use the 'timeout' argument with the TestClient. "
                "See https://github.com/encode/starlette/issues/1108 for more information.",
                DeprecationWarning,
            )
        url = self._merge_url(url)
>       return super().request(
            method,
            url,
            content=content,
            data=data,
            files=files,
            json=json,
            params=params,
            headers=headers,
            cookies=cookies,
            auth=auth,
            follow_redirects=follow_redirects,
            timeout=timeout,
            extensions=extensions,
        )

.venv\lib\site-packages\starlette\testclient.py:451: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient.TestClient object at 0x00000150FACF4100>
method = 'POST', url = URL('http://testserver/v2/anthropic/messages')

    def request(
        self,
        method: str,
        url: URL | str,
        *,
        content: RequestContent | None = None,
        data: RequestData | None = None,
        files: RequestFiles | None = None,
        json: typing.Any | None = None,
        params: QueryParamTypes | None = None,
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault | None = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> Response:
        """
        Build and send a request.
    
        Equivalent to:
    
        ```python
        request = client.build_request(...)
        response = client.send(request, ...)
        ```
    
        See `Client.build_request()`, `Client.send()` and
        [Merging of configuration][0] for how the various parameters
        are merged with client-level configuration.
    
        [0]: /advanced/clients/#merging-of-configuration
        """
        if cookies is not None:
            message = (
                "Setting per-request cookies=<...> is being deprecated, because "
                "the expected behaviour on cookie persistence is ambiguous. Set "
                "cookies directly on the client instance instead."
            )
            warnings.warn(message, DeprecationWarning, stacklevel=2)
    
        request = self.build_request(
            method=method,
            url=url,
            content=content,
            data=data,
            files=files,
            json=json,
            params=params,
            headers=headers,
            cookies=cookies,
            timeout=timeout,
            extensions=extensions,
        )
>       return self.send(request, auth=auth, follow_redirects=follow_redirects)

.venv\lib\site-packages\httpx\_client.py:825: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient.TestClient object at 0x00000150FACF4100>
request = <Request('POST', 'http://testserver/v2/anthropic/messages')>

    def send(
        self,
        request: Request,
        *,
        stream: bool = False,
        auth: AuthTypes | UseClientDefault | None = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
    ) -> Response:
        """
        Send a request.
    
        The request is sent as-is, unmodified.
    
        Typically you'll want to build one with `Client.build_request()`
        so that any client-level configuration is merged into the request,
        but passing an explicit `httpx.Request()` is supported as well.
    
        See also: [Request instances][0]
    
        [0]: /advanced/clients/#request-instances
        """
        if self._state == ClientState.CLOSED:
            raise RuntimeError("Cannot send a request, as the client has been closed.")
    
        self._state = ClientState.OPENED
        follow_redirects = (
            self.follow_redirects
            if isinstance(follow_redirects, UseClientDefault)
            else follow_redirects
        )
    
        self._set_timeout(request)
    
        auth = self._build_request_auth(request, auth)
    
>       response = self._send_handling_auth(
            request,
            auth=auth,
            follow_redirects=follow_redirects,
            history=[],
        )

.venv\lib\site-packages\httpx\_client.py:914: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient.TestClient object at 0x00000150FACF4100>
request = <Request('POST', 'http://testserver/v2/anthropic/messages')>
auth = <httpx.Auth object at 0x00000150FACF65C0>, follow_redirects = True
history = []

    def _send_handling_auth(
        self,
        request: Request,
        auth: Auth,
        follow_redirects: bool,
        history: list[Response],
    ) -> Response:
        auth_flow = auth.sync_auth_flow(request)
        try:
            request = next(auth_flow)
    
            while True:
>               response = self._send_handling_redirects(
                    request,
                    follow_redirects=follow_redirects,
                    history=history,
                )

.venv\lib\site-packages\httpx\_client.py:942: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient.TestClient object at 0x00000150FACF4100>
request = <Request('POST', 'http://testserver/v2/anthropic/messages')>
follow_redirects = True, history = []

    def _send_handling_redirects(
        self,
        request: Request,
        follow_redirects: bool,
        history: list[Response],
    ) -> Response:
        while True:
            if len(history) > self.max_redirects:
                raise TooManyRedirects(
                    "Exceeded maximum allowed redirects.", request=request
                )
    
            for hook in self._event_hooks["request"]:
                hook(request)
    
>           response = self._send_single_request(request)

.venv\lib\site-packages\httpx\_client.py:979: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient.TestClient object at 0x00000150FACF4100>
request = <Request('POST', 'http://testserver/v2/anthropic/messages')>

    def _send_single_request(self, request: Request) -> Response:
        """
        Sends a single request, without handling any redirections.
        """
        transport = self._transport_for_url(request.url)
        start = time.perf_counter()
    
        if not isinstance(request.stream, SyncByteStream):
            raise RuntimeError(
                "Attempted to send an async request with a sync Client instance."
            )
    
        with request_context(request=request):
>           response = transport.handle_request(request)

.venv\lib\site-packages\httpx\_client.py:1014: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient._TestClientTransport object at 0x00000150FACF4520>
request = <Request('POST', 'http://testserver/v2/anthropic/messages')>

    def handle_request(self, request: httpx.Request) -> httpx.Response:
        scheme = request.url.scheme
        netloc = request.url.netloc.decode(encoding="ascii")
        path = request.url.path
        raw_path = request.url.raw_path
        query = request.url.query.decode(encoding="ascii")
    
        default_port = {"http": 80, "ws": 80, "https": 443, "wss": 443}[scheme]
    
        if ":" in netloc:
            host, port_string = netloc.split(":", 1)
            port = int(port_string)
        else:
            host = netloc
            port = default_port
    
        # Include the 'host' header.
        if "host" in request.headers:
            headers: list[tuple[bytes, bytes]] = []
        elif port == default_port:  # pragma: no cover
            headers = [(b"host", host.encode())]
        else:  # pragma: no cover
            headers = [(b"host", (f"{host}:{port}").encode())]
    
        # Include other request headers.
        headers += [(key.lower().encode(), value.encode()) for key, value in request.headers.multi_items()]
    
        scope: dict[str, Any]
    
        if scheme in {"ws", "wss"}:
            subprotocol = request.headers.get("sec-websocket-protocol", None)
            if subprotocol is None:
                subprotocols: Sequence[str] = []
            else:
                subprotocols = [value.strip() for value in subprotocol.split(",")]
            scope = {
                "type": "websocket",
                "path": unquote(path),
                "raw_path": raw_path.split(b"?", 1)[0],
                "root_path": self.root_path,
                "scheme": scheme,
                "query_string": query.encode(),
                "headers": headers,
                "client": self.client,
                "server": [host, port],
                "subprotocols": subprotocols,
                "state": self.app_state.copy(),
                "extensions": {"websocket.http.response": {}},
            }
            session = WebSocketTestSession(self.app, scope, self.portal_factory)
            raise _Upgrade(session)
    
        scope = {
            "type": "http",
            "http_version": "1.1",
            "method": request.method,
            "path": unquote(path),
            "raw_path": raw_path.split(b"?", 1)[0],
            "root_path": self.root_path,
            "scheme": scheme,
            "query_string": query.encode(),
            "headers": headers,
            "client": self.client,
            "server": [host, port],
            "extensions": {"http.response.debug": {}},
            "state": self.app_state.copy(),
        }
    
        request_complete = False
        response_started = False
        response_complete: anyio.Event
        raw_kwargs: dict[str, Any] = {"stream": io.BytesIO()}
        template = None
        context = None
    
        async def receive() -> Message:
            nonlocal request_complete
    
            if request_complete:
                if not response_complete.is_set():
                    await response_complete.wait()
                return {"type": "http.disconnect"}
    
            body = request.read()
            if isinstance(body, str):
                body_bytes: bytes = body.encode("utf-8")  # pragma: no cover
            elif body is None:
                body_bytes = b""  # pragma: no cover
            elif isinstance(body, GeneratorType):
                try:  # pragma: no cover
                    chunk = body.send(None)
                    if isinstance(chunk, str):
                        chunk = chunk.encode("utf-8")
                    return {"type": "http.request", "body": chunk, "more_body": True}
                except StopIteration:  # pragma: no cover
                    request_complete = True
                    return {"type": "http.request", "body": b""}
            else:
                body_bytes = body
    
            request_complete = True
            return {"type": "http.request", "body": body_bytes}
    
        async def send(message: Message) -> None:
            nonlocal raw_kwargs, response_started, template, context
    
            if message["type"] == "http.response.start":
                assert not response_started, 'Received multiple "http.response.start" messages.'
                raw_kwargs["status_code"] = message["status"]
                raw_kwargs["headers"] = [(key.decode(), value.decode()) for key, value in message.get("headers", [])]
                response_started = True
            elif message["type"] == "http.response.body":
                assert response_started, 'Received "http.response.body" without "http.response.start".'
                assert not response_complete.is_set(), 'Received "http.response.body" after response completed.'
                body = message.get("body", b"")
                more_body = message.get("more_body", False)
                if request.method != "HEAD":
                    raw_kwargs["stream"].write(body)
                if not more_body:
                    raw_kwargs["stream"].seek(0)
                    response_complete.set()
            elif message["type"] == "http.response.debug":
                template = message["info"]["template"]
                context = message["info"]["context"]
    
        try:
            with self.portal_factory() as portal:
                response_complete = portal.call(anyio.Event)
                portal.call(self.app, scope, receive, send)
        except BaseException as exc:
            if self.raise_server_exceptions:
>               raise exc

.venv\lib\site-packages\starlette\testclient.py:354: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.testclient._TestClientTransport object at 0x00000150FACF4520>
request = <Request('POST', 'http://testserver/v2/anthropic/messages')>

    def handle_request(self, request: httpx.Request) -> httpx.Response:
        scheme = request.url.scheme
        netloc = request.url.netloc.decode(encoding="ascii")
        path = request.url.path
        raw_path = request.url.raw_path
        query = request.url.query.decode(encoding="ascii")
    
        default_port = {"http": 80, "ws": 80, "https": 443, "wss": 443}[scheme]
    
        if ":" in netloc:
            host, port_string = netloc.split(":", 1)
            port = int(port_string)
        else:
            host = netloc
            port = default_port
    
        # Include the 'host' header.
        if "host" in request.headers:
            headers: list[tuple[bytes, bytes]] = []
        elif port == default_port:  # pragma: no cover
            headers = [(b"host", host.encode())]
        else:  # pragma: no cover
            headers = [(b"host", (f"{host}:{port}").encode())]
    
        # Include other request headers.
        headers += [(key.lower().encode(), value.encode()) for key, value in request.headers.multi_items()]
    
        scope: dict[str, Any]
    
        if scheme in {"ws", "wss"}:
            subprotocol = request.headers.get("sec-websocket-protocol", None)
            if subprotocol is None:
                subprotocols: Sequence[str] = []
            else:
                subprotocols = [value.strip() for value in subprotocol.split(",")]
            scope = {
                "type": "websocket",
                "path": unquote(path),
                "raw_path": raw_path.split(b"?", 1)[0],
                "root_path": self.root_path,
                "scheme": scheme,
                "query_string": query.encode(),
                "headers": headers,
                "client": self.client,
                "server": [host, port],
                "subprotocols": subprotocols,
                "state": self.app_state.copy(),
                "extensions": {"websocket.http.response": {}},
            }
            session = WebSocketTestSession(self.app, scope, self.portal_factory)
            raise _Upgrade(session)
    
        scope = {
            "type": "http",
            "http_version": "1.1",
            "method": request.method,
            "path": unquote(path),
            "raw_path": raw_path.split(b"?", 1)[0],
            "root_path": self.root_path,
            "scheme": scheme,
            "query_string": query.encode(),
            "headers": headers,
            "client": self.client,
            "server": [host, port],
            "extensions": {"http.response.debug": {}},
            "state": self.app_state.copy(),
        }
    
        request_complete = False
        response_started = False
        response_complete: anyio.Event
        raw_kwargs: dict[str, Any] = {"stream": io.BytesIO()}
        template = None
        context = None
    
        async def receive() -> Message:
            nonlocal request_complete
    
            if request_complete:
                if not response_complete.is_set():
                    await response_complete.wait()
                return {"type": "http.disconnect"}
    
            body = request.read()
            if isinstance(body, str):
                body_bytes: bytes = body.encode("utf-8")  # pragma: no cover
            elif body is None:
                body_bytes = b""  # pragma: no cover
            elif isinstance(body, GeneratorType):
                try:  # pragma: no cover
                    chunk = body.send(None)
                    if isinstance(chunk, str):
                        chunk = chunk.encode("utf-8")
                    return {"type": "http.request", "body": chunk, "more_body": True}
                except StopIteration:  # pragma: no cover
                    request_complete = True
                    return {"type": "http.request", "body": b""}
            else:
                body_bytes = body
    
            request_complete = True
            return {"type": "http.request", "body": body_bytes}
    
        async def send(message: Message) -> None:
            nonlocal raw_kwargs, response_started, template, context
    
            if message["type"] == "http.response.start":
                assert not response_started, 'Received multiple "http.response.start" messages.'
                raw_kwargs["status_code"] = message["status"]
                raw_kwargs["headers"] = [(key.decode(), value.decode()) for key, value in message.get("headers", [])]
                response_started = True
            elif message["type"] == "http.response.body":
                assert response_started, 'Received "http.response.body" without "http.response.start".'
                assert not response_complete.is_set(), 'Received "http.response.body" after response completed.'
                body = message.get("body", b"")
                more_body = message.get("more_body", False)
                if request.method != "HEAD":
                    raw_kwargs["stream"].write(body)
                if not more_body:
                    raw_kwargs["stream"].seek(0)
                    response_complete.set()
            elif message["type"] == "http.response.debug":
                template = message["info"]["template"]
                context = message["info"]["context"]
    
        try:
            with self.portal_factory() as portal:
                response_complete = portal.call(anyio.Event)
>               portal.call(self.app, scope, receive, send)

.venv\lib\site-packages\starlette\testclient.py:351: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <anyio._backends._asyncio.BlockingPortal object at 0x00000150F9BE9960>
func = <fastapi.applications.FastAPI object at 0x00000150FACF70D0>
args = ({'app': <fastapi.applications.FastAPI object at 0x00000150FACF70D0>, 'client': ('testclient', 50000), 'endpoint': <fu...ls>.receive at 0x00000150FB11E560>, <function _TestClientTransport.handle_request.<locals>.send at 0x00000150FB0F35B0>)

    def call(
        self,
        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval] | T_Retval],
        *args: Unpack[PosArgsT],
    ) -> T_Retval:
        """
        Call the given function in the event loop thread.
    
        If the callable returns a coroutine object, it is awaited on.
    
        :param func: any callable
        :raises RuntimeError: if the portal is not running or if this method is called
            from within the event loop thread
    
        """
>       return cast(T_Retval, self.start_task_soon(func, *args).result())

.venv\lib\site-packages\anyio\from_thread.py:291: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = None, timeout = None

    def result(self, timeout=None):
        """Return the result of the call that the future represents.
    
        Args:
            timeout: The number of seconds to wait for the result if the future
                isn't done. If None, then there is no limit on the wait time.
    
        Returns:
            The result of the call that the future represents.
    
        Raises:
            CancelledError: If the future was cancelled.
            TimeoutError: If the future didn't finish executing before the given
                timeout.
            Exception: If the call raised then that exception will be raised.
        """
        try:
            with self._condition:
                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
                    raise CancelledError()
                elif self._state == FINISHED:
                    return self.__get_result()
    
                self._condition.wait(timeout)
    
                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
                    raise CancelledError()
                elif self._state == FINISHED:
>                   return self.__get_result()

C:\Program Files\Python310\lib\concurrent\futures\_base.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = None

    def __get_result(self):
        if self._exception:
            try:
>               raise self._exception

C:\Program Files\Python310\lib\concurrent\futures\_base.py:403: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <anyio._backends._asyncio.BlockingPortal object at 0x00000150F9BE9960>
func = <fastapi.applications.FastAPI object at 0x00000150FACF70D0>
args = ({'app': <fastapi.applications.FastAPI object at 0x00000150FACF70D0>, 'client': ('testclient', 50000), 'endpoint': <fu...ls>.receive at 0x00000150FB11E560>, <function _TestClientTransport.handle_request.<locals>.send at 0x00000150FB0F35B0>)
kwargs = {}, future = <Future at 0x150fb5ea0e0 state=finished raised KeyError>

    async def _call_func(
        self,
        func: Callable[[Unpack[PosArgsT]], Awaitable[T_Retval] | T_Retval],
        args: tuple[Unpack[PosArgsT]],
        kwargs: dict[str, Any],
        future: Future[T_Retval],
    ) -> None:
        def callback(f: Future[T_Retval]) -> None:
            if f.cancelled() and self._event_loop_thread_id not in (
                None,
                get_ident(),
            ):
                self.call(scope.cancel)
    
        try:
            retval_or_awaitable = func(*args, **kwargs)
            if isawaitable(retval_or_awaitable):
                with CancelScope() as scope:
                    if future.cancelled():
                        scope.cancel()
                    else:
                        future.add_done_callback(callback)
    
>                   retval = await retval_or_awaitable

.venv\lib\site-packages\anyio\from_thread.py:222: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <fastapi.applications.FastAPI object at 0x00000150FACF70D0>
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150FACF70D0>, 'client': ('testclient', 50000), 'endpoint': <function hybrid_anthropic_messages at 0x00000150F77913F0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150FB11E560>
send = <function _TestClientTransport.handle_request.<locals>.send at 0x00000150FB0F35B0>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if self.root_path:
            scope["root_path"] = self.root_path
>       await super().__call__(scope, receive, send)

.venv\lib\site-packages\fastapi\applications.py:1054: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <fastapi.applications.FastAPI object at 0x00000150FACF70D0>
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150FACF70D0>, 'client': ('testclient', 50000), 'endpoint': <function hybrid_anthropic_messages at 0x00000150F77913F0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150FB11E560>
send = <function _TestClientTransport.handle_request.<locals>.send at 0x00000150FB0F35B0>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        scope["app"] = self
        if self.middleware_stack is None:
            self.middleware_stack = self.build_middleware_stack()
>       await self.middleware_stack(scope, receive, send)

.venv\lib\site-packages\starlette\applications.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.middleware.errors.ServerErrorMiddleware object at 0x00000150F9BEAB00>
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150FACF70D0>, 'client': ('testclient', 50000), 'endpoint': <function hybrid_anthropic_messages at 0x00000150F77913F0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150FB11E560>
send = <function _TestClientTransport.handle_request.<locals>.send at 0x00000150FB0F35B0>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        response_started = False
    
        async def _send(message: Message) -> None:
            nonlocal response_started, send
    
            if message["type"] == "http.response.start":
                response_started = True
            await send(message)
    
        try:
            await self.app(scope, receive, _send)
        except Exception as exc:
            request = Request(scope)
            if self.debug:
                # In debug mode, return traceback responses.
                response = self.debug_response(request, exc)
            elif self.handler is None:
                # Use our default 500 error handler.
                response = self.error_response(request, exc)
            else:
                # Use an installed 500 error handler.
                if is_async_callable(self.handler):
                    response = await self.handler(request, exc)
                else:
                    response = await run_in_threadpool(self.handler, request, exc)
    
            if not response_started:
                await response(scope, receive, send)
    
            # We always continue to raise the exception.
            # This allows servers to log the error, or allows test clients
            # to optionally raise the error within the test case.
>           raise exc

.venv\lib\site-packages\starlette\middleware\errors.py:186: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.middleware.errors.ServerErrorMiddleware object at 0x00000150F9BEAB00>
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150FACF70D0>, 'client': ('testclient', 50000), 'endpoint': <function hybrid_anthropic_messages at 0x00000150F77913F0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150FB11E560>
send = <function _TestClientTransport.handle_request.<locals>.send at 0x00000150FB0F35B0>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        response_started = False
    
        async def _send(message: Message) -> None:
            nonlocal response_started, send
    
            if message["type"] == "http.response.start":
                response_started = True
            await send(message)
    
        try:
>           await self.app(scope, receive, _send)

.venv\lib\site-packages\starlette\middleware\errors.py:164: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.middleware.exceptions.ExceptionMiddleware object at 0x00000150F9BEBA30>
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150FACF70D0>, 'client': ('testclient', 50000), 'endpoint': <function hybrid_anthropic_messages at 0x00000150F77913F0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150FB11E560>
send = <function ServerErrorMiddleware.__call__.<locals>._send at 0x00000150FB854940>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] not in ("http", "websocket"):
            await self.app(scope, receive, send)
            return
    
        scope["starlette.exception_handlers"] = (
            self._exception_handlers,
            self._status_handlers,
        )
    
        conn: Request | WebSocket
        if scope["type"] == "http":
            conn = Request(scope, receive, send)
        else:
            conn = WebSocket(scope, receive, send)
    
>       await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)

.venv\lib\site-packages\starlette\middleware\exceptions.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

scope = {'app': <fastapi.applications.FastAPI object at 0x00000150FACF70D0>, 'client': ('testclient', 50000), 'endpoint': <function hybrid_anthropic_messages at 0x00000150F77913F0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150FB11E560>
send = <function ServerErrorMiddleware.__call__.<locals>._send at 0x00000150FB854940>

    async def wrapped_app(scope: Scope, receive: Receive, send: Send) -> None:
        response_started = False
    
        async def sender(message: Message) -> None:
            nonlocal response_started
    
            if message["type"] == "http.response.start":
                response_started = True
            await send(message)
    
        try:
            await app(scope, receive, sender)
        except Exception as exc:
            handler = None
    
            if isinstance(exc, HTTPException):
                handler = status_handlers.get(exc.status_code)
    
            if handler is None:
                handler = _lookup_exception_handler(exception_handlers, exc)
    
            if handler is None:
>               raise exc

.venv\lib\site-packages\starlette\_exception_handler.py:53: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

scope = {'app': <fastapi.applications.FastAPI object at 0x00000150FACF70D0>, 'client': ('testclient', 50000), 'endpoint': <function hybrid_anthropic_messages at 0x00000150F77913F0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150FB11E560>
send = <function ServerErrorMiddleware.__call__.<locals>._send at 0x00000150FB854940>

    async def wrapped_app(scope: Scope, receive: Receive, send: Send) -> None:
        response_started = False
    
        async def sender(message: Message) -> None:
            nonlocal response_started
    
            if message["type"] == "http.response.start":
                response_started = True
            await send(message)
    
        try:
>           await app(scope, receive, sender)

.venv\lib\site-packages\starlette\_exception_handler.py:42: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <fastapi.routing.APIRouter object at 0x00000150FACF6680>
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150FACF70D0>, 'client': ('testclient', 50000), 'endpoint': <function hybrid_anthropic_messages at 0x00000150F77913F0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150FB11E560>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x00000150FB8549D0>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        """
        The main entry point to the Router class.
        """
>       await self.middleware_stack(scope, receive, send)

.venv\lib\site-packages\starlette\routing.py:716: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <fastapi.routing.APIRouter object at 0x00000150FACF6680>
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150FACF70D0>, 'client': ('testclient', 50000), 'endpoint': <function hybrid_anthropic_messages at 0x00000150F77913F0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150FB11E560>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x00000150FB8549D0>

    async def app(self, scope: Scope, receive: Receive, send: Send) -> None:
        assert scope["type"] in ("http", "websocket", "lifespan")
    
        if "router" not in scope:
            scope["router"] = self
    
        if scope["type"] == "lifespan":
            await self.lifespan(scope, receive, send)
            return
    
        partial = None
    
        for route in self.routes:
            # Determine if any route matches the incoming scope,
            # and hand over to the matching route if found.
            match, child_scope = route.matches(scope)
            if match == Match.FULL:
                scope.update(child_scope)
>               await route.handle(scope, receive, send)

.venv\lib\site-packages\starlette\routing.py:736: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = APIRoute(path='/v2/anthropic/messages', name='hybrid_anthropic_messages', methods=['POST'])
scope = {'app': <fastapi.applications.FastAPI object at 0x00000150FACF70D0>, 'client': ('testclient', 50000), 'endpoint': <function hybrid_anthropic_messages at 0x00000150F77913F0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150FB11E560>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x00000150FB8549D0>

    async def handle(self, scope: Scope, receive: Receive, send: Send) -> None:
        if self.methods and scope["method"] not in self.methods:
            headers = {"Allow": ", ".join(self.methods)}
            if "app" in scope:
                raise HTTPException(status_code=405, headers=headers)
            else:
                response = PlainTextResponse("Method Not Allowed", status_code=405, headers=headers)
            await response(scope, receive, send)
        else:
>           await self.app(scope, receive, send)

.venv\lib\site-packages\starlette\routing.py:290: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

scope = {'app': <fastapi.applications.FastAPI object at 0x00000150FACF70D0>, 'client': ('testclient', 50000), 'endpoint': <function hybrid_anthropic_messages at 0x00000150F77913F0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150FB11E560>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x00000150FB8549D0>

    async def app(scope: Scope, receive: Receive, send: Send) -> None:
        request = Request(scope, receive, send)
    
        async def app(scope: Scope, receive: Receive, send: Send) -> None:
            response = await f(request)
            await response(scope, receive, send)
    
>       await wrap_app_handling_exceptions(app, request)(scope, receive, send)

.venv\lib\site-packages\starlette\routing.py:78: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

scope = {'app': <fastapi.applications.FastAPI object at 0x00000150FACF70D0>, 'client': ('testclient', 50000), 'endpoint': <function hybrid_anthropic_messages at 0x00000150F77913F0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150FB11E560>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x00000150FB8549D0>

    async def wrapped_app(scope: Scope, receive: Receive, send: Send) -> None:
        response_started = False
    
        async def sender(message: Message) -> None:
            nonlocal response_started
    
            if message["type"] == "http.response.start":
                response_started = True
            await send(message)
    
        try:
            await app(scope, receive, sender)
        except Exception as exc:
            handler = None
    
            if isinstance(exc, HTTPException):
                handler = status_handlers.get(exc.status_code)
    
            if handler is None:
                handler = _lookup_exception_handler(exception_handlers, exc)
    
            if handler is None:
>               raise exc

.venv\lib\site-packages\starlette\_exception_handler.py:53: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

scope = {'app': <fastapi.applications.FastAPI object at 0x00000150FACF70D0>, 'client': ('testclient', 50000), 'endpoint': <function hybrid_anthropic_messages at 0x00000150F77913F0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150FB11E560>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x00000150FB8549D0>

    async def wrapped_app(scope: Scope, receive: Receive, send: Send) -> None:
        response_started = False
    
        async def sender(message: Message) -> None:
            nonlocal response_started
    
            if message["type"] == "http.response.start":
                response_started = True
            await send(message)
    
        try:
>           await app(scope, receive, sender)

.venv\lib\site-packages\starlette\_exception_handler.py:42: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

scope = {'app': <fastapi.applications.FastAPI object at 0x00000150FACF70D0>, 'client': ('testclient', 50000), 'endpoint': <function hybrid_anthropic_messages at 0x00000150F77913F0>, 'extensions': {'http.response.debug': {}}, ...}
receive = <function _TestClientTransport.handle_request.<locals>.receive at 0x00000150FB11E560>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x00000150FB854AF0>

    async def app(scope: Scope, receive: Receive, send: Send) -> None:
>       response = await f(request)

.venv\lib\site-packages\starlette\routing.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

request = <starlette.requests.Request object at 0x00000150F9BEB670>

    async def app(request: Request) -> Response:
        response: Union[Response, None] = None
        async with AsyncExitStack() as file_stack:
            try:
                body: Any = None
                if body_field:
                    if is_body_form:
                        body = await request.form()
                        file_stack.push_async_callback(body.close)
                    else:
                        body_bytes = await request.body()
                        if body_bytes:
                            json_body: Any = Undefined
                            content_type_value = request.headers.get("content-type")
                            if not content_type_value:
                                json_body = await request.json()
                            else:
                                message = email.message.Message()
                                message["content-type"] = content_type_value
                                if message.get_content_maintype() == "application":
                                    subtype = message.get_content_subtype()
                                    if subtype == "json" or subtype.endswith("+json"):
                                        json_body = await request.json()
                            if json_body != Undefined:
                                body = json_body
                            else:
                                body = body_bytes
            except json.JSONDecodeError as e:
                validation_error = RequestValidationError(
                    [
                        {
                            "type": "json_invalid",
                            "loc": ("body", e.pos),
                            "msg": "JSON decode error",
                            "input": {},
                            "ctx": {"error": e.msg},
                        }
                    ],
                    body=e.doc,
                )
                raise validation_error from e
            except HTTPException:
                # If a middleware raises an HTTPException, it should be raised again
                raise
            except Exception as e:
                http_error = HTTPException(
                    status_code=400, detail="There was an error parsing the body"
                )
                raise http_error from e
            errors: List[Any] = []
            async with AsyncExitStack() as async_exit_stack:
                solved_result = await solve_dependencies(
                    request=request,
                    dependant=dependant,
                    body=body,
                    dependency_overrides_provider=dependency_overrides_provider,
                    async_exit_stack=async_exit_stack,
                    embed_body_fields=embed_body_fields,
                )
                errors = solved_result.errors
                if not errors:
>                   raw_response = await run_endpoint_function(
                        dependant=dependant,
                        values=solved_result.values,
                        is_coroutine=is_coroutine,
                    )

.venv\lib\site-packages\fastapi\routing.py:302: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    async def run_endpoint_function(
        *, dependant: Dependant, values: Dict[str, Any], is_coroutine: bool
    ) -> Any:
        # Only called by get_request_handler. Has been split into its own function to
        # facilitate profiling endpoints, since inner functions are harder to profile.
        assert dependant.call is not None, "dependant.call must be a function"
    
        if is_coroutine:
>           return await dependant.call(**values)

.venv\lib\site-packages\fastapi\routing.py:213: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

http_request = <starlette.requests.Request object at 0x00000150F9BEB670>
request_data = AnthropicMessagesRequest(model='claude-3-sonnet', messages=[AnthropicMessage(role='user', content='Test message')], system=None, max_tokens=100, metadata=None, stop_sequences=None, stream=False, temperature=None, top_p=None, top_k=None)
service_provider = <MagicMock id='1447317724496'>

    async def hybrid_anthropic_messages(
        http_request: Request,
        request_data: AnthropicMessagesRequest,
        service_provider: IServiceProvider | None = Depends(
            get_service_provider_if_available
        ),
    ) -> Response:
        """Handle Anthropic messages using the new SOLID architecture.
    
        Args:
            http_request: The HTTP request
            request_data: The parsed request data
            service_provider: The service provider (if available)
    
        Returns:
            The response
        """
        if service_provider is None:
            logger.error(
                "Service provider not available - this should not happen in production"
            )
            raise RuntimeError("Service provider not available")
    
        # Convert Anthropic request to OpenAI format
        from src.anthropic_converters import anthropic_to_openai_request
    
        openai_request_data = anthropic_to_openai_request(request_data)
    
        logger.debug("Using new SOLID architecture for Anthropic request")
    
        # Always use the new RequestProcessor
        request_processor = service_provider.get_required_service(IRequestProcessor)  # type: ignore
        openai_response = await request_processor.process_request(
            http_request, openai_request_data
        )
    
        # Convert the OpenAI response back to Anthropic format
        import json
    
        from fastapi import Response as FastAPIResponse
    
        from src.anthropic_converters import openai_to_anthropic_response
    
        # Parse the OpenAI response JSON
        openai_response_data = json.loads(openai_response.body.decode())
    
        # Convert to Anthropic format
>       anthropic_response_data = openai_to_anthropic_response(openai_response_data)

src\core\integration\hybrid_controller.py:139: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

openai_response = {'message': 'processed'}

    def openai_to_anthropic_response(openai_response: Any) -> dict[str, Any]:
        """Convert an OpenAI chat completion response into Anthropic format."""
        oai_dict = _normalize_openai_response_to_dict(openai_response)
>       choice = oai_dict["choices"][0]
E       KeyError: 'choices'

src\anthropic_converters.py:49: KeyError
____________________ test_versioned_endpoint_with_commands ____________________

initialized_app = <fastapi.applications.FastAPI object at 0x00000150FADA24D0>

    @pytest.mark.asyncio
    async def test_versioned_endpoint_with_commands(initialized_app):
        """Test that the versioned endpoint processes commands."""
        # Create a test client
        client = TestClient(initialized_app)
    
        # Make a request with a command
        response = client.post(
            "/v2/chat/completions",
            json={
                "model": "test-model",
                "messages": [{"role": "user", "content": "!/help"}],
                "session_id": "test-command-session",
            },
            headers={"Authorization": "Bearer test-proxy-key"},
        )
    
        # Check response
>       assert response.status_code == 200
E       assert 500 == 200
E        +  where 500 = <Response [500 Internal Server Error]>.status_code

tests\integration\test_versioned_api.py:164: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:36.989234Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:36.989234Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:36.997234Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:37.012735Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
INFO     src.core.app.controllers.chat_controller:chat_controller.py:59 Handling chat completion request: model=test-model
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openai: {}
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
ERROR    src.core.services.request_processor:request_processor.py:259 Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 79, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 85, in _handle_non_streaming_response
    raise HTTPException(
fastapi.exceptions.HTTPException: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 215, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
_________ TestChatCompletionRegression.test_streaming_chat_completion _________

self = <test_chat_completion_regression.TestChatCompletionRegression object at 0x00000150F70FB2B0>
mock_chat_completions = <AsyncMock name='chat_completions' id='1447326206752'>
mock_initialize = <AsyncMock name='initialize' id='1447317390336'>
test_client = <starlette.testclient.TestClient object at 0x00000150FB5D6500>

    @patch("src.connectors.openai.OpenAIConnector.initialize", new_callable=AsyncMock)
    @patch("src.connectors.openai.OpenAIConnector.chat_completions")
    def test_streaming_chat_completion(
        self, mock_chat_completions, mock_initialize, test_client
    ):
        """Test streaming chat completion functionality."""
    
        async def mock_stream():
            yield 'data: {"id": "1", "choices": [{"delta": {"content": "Hello"}}]}'
            yield 'data: {"id": "2", "choices": [{"delta": {"content": ", world!"}}]}'
            yield "data: [DONE]"
    
        mock_chat_completions.return_value = StreamingResponse(
            mock_stream(), media_type="text/event-stream"
        )
        mock_initialize.return_value = None
    
        # Define a streaming chat completion request
        request_payload = {
            "model": "mock-model",
            "messages": [{"role": "user", "content": "Count to 5"}],
            "max_tokens": 50,
            "temperature": 0.7,
            "stream": True,
        }
    
        headers = {"Authorization": "Bearer test_api_key"}
    
        # Send request to the implementation
        response = test_client.post(
            "/v1/chat/completions", json=request_payload, headers=headers
        )
    
        # Should succeed
        assert response.status_code == 200
    
        # Should have streaming content type
        assert "text/event-stream" in response.headers.get("content-type", "")
    
        # Collect streaming chunks from the response
        chunks = list(self._parse_streaming_response(response))
    
        # Verify we got some chunks
        assert len(chunks) > 0
    
        # Extract content from chunks
        content = self._extract_content_from_chunks(chunks)
    
        # Verify content
        assert len(content) > 0
>       assert "Hello" in content
E       assert 'Hello' in "ERROR: Stream processing failed: 'async for' requires an object with __aiter__ method, got StreamingResponse"

tests\regression\test_chat_completion_regression.py:129: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:37.088734Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:37.088734Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:40 2025-08-17T00:00:37.096733Z [info     ] API Key authentication is disabled [src.core.app.middleware_config]
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:37.110234Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openai: {}
ERROR    src.core.services.response_processor:response_processor.py:199 Error in stream processing: 'async for' requires an object with __aiter__ method, got StreamingResponse
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\response_processor.py", line 137, in _process_stream
    async for chunk in response_iterator:
TypeError: 'async for' requires an object with __aiter__ method, got StreamingResponse
_________________________ test_failover_missing_keys __________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x00000150FB409570>
httpx_mock = <pytest_httpx._httpx_mock.HTTPXMock object at 0x00000150FB408730>

    @pytest.mark.httpx_mock()
    def test_failover_missing_keys(monkeypatch, httpx_mock: HTTPXMock):
        monkeypatch.delenv("OPENROUTER_API_KEY", raising=False)
        for i in range(1, 21):
            monkeypatch.delenv(f"OPENROUTER_API_KEY_{i}", raising=False)
        monkeypatch.delenv("GEMINI_API_KEY", raising=False)
        for i in range(1, 21):
            monkeypatch.delenv(f"GEMINI_API_KEY_{i}", raising=False)
    
        monkeypatch.setenv("GEMINI_API_KEY", "G")
        monkeypatch.setenv("LLM_BACKEND", "gemini")
    
        from fastapi.testclient import TestClient
        from src.core.app import application_factory as app_main
    
        app = app_main.build_app()
        with TestClient(app, headers={"Authorization": "Bearer test-proxy-key"}) as client:
            client.post(
                "/v1/chat/completions",
                json={
                    "model": "d",
                    "messages": [
                        {
                            "role": "user",
                            "content": "!/create-failover-route(name=r,policy=m)",
                        }
                    ],
                },
            )
            client.post(
                "/v1/chat/completions",
                json={
                    "model": "d",
                    "messages": [
                        {
                            "role": "user",
                            "content": "!/route-append(name=r,openrouter:model-x)",
                        }
                    ],
                },
            )
            resp = client.post(
                "/v1/chat/completions",
                json={"model": "r", "messages": [{"role": "user", "content": "hi"}]},
            )
            assert resp.status_code == 500
>           assert resp.json()["detail"]["error"] == "all backends failed"
E           KeyError: 'detail'

tests\unit\chat_completions_tests\test_failover.py:101: KeyError
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:37.870501Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:37.870501Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:37.878487Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:37.963443Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:354 2025-08-17T00:00:37.965443Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: help
INFO     src.core.services.command_service:command_service.py:78 Registered command: backend
INFO     src.core.services.command_service:command_service.py:78 Registered command: model
INFO     src.core.services.command_service:command_service.py:78 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:78 Registered command: project
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:78 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:78 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:78 Registered command: set
INFO     src.core.services.command_service:command_service.py:78 Registered command: unset
INFO     src.core.services.command_service:command_service.py:78 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:78 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:78 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:78 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:78 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:269 Registered 26 command handlers with registry
INFO     src.core.services.command_service:command_service.py:183 Executing command: create-failover-route with session: bfbd8f59-3edd-4b0c-9ad0-1908c04a32ce
INFO     src.core.services.command_service:command_service.py:183 Executing command: route-append with session: 4489457d-1a55-45ee-9feb-b25fb53de8d2
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openai: {}
ERROR    src.core.services.backend_service:backend_service.py:289 Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
ERROR    src.core.services.request_processor:request_processor.py:259 Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 160, in call_completion
    result = await backend.chat_completions(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 79, in chat_completions
    return await self._handle_non_streaming_response(url, payload, headers)
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\connectors\openai.py", line 85, in _handle_non_streaming_response
    raise HTTPException(
fastapi.exceptions.HTTPException: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\request_processor.py", line 215, in process_request
    response = await self._backend_service.call_completion(
  File "C:\Users\Mateusz\source\repos\llm-interactive-proxy\src\core\services\backend_service.py", line 290, in call_completion
    raise BackendError(
src.core.common.exceptions.BackendError: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
WARNING  src.core.app.error_handlers:error_handlers.py:64 HTTP error 500: Error calling backend: Backend call failed: 401: {'error': {'message': 'No auth credentials found', 'code': 401}}
INFO     src.core.app.application_factory:application_factory.py:368 2025-08-17T00:00:37.986946Z [info     ] Shutting down application      [src.core.app.application_factory]
______________ test_openai_frontend_to_gemini_backend_multimodal ______________

client = <starlette.testclient.TestClient object at 0x00000150FB38C910>

    def test_openai_frontend_to_gemini_backend_multimodal(client):
        # Route to Gemini backend explicitly
        client.app.state.backend_type = "gemini"
    
        # Ensure backend exists on app state and patch its chat_completions
        if (
            not hasattr(client.app.state, "gemini_backend")
            or client.app.state.gemini_backend is None
        ):
    
            class _GB:  # minimal stub
                async def chat_completions(self, *args, **kwargs):
                    return {
                        "id": "x",
                        "object": "chat.completion",
                        "created": 0,
                        "model": "gemini:gemini-pro",
                        "choices": [
                            {
                                "index": 0,
                                "message": {"role": "assistant", "content": "ok"},
                                "finish_reason": "stop",
                            }
                        ],
                        "usage": {
                            "prompt_tokens": 0,
                            "completion_tokens": 0,
                            "total_tokens": 0,
                        },
                    }
    
            client.app.state.gemini_backend = _GB()
    
        with patch.object(
            client.app.state.gemini_backend, "chat_completions", new_callable=AsyncMock
        ) as mock_gemini:
            mock_gemini.return_value = {
                "id": "x",
                "object": "chat.completion",
                "created": 0,
                "model": "gemini:gemini-pro",
                "choices": [
                    {
                        "index": 0,
                        "message": {"role": "assistant", "content": "ok"},
                        "finish_reason": "stop",
                    }
                ],
                "usage": {"prompt_tokens": 1, "completion_tokens": 1, "total_tokens": 2},
            }
    
            payload = {
                "model": "gemini:gemini-pro",
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "Describe this"},
                            {
                                "type": "image_url",
                                "image_url": {"url": "http://example.com/img.jpg"},
                            },
                        ],
                    }
                ],
            }
            r = client.post("/v1/chat/completions", json=payload)
>           assert r.status_code == 200
E           assert 401 == 200
E            +  where 401 = <Response [401 Unauthorized]>.status_code

tests\unit\chat_completions_tests\test_multimodal_cross_protocol.py:93: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:38.088443Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:38.088443Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:38.096443Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:38.110446Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for POST /v1/chat/completions from client testclient
______________ test_gemini_frontend_to_openai_backend_multimodal ______________

client = <starlette.testclient.TestClient object at 0x00000150F89FDFF0>

    def test_gemini_frontend_to_openai_backend_multimodal(client):
        # Ensure openrouter backend exists on state and patch it directly
        if (
            not hasattr(client.app.state, "openrouter_backend")
            or client.app.state.openrouter_backend is None
        ):
    
            class _OR:
                async def chat_completions(self, *args, **kwargs):
                    return {
                        "choices": [
                            {
                                "index": 0,
                                "message": {"role": "assistant", "content": "ok"},
                                "finish_reason": "stop",
                            }
                        ]
                    }, {}
    
            client.app.state.openrouter_backend = _OR()
    
        with patch.object(
            client.app.state.openrouter_backend, "chat_completions", new_callable=AsyncMock
        ) as mock_or:
            mock_or.return_value = (
                {
                    "id": "y",
                    "object": "chat.completion",
                    "created": 0,
                    "model": "openrouter:gpt-4",
                    "choices": [
                        {
                            "index": 0,
                            "message": {"role": "assistant", "content": "ok"},
                            "finish_reason": "stop",
                        }
                    ],
                    "usage": {
                        "prompt_tokens": 1,
                        "completion_tokens": 1,
                        "total_tokens": 2,
                    },
                },
                {},
            )
    
            gemini_request = {
                "contents": [
                    {
                        "parts": [
                            {"text": "What is in this image?"},
                            {"inline_data": {"mime_type": "image/png", "data": "aGVsbG8="}},
                        ],
                        "role": "user",
                    }
                ]
            }
            r = client.post(
                "/v1beta/models/openrouter:gpt-4:generateContent", json=gemini_request
            )
>           assert r.status_code == 200
E           assert 401 == 200
E            +  where 401 = <Response [401 Unauthorized]>.status_code

tests\unit\chat_completions_tests\test_multimodal_cross_protocol.py:163: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:38.119444Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:38.119444Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:38.126445Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:38.141443Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for POST /v1beta/models/openrouter:gpt-4:generateContent from client testclient
_______________ test_set_project_dir_command_valid[project-dir] _______________

self = <starlette.datastructures.State object at 0x00000150FB76D5D0>
key = 'session_manager'

    def __getattr__(self, key: Any) -> Any:
        try:
>           return self._state[key]
E           KeyError: 'session_manager'

.venv\lib\site-packages\starlette\datastructures.py:686: KeyError

During handling of the above exception, another exception occurred:

client_with_config = <starlette.testclient.TestClient object at 0x00000150FB76FE20>
tmp_path = WindowsPath('C:/Users/Mateusz/AppData/Local/Temp/pytest-of-Mateusz/pytest-2396/test_set_project_dir_command_v0')
alias = 'project-dir'

    @pytest.mark.parametrize("alias", ["project-dir", "dir", "project-directory"])
    def test_set_project_dir_command_valid(client_with_config: TestClient, tmp_path, alias):
        with patch.object(
            client_with_config.app.state.openrouter_backend,
            "chat_completions",
            new_callable=AsyncMock,
        ) as mock_method:
            mock_method.return_value = {"choices": [{"message": {"content": "ok"}}]}
            valid_dir = tmp_path.resolve()
            payload = {
                "model": "some-model",
                "messages": [{"role": "user", "content": f'!/set({alias}="{valid_dir}")'}],
            }
            response = client_with_config.post("/v1/chat/completions", json=payload)
    
        assert response.status_code == 200
>       session = client_with_config.app.state.session_manager.get_session("default")

tests\unit\chat_completions_tests\test_project_dir_commands.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.datastructures.State object at 0x00000150FB76D5D0>
key = 'session_manager'

    def __getattr__(self, key: Any) -> Any:
        try:
            return self._state[key]
        except KeyError:
            message = "'{}' object has no attribute '{}'"
>           raise AttributeError(message.format(self.__class__.__name__, key))
E           AttributeError: 'State' object has no attribute 'session_manager'

.venv\lib\site-packages\starlette\datastructures.py:689: AttributeError
----------------------------- Captured log setup ------------------------------
ERROR    src.core.config.app_config:app_config.py:407 Error loading configuration file: Expecting value: line 1 column 1 (char 0)
INFO     root:_base.py:223 2025-08-17T00:00:38.209443Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:38.209443Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:38.217944Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:38.231943Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:354 2025-08-17T00:00:38.233958Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: help
INFO     src.core.services.command_service:command_service.py:78 Registered command: backend
INFO     src.core.services.command_service:command_service.py:78 Registered command: model
INFO     src.core.services.command_service:command_service.py:78 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:78 Registered command: project
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:78 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:78 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:78 Registered command: set
INFO     src.core.services.command_service:command_service.py:78 Registered command: unset
INFO     src.core.services.command_service:command_service.py:78 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:78 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:78 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:78 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:78 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:269 Registered 26 command handlers with registry
------------------------------ Captured log call ------------------------------
INFO     src.core.services.command_service:command_service.py:183 Executing command: set with session: af0d0ff8-157b-496a-a9a4-6b334eebe174
---------------------------- Captured log teardown ----------------------------
INFO     src.core.app.application_factory:application_factory.py:368 2025-08-17T00:00:38.268944Z [info     ] Shutting down application      [src.core.app.application_factory]
___________________ test_set_project_dir_command_valid[dir] ___________________

self = <starlette.datastructures.State object at 0x00000150FB2C8FD0>
key = 'session_manager'

    def __getattr__(self, key: Any) -> Any:
        try:
>           return self._state[key]
E           KeyError: 'session_manager'

.venv\lib\site-packages\starlette\datastructures.py:686: KeyError

During handling of the above exception, another exception occurred:

client_with_config = <starlette.testclient.TestClient object at 0x00000150FACF6590>
tmp_path = WindowsPath('C:/Users/Mateusz/AppData/Local/Temp/pytest-of-Mateusz/pytest-2396/test_set_project_dir_command_v1')
alias = 'dir'

    @pytest.mark.parametrize("alias", ["project-dir", "dir", "project-directory"])
    def test_set_project_dir_command_valid(client_with_config: TestClient, tmp_path, alias):
        with patch.object(
            client_with_config.app.state.openrouter_backend,
            "chat_completions",
            new_callable=AsyncMock,
        ) as mock_method:
            mock_method.return_value = {"choices": [{"message": {"content": "ok"}}]}
            valid_dir = tmp_path.resolve()
            payload = {
                "model": "some-model",
                "messages": [{"role": "user", "content": f'!/set({alias}="{valid_dir}")'}],
            }
            response = client_with_config.post("/v1/chat/completions", json=payload)
    
        assert response.status_code == 200
>       session = client_with_config.app.state.session_manager.get_session("default")

tests\unit\chat_completions_tests\test_project_dir_commands.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.datastructures.State object at 0x00000150FB2C8FD0>
key = 'session_manager'

    def __getattr__(self, key: Any) -> Any:
        try:
            return self._state[key]
        except KeyError:
            message = "'{}' object has no attribute '{}'"
>           raise AttributeError(message.format(self.__class__.__name__, key))
E           AttributeError: 'State' object has no attribute 'session_manager'

.venv\lib\site-packages\starlette\datastructures.py:689: AttributeError
----------------------------- Captured log setup ------------------------------
ERROR    src.core.config.app_config:app_config.py:407 Error loading configuration file: Expecting value: line 1 column 1 (char 0)
INFO     root:_base.py:223 2025-08-17T00:00:38.272945Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:38.272945Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:38.281444Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:38.294943Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:354 2025-08-17T00:00:38.296945Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: help
INFO     src.core.services.command_service:command_service.py:78 Registered command: backend
INFO     src.core.services.command_service:command_service.py:78 Registered command: model
INFO     src.core.services.command_service:command_service.py:78 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:78 Registered command: project
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:78 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:78 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:78 Registered command: set
INFO     src.core.services.command_service:command_service.py:78 Registered command: unset
INFO     src.core.services.command_service:command_service.py:78 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:78 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:78 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:78 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:78 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:269 Registered 26 command handlers with registry
------------------------------ Captured log call ------------------------------
INFO     src.core.services.command_service:command_service.py:183 Executing command: set with session: 874f7e7a-89c1-4997-9a55-9e277e29ece1
---------------------------- Captured log teardown ----------------------------
INFO     src.core.app.application_factory:application_factory.py:368 2025-08-17T00:00:38.332944Z [info     ] Shutting down application      [src.core.app.application_factory]
____________ test_set_project_dir_command_valid[project-directory] ____________

self = <starlette.datastructures.State object at 0x00000150FB599C90>
key = 'session_manager'

    def __getattr__(self, key: Any) -> Any:
        try:
>           return self._state[key]
E           KeyError: 'session_manager'

.venv\lib\site-packages\starlette\datastructures.py:686: KeyError

During handling of the above exception, another exception occurred:

client_with_config = <starlette.testclient.TestClient object at 0x00000150FB59B820>
tmp_path = WindowsPath('C:/Users/Mateusz/AppData/Local/Temp/pytest-of-Mateusz/pytest-2396/test_set_project_dir_command_v2')
alias = 'project-directory'

    @pytest.mark.parametrize("alias", ["project-dir", "dir", "project-directory"])
    def test_set_project_dir_command_valid(client_with_config: TestClient, tmp_path, alias):
        with patch.object(
            client_with_config.app.state.openrouter_backend,
            "chat_completions",
            new_callable=AsyncMock,
        ) as mock_method:
            mock_method.return_value = {"choices": [{"message": {"content": "ok"}}]}
            valid_dir = tmp_path.resolve()
            payload = {
                "model": "some-model",
                "messages": [{"role": "user", "content": f'!/set({alias}="{valid_dir}")'}],
            }
            response = client_with_config.post("/v1/chat/completions", json=payload)
    
        assert response.status_code == 200
>       session = client_with_config.app.state.session_manager.get_session("default")

tests\unit\chat_completions_tests\test_project_dir_commands.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.datastructures.State object at 0x00000150FB599C90>
key = 'session_manager'

    def __getattr__(self, key: Any) -> Any:
        try:
            return self._state[key]
        except KeyError:
            message = "'{}' object has no attribute '{}'"
>           raise AttributeError(message.format(self.__class__.__name__, key))
E           AttributeError: 'State' object has no attribute 'session_manager'

.venv\lib\site-packages\starlette\datastructures.py:689: AttributeError
----------------------------- Captured log setup ------------------------------
ERROR    src.core.config.app_config:app_config.py:407 Error loading configuration file: Expecting value: line 1 column 1 (char 0)
INFO     root:_base.py:223 2025-08-17T00:00:38.336943Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:38.336943Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:38.345944Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:38.360443Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:354 2025-08-17T00:00:38.361943Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: help
INFO     src.core.services.command_service:command_service.py:78 Registered command: backend
INFO     src.core.services.command_service:command_service.py:78 Registered command: model
INFO     src.core.services.command_service:command_service.py:78 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:78 Registered command: project
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:78 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:78 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:78 Registered command: set
INFO     src.core.services.command_service:command_service.py:78 Registered command: unset
INFO     src.core.services.command_service:command_service.py:78 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:78 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:78 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:78 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:78 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:269 Registered 26 command handlers with registry
------------------------------ Captured log call ------------------------------
INFO     src.core.services.command_service:command_service.py:183 Executing command: set with session: a0ea85da-be60-4b3a-a0d9-e7439267057c
---------------------------- Captured log teardown ----------------------------
INFO     src.core.app.application_factory:application_factory.py:368 2025-08-17T00:00:38.397956Z [info     ] Shutting down application      [src.core.app.application_factory]
_____ TestTemperatureProxyState.test_proxy_state_set_temperature_invalid ______

self = <tests.unit.chat_completions_tests.test_temperature_commands.TestTemperatureProxyState object at 0x00000150F7285FC0>

    def test_proxy_state_set_temperature_invalid(self):
        """Test setting invalid temperature values in SessionState."""
        session = Session(session_id="test_session")
        current_session_state = session.state
    
        # Test negative value
>       with pytest.raises(ValueError, match="Temperature must be between 0.0 and 2.0"):
E       Failed: DID NOT RAISE <class 'ValueError'>

tests\unit\chat_completions_tests\test_temperature_commands.py:535: Failed
_____ TestBackendServiceCompletions.test_call_completion_streaming_error ______

self = <tests.unit.core.test_backend_service_enhanced.TestBackendServiceCompletions object at 0x00000150F72D3340>
service = <tests.unit.core.test_backend_service_enhanced.ConcreteBackendService object at 0x00000150FB88B160>
chat_request = ChatRequest(model='model1', messages=[ChatMessage(role='user', content='Hello', name=None, tool_calls=None, tool_call_...: <BackendType.OPENAI: 'openai'>}, reasoning_effort=None, reasoning=None, thinking_budget=None, generation_config=None)

    @pytest.mark.asyncio
    async def test_call_completion_streaming_error(self, service, chat_request):
        """Test error handling in streaming completion."""
    
        # Arrange
        class MockErrorStreamingResponse:
            """Mock streaming response that raises an error."""
    
            def __aiter__(self):
                return self._stream()
    
            async def _stream(self):
                yield b'data: {"id":"chunk1"}\n\n'
                raise ValueError("Streaming error")
    
        mock_response = MockErrorStreamingResponse()
    
        mock_backend = MockBackend(None)
        mock_backend.chat_completions_mock.return_value = mock_response
    
        with patch.object(service, "_get_or_create_backend", return_value=mock_backend):
            # Act
            stream = await service.call_completion(chat_request, stream=True)
    
            # Collect chunks from the stream
            result_chunks = []
>           async for chunk in stream:

tests\unit\core\test_backend_service_enhanced.py:321: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.unit.core.test_backend_service_enhanced.TestBackendServiceCompletions.test_call_completion_streaming_error.<locals>.MockErrorStreamingResponse object at 0x00000150F72655D0>

    async def _stream(self):
        yield b'data: {"id":"chunk1"}\n\n'
>       raise ValueError("Streaming error")
E       ValueError: Streaming error

tests\unit\core\test_backend_service_enhanced.py:308: ValueError
_____ TestBackendServiceCompletions.test_call_completion_invalid_response _____

self = <tests.unit.core.test_backend_service_enhanced.TestBackendServiceCompletions object at 0x00000150F71C3EB0>
service = <tests.unit.core.test_backend_service_enhanced.ConcreteBackendService object at 0x00000150FB57B610>
chat_request = ChatRequest(model='model1', messages=[ChatMessage(role='user', content='Hello', name=None, tool_calls=None, tool_call_...: <BackendType.OPENAI: 'openai'>}, reasoning_effort=None, reasoning=None, thinking_budget=None, generation_config=None)

    @pytest.mark.asyncio
    async def test_call_completion_invalid_response(self, service, chat_request):
        """Test error handling for invalid response format."""
        # Arrange
        mock_backend = MockBackend(None)
        # Return invalid response format (not a tuple)
        mock_backend.chat_completions_mock.return_value = "invalid-response"
    
        with patch.object(service, "_get_or_create_backend", return_value=mock_backend):
            # Act & Assert
>           with pytest.raises(BackendError) as exc_info:
E           Failed: DID NOT RAISE <class 'src.core.common.exceptions.BackendError'>

tests\unit\core\test_backend_service_enhanced.py:387: Failed
_ TestBackendServiceCompletions.test_call_completion_invalid_streaming_response _

self = <tests.unit.core.test_backend_service_enhanced.TestBackendServiceCompletions object at 0x00000150F72D3520>
service = <tests.unit.core.test_backend_service_enhanced.ConcreteBackendService object at 0x00000150F8AEB250>
chat_request = ChatRequest(model='model1', messages=[ChatMessage(role='user', content='Hello', name=None, tool_calls=None, tool_call_...: <BackendType.OPENAI: 'openai'>}, reasoning_effort=None, reasoning=None, thinking_budget=None, generation_config=None)

    @pytest.mark.asyncio
    async def test_call_completion_invalid_streaming_response(
        self, service, chat_request
    ):
        """Test error handling for invalid streaming response format."""
        # Arrange
        mock_backend = MockBackend(None)
        # Return invalid response format (not a StreamingResponse)
        mock_backend.chat_completions_mock.return_value = "invalid-streaming-response"
    
        with patch.object(service, "_get_or_create_backend", return_value=mock_backend):
            # Act & Assert
>           with pytest.raises(BackendError) as exc_info:
E           Failed: DID NOT RAISE <class 'src.core.common.exceptions.BackendError'>

tests\unit\core\test_backend_service_enhanced.py:405: Failed
______________________ test_app_config_to_legacy_config _______________________

    def test_app_config_to_legacy_config():
        """Test conversion to legacy config format."""
        # Arrange
        config = AppConfig(
            host="localhost",
            port=9000,
            backends=AppConfig.BackendSettings(
                default_backend="openai",
>               openai=AppConfig.BackendConfig(
                    api_key="test_key",
                    api_url="https://api.example.com",
                    timeout=30,
                    extra={"foo": "bar"},
                ),
            ),
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for BackendConfig
E       api_key
E         Input should be a valid list [type=list_type, input_value='test_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

tests\unit\core\test_config.py:49: ValidationError
_____________________ test_app_config_from_legacy_config ______________________

    def test_app_config_from_legacy_config():
        """Test creation from legacy config format."""
        # Arrange
        legacy_config = {
            "host": "localhost",
            "port": 9000,
            "default_backend": "openai",
            "openai_api_key": "test_key",
            "openai_api_url": "https://api.example.com",
            "openai_timeout": 30,
            "openai_foo": "bar",
        }
    
        # Act
>       config = AppConfig.from_legacy_config(legacy_config)

tests\unit\core\test_config.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'src.core.config.app_config.AppConfig'>
legacy_config = {'default_backend': 'openai', 'host': 'localhost', 'openai_api_key': 'test_key', 'openai_api_url': 'https://api.example.com', ...}

    @classmethod
    def from_legacy_config(cls, legacy_config: dict[str, Any]) -> AppConfig:
        """Create AppConfig from the legacy configuration format.
    
        Args:
            legacy_config: Dictionary in the legacy configuration format
    
        Returns:
            AppConfig instance
        """
        # Create basic config structure
        config = {
            "host": legacy_config.get("host", "0.0.0.0"),
            "port": legacy_config.get("port", 8000),
            "proxy_timeout": legacy_config.get("proxy_timeout", 120),
            "command_prefix": legacy_config.get("command_prefix", "!/"),
            "auth": {
                "disable_auth": legacy_config.get("disable_auth", False),
                "api_keys": legacy_config.get("api_keys", []),
                "auth_token": legacy_config.get("auth_token"),
            },
            "session": {
                "cleanup_enabled": legacy_config.get("session_cleanup_enabled", True),
                "cleanup_interval": legacy_config.get("session_cleanup_interval", 3600),
                "max_age": legacy_config.get("session_max_age", 86400),
                "default_interactive_mode": legacy_config.get(
                    "default_interactive_mode", True
                ),
                "force_set_project": legacy_config.get("force_set_project", False),
            },
            "logging": {
                "level": legacy_config.get("log_level", "INFO"),
                "request_logging": legacy_config.get("request_logging", False),
                "response_logging": legacy_config.get("response_logging", False),
                "log_file": legacy_config.get("log_file"),
            },
            "model_defaults": legacy_config.get("model_defaults", {}),
            "failover_routes": legacy_config.get("failover_routes", {}),
            "backends": {
                "default_backend": legacy_config.get("default_backend", "openai"),
            },
        }
    
        # Extract backend configurations
        backends = ["openai", "openrouter", "anthropic", "gemini", "qwen_oauth", "zai"]
        for backend in backends:
            backend_config = {}
    
            # Extract common backend settings
            api_key = legacy_config.get(f"{backend}_api_key", "")
            if api_key:
                backend_config["api_key"] = api_key
    
            api_url = legacy_config.get(f"{backend}_api_url")
            if api_url:
                backend_config["api_url"] = api_url
    
            timeout = legacy_config.get(f"{backend}_timeout")
            if timeout:
                backend_config["timeout"] = timeout
    
            # Extract extra backend settings
            extra = {}
            for key, value in legacy_config.items():
                if key.startswith(f"{backend}_") and key not in [
                    f"{backend}_api_key",
                    f"{backend}_api_url",
                    f"{backend}_timeout",
                ]:
                    # Extract the part after {backend}_
                    extra_key = key[len(f"{backend}_") :]
                    extra[extra_key] = value
    
            if extra:
                backend_config["extra"] = extra
    
            if backend_config:
                config["backends"][backend] = backend_config
    
>       return cls(**config)
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for AppConfig
E       backends.openai.api_key
E         Input should be a valid list [type=list_type, input_value='test_key', input_type=str]
E           For further information visit https://errors.pydantic.dev/2.11/v/list_type

src\core\config\app_config.py:279: ValidationError
__________________________ test_app_config_from_env ___________________________

mock_env_vars = {'APP_HOST': 'localhost', 'APP_PORT': '9000', 'DISABLE_AUTH': 'true', 'OPENAI_API_KEY': 'test_openai_key', ...}

    def test_app_config_from_env(mock_env_vars: dict[str, str]):
        """Test creation from environment variables."""
        # Arrange & Act
        config = AppConfig.from_env()
    
        # Assert
        assert config.host == mock_env_vars["APP_HOST"]
        assert config.port == int(mock_env_vars["APP_PORT"])
>       assert config.backends.openai.api_key == mock_env_vars["OPENAI_API_KEY"]
E       AssertionError: assert ['test_openai_key'] == 'test_openai_key'
E        +  where ['test_openai_key'] = BackendConfig(api_key=['test_openai_key'], api_url=None, models=[], timeout=120, extra={}).api_key
E        +    where BackendConfig(api_key=['test_openai_key'], api_url=None, models=[], timeout=120, extra={}) = BackendSettings(default_backend='openai', openai=BackendConfig(api_key=['test_openai_key'], api_url=None, models=[], timeout=120, extra={}), openrouter=BackendConfig(api_key=['test_openrouter_key'], api_url=None, models=[], timeout=120, extra={}), anthropic=BackendConfig(api_key=[], api_url=None, models=[], timeout=120, extra={}), gemini=BackendConfig(api_key=['AIzaSyAcpFWc7ijSBO4xM48MKRVyV6LNCFnihUM', 'AIzaSyA2LTvTF9H-CuQtAGVLnolFhCc79sAUzm0', 'AIzaSyA2LTvTF9H-CuQtAGVLnolFhCc79sAUzm0', 'AIzaSyBO5QAP9t4ilOrnzJFLMrsO67WeYuUNS5A', 'AIzaSyAwEGYb4X8mEreuWy9UViZjkYkZJljhBbE'], api_url=None, models=[], timeout=120, extra={}), qwen_oauth=BackendConfig(api_key=[], api_url=None, models=[], timeout=120, extra={}), zai=BackendConfig(api_key=[], api_url=None, models=[], timeout=120, extra={})).openai
E        +      where BackendSettings(default_backend='openai', openai=BackendConfig(api_key=['test_openai_key'], api_url=None, models=[], timeout=120, extra={}), openrouter=BackendConfig(api_key=['test_openrouter_key'], api_url=None, models=[], timeout=120, extra={}), anthropic=BackendConfig(api_key=[], api_url=None, models=[], timeout=120, extra={}), gemini=BackendConfig(api_key=['AIzaSyAcpFWc7ijSBO4xM48MKRVyV6LNCFnihUM', 'AIzaSyA2LTvTF9H-CuQtAGVLnolFhCc79sAUzm0', 'AIzaSyA2LTvTF9H-CuQtAGVLnolFhCc79sAUzm0', 'AIzaSyBO5QAP9t4ilOrnzJFLMrsO67WeYuUNS5A', 'AIzaSyAwEGYb4X8mEreuWy9UViZjkYkZJljhBbE'], api_url=None, models=[], timeout=120, extra={}), qwen_oauth=BackendConfig(api_key=[], api_url=None, models=[], timeout=120, extra={}), zai=BackendConfig(api_key=[], api_url=None, models=[], timeout=120, extra={})) = AppConfig(host='localhost', port=9000, proxy_timeout=120, command_prefix='!/', default_rate_limit=60, default_rate_window=60, backends=BackendSettings(default_backend='openai', openai=BackendConfig(api_key=['test_openai_key'], api_url=None, models=[], timeout=120, extra={}), openrouter=BackendConfig(api_key=['test_openrouter_key'], api_url=None, models=[], timeout=120, extra={}), anthropic=BackendConfig(api_key=[], api_url=None, models=[], timeout=120, extra={}), gemini=BackendConfig(api_key=['AIzaSyAcpFWc7ijSBO4xM48MKRVyV6LNCFnihUM', 'AIzaSyA2LTvTF9H-CuQtAGVLnolFhCc79sAUzm0', 'AIzaSyA2LTvTF9H-CuQtAGVLnolFhCc79sAUzm0', 'AIzaSyBO5QAP9t4ilOrnzJFLMrsO67WeYuUNS5A', 'AIzaSyAwEGYb4X8mEreuWy9UViZjkYkZJljhBbE'], api_url=None, models=[], timeout=120, extra={}), qwen_oauth=BackendConfig(api_key=[], api_url=None, models=[], timeout=120, extra={}), zai=BackendConfig(api_key=[], api_url=None, models=[], timeout=120, extra={})), model_defaults={}, failover_routes={}, auth=AuthConfig(disable_auth=True, api_keys=[], auth_token=None, redact_api_keys_in_prompts=True), session=SessionConfig(cleanup_enabled=True, cleanup_interval=3600, max_age=86400, default_interactive_mode=True, force_set_project=False, disable_interactive_commands=False), logging=LoggingConfig(level=<LogLevel.INFO: 'INFO'>, request_logging=False, response_logging=False, log_file=None)).backends

tests\unit\core\test_config.py:105: AssertionError
______________________________ test_load_config _______________________________

temp_config_path = WindowsPath('C:/Users/Mateusz/AppData/Local/Temp/pytest-of-Mateusz/pytest-2396/test_load_config0/config.yaml')

    def test_load_config(temp_config_path: Path):
        """Test the load_config function."""
        # Arrange & Act
        config = load_config(temp_config_path)
    
        # Assert
        assert isinstance(config, AppConfig)
>       assert config.host == "localhost"
E       AssertionError: assert '0.0.0.0' == 'localhost'
E         
E         - localhost
E         + 0.0.0.0

tests\unit\core\test_config.py:128: AssertionError
------------------------------ Captured log call ------------------------------
ERROR    src.core.config.app_config:app_config.py:407 Error loading configuration file: 1 validation error for AppConfig
backends.openai.api_key
  Input should be a valid list [type=list_type, input_value='test_key', input_type=str]
    For further information visit https://errors.pydantic.dev/2.11/v/list_type
_________________________ test_set_openai_url_command _________________________

client = <starlette.testclient.TestClient object at 0x00000150FB49EF20>

    def test_set_openai_url_command(client):
        """Test that the !set(openai_url=...) command works correctly."""
        # Set the OpenAI URL
        response = client.post(
            "/v1/chat/completions",
            json={
                "model": "gpt-3.5-turbo",
                "messages": [
                    {
                        "role": "user",
                        "content": "!/set(openai_url=https://custom-api.example.com/v1)",
                    }
                ],
            },
        )
    
>       assert response.status_code == 200
E       assert 401 == 200
E        +  where 401 = <Response [401 Unauthorized]>.status_code

tests\unit\openai_connector_tests\test_integration.py:84: AssertionError
----------------------------- Captured log setup ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:39.605546Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:39.605546Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:39.613547Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:39.627052Z [info     ] Application built successfully [src.core.app.application_factory]
------------------------------ Captured log call ------------------------------
WARNING  src.core.security.middleware:middleware.py:61 Invalid or missing API key for POST /v1/chat/completions from client testclient
_______________________ test_initialize_with_custom_url _______________________

mock_client = <AsyncMock spec='AsyncClient' id='1447322997744'>

    async def test_initialize_with_custom_url(mock_client):
        """Test that initialize uses a custom URL when provided."""
        # Setup
        connector = OpenAIConnector(client=mock_client)
        custom_url = "https://custom-api.example.com/v1"
    
        mock_response = MagicMock()
        mock_response.json.return_value = {
            "data": [
                {"id": "gpt-3.5-turbo"},
                {"id": "gpt-4"},
            ]
        }
        mock_client.get.return_value = mock_response
    
        # Execute
        await connector.initialize(api_key="test-api-key", api_base_url=custom_url)
    
        # Verify
>       mock_client.get.assert_called_once()

tests\unit\openai_connector_tests\test_url_override.py:255: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <AsyncMock name='mock.get' id='1447318409120'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'get' to have been called once. Called 0 times.

C:\Program Files\Python310\lib\unittest\mock.py:908: AssertionError
___________________ test_set_command_openai_url_integration ___________________

    async def test_set_command_openai_url_integration():
        """Test that the set command properly sets the OpenAI URL in the proxy state."""
        from src.commands.set_cmd import SetCommand
    
        # Removed legacy import
    
        # Setup
        state = SessionStateAdapter(SessionState())
        set_cmd = SetCommand()
    
        # Execute with valid URL
        result = set_cmd.execute({"openai_url": "https://custom-api.example.com/v1"}, state)
    
        # Verify
>       assert result.success is True
E       assert False is True
E        +  where False = CommandResult(name='set', success=False, message="Failed to set OpenAI URL: 'SessionStateAdapter' object has no attribute 'set_openai_url'").success

tests\unit\openai_connector_tests\test_url_override.py:277: AssertionError
_________________ test_chat_completions_http_error_streaming __________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x00000150FB457BE0>
sample_chat_request_data = ChatCompletionRequest(model='test-model', messages=[ChatMessage(role='user', content='Hello', name=None, tool_calls=No...asoning_effort=None, reasoning=None, thinking_budget=None, generation_config=None, temperature=None, extra_params=None)
sample_processed_messages = [ChatMessage(role='user', content='Hello', name=None, tool_calls=None, tool_call_id=None)]

    @pytest.mark.asyncio
    async def test_chat_completions_http_error_streaming(
        monkeypatch: pytest.MonkeyPatch,  # Add monkeypatch fixture
        sample_chat_request_data: models.ChatCompletionRequest,
        sample_processed_messages: list[models.ChatMessage],
    ):
        sample_chat_request_data.stream = True
        error_text_response = "OpenRouter internal server error"
    
        async def mock_send_method(self, request, **kwargs):
            mock_response = httpx.Response(
                status_code=500,
                request=request,
                stream=httpx.ByteStream(error_text_response.encode("utf-8")),
                headers={"Content-Type": "text/plain"},
            )
    
            # Properly mock async methods using setattr
            async def mock_aclose():
                pass
    
            mock_response.aclose = mock_aclose
    
            # Mock the aread method to be async
            async def mock_aread():
                return error_text_response.encode("utf-8")
    
            mock_response.aread = mock_aread
    
            return mock_response
    
        monkeypatch.setattr(httpx.AsyncClient, "send", mock_send_method)
    
        async with httpx.AsyncClient() as client:
            openrouter_backend = OpenRouterBackend(client=client)
    
            with pytest.raises(Exception) as exc_info:
                await openrouter_backend.chat_completions(
                    request_data=sample_chat_request_data,
                    processed_messages=sample_processed_messages,
                    effective_model="test-model",
                    openrouter_api_base_url=TEST_OPENROUTER_API_BASE_URL,
                    openrouter_headers_provider=mock_get_openrouter_headers,
                    key_name="test_key",
                    api_key="FAKE_KEY",
                )
    
        assert exc_info.value.status_code == 500
        detail = exc_info.value.detail
        assert isinstance(detail, dict)
>       assert (
            detail.get("message")
            == "OpenRouter stream error: 500 - OpenRouter internal server error"
        )
E       AssertionError: assert 'OpenRouter i... server error' == 'OpenRouter s... server error'
E         
E         - OpenRouter stream error: 500 - OpenRouter internal server error
E         + OpenRouter internal server error

tests\unit\openrouter_connector_tests\test_http_error_streaming.py:101: AssertionError
_____________________ test_chat_completions_request_error _____________________

openrouter_backend = <src.connectors.openrouter.OpenRouterBackend object at 0x00000150FAE7AC20>
httpx_mock = <pytest_httpx._httpx_mock.HTTPXMock object at 0x00000150F9C9B3A0>
sample_chat_request_data = ChatCompletionRequest(model='test-model', messages=[ChatMessage(role='user', content='Hello', name=None, tool_calls=No...asoning_effort=None, reasoning=None, thinking_budget=None, generation_config=None, temperature=None, extra_params=None)
sample_processed_messages = [ChatMessage(role='user', content='Hello', name=None, tool_calls=None, tool_call_id=None)]

    @pytest.mark.asyncio
    @pytest.mark.httpx_mock()
    async def test_chat_completions_request_error(
        openrouter_backend: OpenRouterBackend,
        httpx_mock: HTTPXMock,
        sample_chat_request_data: models.ChatCompletionRequest,
        sample_processed_messages: list[models.ChatMessage],
    ):
        httpx_mock.add_exception(httpx.ConnectError("Connection failed"))
    
        with pytest.raises(HTTPException) as exc_info:
>           await openrouter_backend.chat_completions(
                request_data=sample_chat_request_data,
                processed_messages=sample_processed_messages,
                effective_model="test-model",
                openrouter_api_base_url=TEST_OPENROUTER_API_BASE_URL,
                openrouter_headers_provider=mock_get_openrouter_headers,
                key_name="test_key",
                api_key="FAKE_KEY",
            )

tests\unit\openrouter_connector_tests\test_request_error.py:64: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.connectors.openrouter.OpenRouterBackend object at 0x00000150FAE7AC20>
request_data = ChatCompletionRequest(model='test-model', messages=[ChatMessage(role='user', content='Hello', name=None, tool_calls=No...asoning_effort=None, reasoning=None, thinking_budget=None, generation_config=None, temperature=None, extra_params=None)
processed_messages = [ChatMessage(role='user', content='Hello', name=None, tool_calls=None, tool_call_id=None)]
effective_model = 'test-model', project = None, kwargs = {}
headers_provider = <function mock_get_openrouter_headers at 0x00000150F73C76D0>
key_name = 'test_key', api_key = 'FAKE_KEY'
api_base_url = 'https://openrouter.ai/api/v1', original_headers_provider = None
original_key_name = None, original_api_key = None

    async def chat_completions(  # type: ignore[override]
        self,
        request_data: ChatCompletionRequest,
        processed_messages: list[Any],
        effective_model: str,
        project: str | None = None,
        **kwargs: Any,
    ) -> StreamingResponse | tuple[dict[str, Any], dict[str, str]]:
        # Allow tests and callers to provide per-call OpenRouter settings via kwargs
        headers_provider = kwargs.pop("openrouter_headers_provider", None)
        key_name = kwargs.pop("key_name", None)
        api_key = kwargs.pop("api_key", None)
        api_base_url = kwargs.pop("openrouter_api_base_url", None)
    
        original_headers_provider = self.headers_provider
        original_key_name = self.key_name
        original_api_key = self.api_key
        original_api_base_url = self.api_base_url
    
        try:
            if headers_provider is not None:
                self.headers_provider = cast(
                    Callable[[str, str], dict[str, str]], headers_provider
                )
            if key_name is not None:
                self.key_name = cast(str, key_name)
            if api_key is not None:
                self.api_key = cast(str, api_key)
            if api_base_url:
                self.api_base_url = cast(str, api_base_url)
    
            # Compute explicit headers for this call if possible and pass
            # them to the parent as a headers_override so the streaming
            # implementation will see the Authorization header.
            headers_override = None
            try:
                if self.key_name and self.api_key and self.headers_provider:
                    headers_override = self.headers_provider(
                        self.key_name, self.api_key
                    )
            except Exception:
                headers_override = None
    
            # No-op: computed headers_override is passed to parent; keep
            # quiet in normal runs (debug-level logs already available).
            # Defensive: ensure Authorization header present when we have an api_key
            try:
                if (
                    headers_override is not None
                    and "Authorization" not in headers_override
                    and self.api_key
                ):
                    headers_override["Authorization"] = f"Bearer {self.api_key}"
            except Exception:
                pass
            call_kwargs = dict(kwargs)
            if headers_override is not None:
                call_kwargs["headers_override"] = headers_override
    
>           return await super().chat_completions(
                request_data=request_data,
                processed_messages=processed_messages,
                effective_model=effective_model,
                **call_kwargs,
            )

src\connectors\openrouter.py:146: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.connectors.openrouter.OpenRouterBackend object at 0x00000150FAE7AC20>
request_data = ChatCompletionRequest(model='test-model', messages=[ChatMessage(role='user', content='Hello', name=None, tool_calls=No...asoning_effort=None, reasoning=None, thinking_budget=None, generation_config=None, temperature=None, extra_params=None)
processed_messages = [ChatMessage(role='user', content='Hello', name=None, tool_calls=None, tool_call_id=None)]
effective_model = 'test-model', kwargs = {}
payload = {'messages': [{'content': 'Hello', 'name': None, 'role': 'user', 'tool_call_id': None, ...}], 'model': 'openrouter/test-model', 'usage': {'include': True}}
headers = {'Authorization': 'Bearer FAKE_KEY', 'Content-Type': 'application/json', 'HTTP-Referer': 'http://localhost:test', 'X-Title': 'TestProxy'}
api_base = 'https://openrouter.ai/api/v1'
url = 'https://openrouter.ai/api/v1/chat/completions'

    async def chat_completions(
        self,
        request_data: ChatCompletionRequest,
        processed_messages: list[Any],
        effective_model: str,
        **kwargs: Any,
    ) -> StreamingResponse | tuple[dict[str, Any], dict[str, str]]:
        payload = self._prepare_payload(
            request_data, processed_messages, effective_model
        )
        headers = kwargs.pop("headers_override", None)
        if headers is None:
            try:
                headers = self.get_headers()
            except Exception:
                headers = None
    
        api_base = kwargs.get("openai_url") or self.api_base_url
        url = f"{api_base}/chat/completions"
    
        if request_data.stream:
            return await self._handle_streaming_response(url, payload, headers)
        else:
>           return await self._handle_non_streaming_response(url, payload, headers)

src\connectors\openai.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.connectors.openrouter.OpenRouterBackend object at 0x00000150FAE7AC20>
url = 'https://openrouter.ai/api/v1/chat/completions'
payload = {'messages': [{'content': 'Hello', 'name': None, 'role': 'user', 'tool_call_id': None, ...}], 'model': 'openrouter/test-model', 'usage': {'include': True}}
headers = {'Authorization': 'Bearer FAKE_KEY', 'Content-Type': 'application/json', 'HTTP-Referer': 'http://localhost:test', 'X-Title': 'TestProxy'}

    async def _handle_non_streaming_response(
        self, url: str, payload: dict[str, Any], headers: dict[str, str] | None
    ) -> tuple[dict[str, Any], dict[str, str]]:
        if not headers or not headers.get("Authorization"):
            raise HTTPException(
                status_code=401,
                detail={"error": {"message": "No auth credentials found", "code": 401}},
            )
>       response = await self.client.post(url, json=payload, headers=headers)

src\connectors\openai.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <httpx.AsyncClient object at 0x00000150FAE79420>
url = 'https://openrouter.ai/api/v1/chat/completions'

    async def post(
        self,
        url: URL | str,
        *,
        content: RequestContent | None = None,
        data: RequestData | None = None,
        files: RequestFiles | None = None,
        json: typing.Any | None = None,
        params: QueryParamTypes | None = None,
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> Response:
        """
        Send a `POST` request.
    
        **Parameters**: See `httpx.request`.
        """
>       return await self.request(
            "POST",
            url,
            content=content,
            data=data,
            files=files,
            json=json,
            params=params,
            headers=headers,
            cookies=cookies,
            auth=auth,
            follow_redirects=follow_redirects,
            timeout=timeout,
            extensions=extensions,
        )

.venv\lib\site-packages\httpx\_client.py:1859: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <httpx.AsyncClient object at 0x00000150FAE79420>, method = 'POST'
url = 'https://openrouter.ai/api/v1/chat/completions'

    async def request(
        self,
        method: str,
        url: URL | str,
        *,
        content: RequestContent | None = None,
        data: RequestData | None = None,
        files: RequestFiles | None = None,
        json: typing.Any | None = None,
        params: QueryParamTypes | None = None,
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault | None = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> Response:
        """
        Build and send a request.
    
        Equivalent to:
    
        ```python
        request = client.build_request(...)
        response = await client.send(request, ...)
        ```
    
        See `AsyncClient.build_request()`, `AsyncClient.send()`
        and [Merging of configuration][0] for how the various parameters
        are merged with client-level configuration.
    
        [0]: /advanced/clients/#merging-of-configuration
        """
    
        if cookies is not None:  # pragma: no cover
            message = (
                "Setting per-request cookies=<...> is being deprecated, because "
                "the expected behaviour on cookie persistence is ambiguous. Set "
                "cookies directly on the client instance instead."
            )
            warnings.warn(message, DeprecationWarning, stacklevel=2)
    
        request = self.build_request(
            method=method,
            url=url,
            content=content,
            data=data,
            files=files,
            json=json,
            params=params,
            headers=headers,
            cookies=cookies,
            timeout=timeout,
            extensions=extensions,
        )
>       return await self.send(request, auth=auth, follow_redirects=follow_redirects)

.venv\lib\site-packages\httpx\_client.py:1540: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <httpx.AsyncClient object at 0x00000150FAE79420>
request = <Request('POST', 'https://openrouter.ai/api/v1/chat/completions')>

    async def send(
        self,
        request: Request,
        *,
        stream: bool = False,
        auth: AuthTypes | UseClientDefault | None = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
    ) -> Response:
        """
        Send a request.
    
        The request is sent as-is, unmodified.
    
        Typically you'll want to build one with `AsyncClient.build_request()`
        so that any client-level configuration is merged into the request,
        but passing an explicit `httpx.Request()` is supported as well.
    
        See also: [Request instances][0]
    
        [0]: /advanced/clients/#request-instances
        """
        if self._state == ClientState.CLOSED:
            raise RuntimeError("Cannot send a request, as the client has been closed.")
    
        self._state = ClientState.OPENED
        follow_redirects = (
            self.follow_redirects
            if isinstance(follow_redirects, UseClientDefault)
            else follow_redirects
        )
    
        self._set_timeout(request)
    
        auth = self._build_request_auth(request, auth)
    
>       response = await self._send_handling_auth(
            request,
            auth=auth,
            follow_redirects=follow_redirects,
            history=[],
        )

.venv\lib\site-packages\httpx\_client.py:1629: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <httpx.AsyncClient object at 0x00000150FAE79420>
request = <Request('POST', 'https://openrouter.ai/api/v1/chat/completions')>
auth = <httpx.Auth object at 0x00000150FB4B9570>, follow_redirects = False
history = []

    async def _send_handling_auth(
        self,
        request: Request,
        auth: Auth,
        follow_redirects: bool,
        history: list[Response],
    ) -> Response:
        auth_flow = auth.async_auth_flow(request)
        try:
            request = await auth_flow.__anext__()
    
            while True:
>               response = await self._send_handling_redirects(
                    request,
                    follow_redirects=follow_redirects,
                    history=history,
                )

.venv\lib\site-packages\httpx\_client.py:1657: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <httpx.AsyncClient object at 0x00000150FAE79420>
request = <Request('POST', 'https://openrouter.ai/api/v1/chat/completions')>
follow_redirects = False, history = []

    async def _send_handling_redirects(
        self,
        request: Request,
        follow_redirects: bool,
        history: list[Response],
    ) -> Response:
        while True:
            if len(history) > self.max_redirects:
                raise TooManyRedirects(
                    "Exceeded maximum allowed redirects.", request=request
                )
    
            for hook in self._event_hooks["request"]:
                await hook(request)
    
>           response = await self._send_single_request(request)

.venv\lib\site-packages\httpx\_client.py:1694: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <httpx.AsyncClient object at 0x00000150FAE79420>
request = <Request('POST', 'https://openrouter.ai/api/v1/chat/completions')>

    async def _send_single_request(self, request: Request) -> Response:
        """
        Sends a single request, without handling any redirections.
        """
        transport = self._transport_for_url(request.url)
        start = time.perf_counter()
    
        if not isinstance(request.stream, AsyncByteStream):
            raise RuntimeError(
                "Attempted to send an sync request with an AsyncClient instance."
            )
    
        with request_context(request=request):
>           response = await transport.handle_async_request(request)

.venv\lib\site-packages\httpx\_client.py:1730: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

transport = <httpx.AsyncHTTPTransport object at 0x00000150F9C99540>
request = <Request('POST', 'https://openrouter.ai/api/v1/chat/completions')>

    async def mocked_handle_async_request(
        transport: httpx.AsyncHTTPTransport, request: httpx.Request
    ) -> httpx.Response:
        if options.should_mock(request):
>           return await mock._handle_async_request(transport, request)

.venv\lib\site-packages\pytest_httpx\__init__.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <pytest_httpx._httpx_mock.HTTPXMock object at 0x00000150F9C9B3A0>
real_transport = <httpx.AsyncHTTPTransport object at 0x00000150F9C99540>
request = <Request('POST', 'https://openrouter.ai/api/v1/chat/completions')>

    async def _handle_async_request(
        self,
        real_transport: httpx.AsyncHTTPTransport,
        request: httpx.Request,
    ) -> httpx.Response:
        # Store the content in request for future matching
        await request.aread()
        self._requests.append((real_transport, request))
    
        callback = self._get_callback(real_transport, request)
        if callback:
>           response = callback(request)

.venv\lib\site-packages\pytest_httpx\_httpx_mock.py:176: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

request = <Request('POST', 'https://openrouter.ai/api/v1/chat/completions')>

    def exception_callback(request: httpx.Request) -> None:
        if isinstance(exception, httpx.RequestError):
            exception.request = request
>       raise exception
E       httpx.ConnectError: Connection failed

.venv\lib\site-packages\pytest_httpx\_httpx_mock.py:143: ConnectError
____ TestOpenRouterTemperatureHandling.test_temperature_streaming_request _____

self = <test_temperature_handling.TestOpenRouterTemperatureHandling object at 0x00000150F7417220>
openrouter_backend = <src.connectors.openrouter.OpenRouterBackend object at 0x00000150FB2D0BE0>
sample_request_data = ChatCompletionRequest(model='openrouter:openai/gpt-4', messages=[ChatMessage(role='user', content='Test message', name...easoning_effort=None, reasoning=None, thinking_budget=None, generation_config=None, temperature=0.9, extra_params=None)
sample_processed_messages = [ChatMessage(role='user', content='Test message', name=None, tool_calls=None, tool_call_id=None)]

    @pytest.mark.asyncio
    async def test_temperature_streaming_request(
        self, openrouter_backend, sample_request_data, sample_processed_messages
    ):
        """Test temperature handling in streaming requests."""
        # Set temperature and enable streaming
        sample_request_data.temperature = 0.9
        sample_request_data.stream = True
    
        # Mock streaming response
        mock_response = Mock()
        mock_response.status_code = 200  # This should be an int, not AsyncMock
        mock_response.aiter_bytes.return_value = [
            b'data: { "choices": [ { "delta": { "content": "Streaming" } } ] }\n\n',
            b'data: { "choices": [ { "delta": { "content": " response" } } ] }\n\n',
            b"data: [DONE]\n\n",
        ]
        mock_response.aclose = AsyncMock()
    
        # Mock the client.send method instead of client.stream
        openrouter_backend.client.build_request = Mock()
        openrouter_backend.client.send = AsyncMock(return_value=mock_response)
    
        # Call the method
>       await openrouter_backend.chat_completions(
            request_data=sample_request_data,
            processed_messages=sample_processed_messages,
            effective_model="openai/gpt-4",
            openrouter_api_base_url=TEST_OPENROUTER_API_BASE_URL,
            openrouter_headers_provider=mock_get_openrouter_headers,
            key_name="OPENROUTER_API_KEY_1",
            api_key="test-key",
        )

tests\unit\openrouter_connector_tests\test_temperature_handling.py:406: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.connectors.openrouter.OpenRouterBackend object at 0x00000150FB2D0BE0>
request_data = ChatCompletionRequest(model='openrouter:openai/gpt-4', messages=[ChatMessage(role='user', content='Test message', name...easoning_effort=None, reasoning=None, thinking_budget=None, generation_config=None, temperature=0.9, extra_params=None)
processed_messages = [ChatMessage(role='user', content='Test message', name=None, tool_calls=None, tool_call_id=None)]
effective_model = 'openai/gpt-4', project = None, kwargs = {}
headers_provider = <function mock_get_openrouter_headers at 0x00000150F73C7C70>
key_name = 'OPENROUTER_API_KEY_1', api_key = 'test-key'
api_base_url = 'https://openrouter.ai/api/v1', original_headers_provider = None
original_key_name = None, original_api_key = None

    async def chat_completions(  # type: ignore[override]
        self,
        request_data: ChatCompletionRequest,
        processed_messages: list[Any],
        effective_model: str,
        project: str | None = None,
        **kwargs: Any,
    ) -> StreamingResponse | tuple[dict[str, Any], dict[str, str]]:
        # Allow tests and callers to provide per-call OpenRouter settings via kwargs
        headers_provider = kwargs.pop("openrouter_headers_provider", None)
        key_name = kwargs.pop("key_name", None)
        api_key = kwargs.pop("api_key", None)
        api_base_url = kwargs.pop("openrouter_api_base_url", None)
    
        original_headers_provider = self.headers_provider
        original_key_name = self.key_name
        original_api_key = self.api_key
        original_api_base_url = self.api_base_url
    
        try:
            if headers_provider is not None:
                self.headers_provider = cast(
                    Callable[[str, str], dict[str, str]], headers_provider
                )
            if key_name is not None:
                self.key_name = cast(str, key_name)
            if api_key is not None:
                self.api_key = cast(str, api_key)
            if api_base_url:
                self.api_base_url = cast(str, api_base_url)
    
            # Compute explicit headers for this call if possible and pass
            # them to the parent as a headers_override so the streaming
            # implementation will see the Authorization header.
            headers_override = None
            try:
                if self.key_name and self.api_key and self.headers_provider:
                    headers_override = self.headers_provider(
                        self.key_name, self.api_key
                    )
            except Exception:
                headers_override = None
    
            # No-op: computed headers_override is passed to parent; keep
            # quiet in normal runs (debug-level logs already available).
            # Defensive: ensure Authorization header present when we have an api_key
            try:
                if (
                    headers_override is not None
                    and "Authorization" not in headers_override
                    and self.api_key
                ):
                    headers_override["Authorization"] = f"Bearer {self.api_key}"
            except Exception:
                pass
            call_kwargs = dict(kwargs)
            if headers_override is not None:
                call_kwargs["headers_override"] = headers_override
    
>           return await super().chat_completions(
                request_data=request_data,
                processed_messages=processed_messages,
                effective_model=effective_model,
                **call_kwargs,
            )

src\connectors\openrouter.py:146: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.connectors.openrouter.OpenRouterBackend object at 0x00000150FB2D0BE0>
request_data = ChatCompletionRequest(model='openrouter:openai/gpt-4', messages=[ChatMessage(role='user', content='Test message', name...easoning_effort=None, reasoning=None, thinking_budget=None, generation_config=None, temperature=0.9, extra_params=None)
processed_messages = [ChatMessage(role='user', content='Test message', name=None, tool_calls=None, tool_call_id=None)]
effective_model = 'openai/gpt-4', kwargs = {}
payload = {'messages': [{'content': 'Test message', 'name': None, 'role': 'user', 'tool_call_id': None, ...}], 'model': 'openai/gpt-4', 'stream': True, 'temperature': 0.9, ...}
headers = {'Authorization': 'Bearer test-key', 'Content-Type': 'application/json', 'HTTP-Referer': 'http://localhost:test', 'X-Title': 'TestProxy'}
api_base = 'https://openrouter.ai/api/v1'
url = 'https://openrouter.ai/api/v1/chat/completions'

    async def chat_completions(
        self,
        request_data: ChatCompletionRequest,
        processed_messages: list[Any],
        effective_model: str,
        **kwargs: Any,
    ) -> StreamingResponse | tuple[dict[str, Any], dict[str, str]]:
        payload = self._prepare_payload(
            request_data, processed_messages, effective_model
        )
        headers = kwargs.pop("headers_override", None)
        if headers is None:
            try:
                headers = self.get_headers()
            except Exception:
                headers = None
    
        api_base = kwargs.get("openai_url") or self.api_base_url
        url = f"{api_base}/chat/completions"
    
        if request_data.stream:
>           return await self._handle_streaming_response(url, payload, headers)

src\connectors\openai.py:77: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.connectors.openrouter.OpenRouterBackend object at 0x00000150FB2D0BE0>
url = 'https://openrouter.ai/api/v1/chat/completions'
payload = {'messages': [{'content': 'Test message', 'name': None, 'role': 'user', 'tool_call_id': None, ...}], 'model': 'openai/gpt-4', 'stream': True, 'temperature': 0.9, ...}
headers = {'Authorization': 'Bearer test-key', 'Content-Type': 'application/json', 'HTTP-Referer': 'http://localhost:test', 'X-Title': 'TestProxy'}

    async def _handle_streaming_response(
        self, url: str, payload: dict[str, Any], headers: dict[str, str] | None
    ) -> StreamingResponse:
        if not headers or not headers.get("Authorization"):
            raise HTTPException(
                status_code=401,
                detail={"error": {"message": "No auth credentials found", "code": 401}},
            )
    
        request = self.client.build_request("POST", url, json=payload, headers=headers)
        response = await self.client.send(request, stream=True)
        status_code = (
            int(response.status_code) if hasattr(response, "status_code") else 200
        )
        if status_code >= 400:
            try:
                body = (await response.aread()).decode("utf-8")
            except Exception:
                body = getattr(response, "text", "")
            raise HTTPException(status_code=status_code, detail={"message": body})
    
        async def gen() -> AsyncGenerator[bytes, None]:
            try:
                it = response.aiter_bytes()
                if hasattr(it, "__aiter__"):
                    async for chunk in it:
                        yield chunk
                elif hasattr(it, "__iter__"):
                    for chunk in it:  # type: ignore[misc]
                        yield chunk
                elif asyncio.iscoroutine(it):
                    res = await it
                    if hasattr(res, "__iter__"):
                        for chunk in res:
                            yield chunk
            finally:
                import contextlib
    
                with contextlib.suppress(Exception):
                    await response.aclose()
    
        return StreamingResponse(
>           gen(), media_type="text/event-stream", headers=dict(response.headers)
        )
E       TypeError: Mock.keys() returned a non-iterable (type Mock)

src\connectors\openai.py:140: TypeError
____ TestProcessCommandsInMessages.test_unset_model_and_project_in_message ____

self = <test_process_commands_in_messages.TestProcessCommandsInMessages object at 0x00000150F744B0A0>

    def test_unset_model_and_project_in_message(self):
        session = Session(session_id="test_session")
        current_session_state = session.state
    
        # Set initial model and project
        new_backend_config = current_session_state.backend_config.with_model("foo")
        new_session_state = current_session_state.with_backend_config(
            new_backend_config
        )
        new_session_state = new_session_state.with_project("bar")
        session.state = new_session_state
    
        messages = [models.ChatMessage(role="user", content="!/unset(model, project)")]
>       processed_messages, processed = process_commands_in_messages(
            messages, session.state, app=self.mock_app
        )

tests\unit\proxy_logic_tests\test_process_commands_in_messages.py:351: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

messages = [ChatMessage(role='user', content='!/unset(model, project)', name=None, tool_calls=None, tool_call_id=None)]
current_proxy_state = <src.core.domain.session.SessionStateAdapter object at 0x00000150F9BC9960>
app = <Mock id='1447298906000'>, command_prefix = '!/'

    def process_commands_in_messages(
        messages: list[models.ChatMessage],
        current_proxy_state: SessionStateAdapter,
        app: FastAPI | None = None,
        command_prefix: str = DEFAULT_COMMAND_PREFIX,
    ) -> tuple[list[models.ChatMessage], bool]:
        """
        Processes a list of chat messages to identify and execute embedded commands.
    
        This is the primary public interface for command processing. It initializes
        a CommandParser and uses it to process the messages.
        """
        if not messages:
            logger.debug("process_commands_in_messages received empty messages list.")
            return messages, False
    
        functional_backends: set[str] | None = None
        if app and hasattr(app, "state") and hasattr(app.state, "functional_backends"):
            functional_backends = app.state.functional_backends
        else:
            logger.warning(
                "FastAPI app instance or functional_backends not available in "
                "app.state. CommandParser will be initialized without specific "
                "functional_backends.",
            )
    
        parser_config = CommandParserConfig(
            proxy_state=current_proxy_state,
            app=app,  # type: ignore
            preserve_unknown=True,
            functional_backends=functional_backends,
        )
        parser = CommandParser(parser_config, command_prefix=command_prefix)
    
>       final_messages, commands_processed = parser.process_messages(messages)

src\command_parser.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150F9BCAE60>
messages = [ChatMessage(role='user', content='!/unset(model, project)', name=None, tool_calls=None, tool_call_id=None)]

    def process_messages(
        self, messages: list[models.ChatMessage]
    ) -> tuple[list[models.ChatMessage], bool]:
        self.command_results.clear()
        if not messages:
            logger.debug("process_messages received empty messages list.")
            return messages, False
    
        # Find the index of the last message containing a command to avoid unnecessary processing.
        command_message_idx = -1
        for i in range(len(messages) - 1, -1, -1):
            text_for_check = get_text_for_command_check(messages[i].content)
            if self.command_pattern.search(text_for_check):
                command_message_idx = i
                break
    
        # If no command is found, return the original list reference, avoiding any copies.
        if command_message_idx == -1:
            return messages, False
    
        # A command was found. Create a shallow copy of the list and deep copy only the message
        # that needs to be modified. This is the core performance optimization.
        modified_messages = list(messages)
        msg_to_process = modified_messages[command_message_idx].model_copy(deep=True)
        modified_messages[command_message_idx] = msg_to_process
    
>       overall_commands_processed = self._execute_commands_in_target_message(
            command_message_idx, modified_messages
        )

src\command_parser.py:213: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150F9BCAE60>
target_idx = 0
modified_messages = [ChatMessage(role='user', content='!/unset(model, project)', name=None, tool_calls=None, tool_call_id=None)]

    def _execute_commands_in_target_message(
        self, target_idx: int, modified_messages: list[models.ChatMessage]
    ) -> bool:
        """Processes commands in the specified message and updates it.
        Returns True if a command was found and an attempt to execute it was made.
        """
        msg_to_process = modified_messages[target_idx]
        original_content = msg_to_process.content
>       processed_content, found, modified = self._process_content(msg_to_process)

src\command_parser.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150F9BCAE60>
msg_to_process = ChatMessage(role='user', content='!/unset(model, project)', name=None, tool_calls=None, tool_call_id=None)

    def _process_content(
        self, msg_to_process: models.ChatMessage
    ) -> tuple[str | list[models.MessageContentPart] | None, bool, bool]:
        if isinstance(msg_to_process.content, str):
>           return self.command_processor.handle_string_content(msg_to_process.content)

src\command_parser.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_processor.CommandProcessor object at 0x00000150F9BC8FA0>
msg_content = '!/unset(model, project)'

    def handle_string_content(
        self,
        msg_content: str,
    ) -> tuple[str, bool, bool]:
        original_content = msg_content
>       processed_text, command_found = self.process_text_and_execute_command(
            original_content
        )

src\command_processor.py:131: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_processor.CommandProcessor object at 0x00000150F9BC8FA0>
text_content = '!/unset(model, project)'

    def process_text_and_execute_command(self, text_content: str) -> tuple[str, bool]:
        """Processes text for a single command and executes it."""
        commands_found = False
        modified_text = text_content
    
        match = self.config.command_pattern.search(text_content)
        if match:
            commands_found = True
            command_full = match.group(0)
            command_name = match.group("cmd").lower()
            args_str = match.group("args") or ""
            logger.debug(
                "Regex match: Full='%s', Command='%s', ArgsStr='%s'",
                command_full,
                command_name,
                args_str,
            )
            args = parse_arguments(args_str)
    
            replacement = ""
            command_handler = self.config.handlers.get(command_name)
            if command_handler:
                # Get the result from the command handler
                # This could be either a legacy or new CommandResult
>               execution_result: Any = command_handler.execute(
                    args, self.config.proxy_state
                )

src\command_processor.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.commands.unset_cmd.UnsetCommand object at 0x00000150F9BC8490>
args = {'model': True, 'project': True}
state = <src.core.domain.session.SessionStateAdapter object at 0x00000150F9BC9960>

    def execute(self, args: Mapping[str, Any], state: Any) -> CommandResult:
        keys_to_unset = [k for k, v in args.items() if v is True]
        actions = self._build_unset_actions()
>       messages, persistent_change = self._apply_unset_actions(
            keys_to_unset, state, actions
        )

src\commands\unset_cmd.py:72: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.commands.unset_cmd.UnsetCommand object at 0x00000150F9BC8490>
keys_to_unset = ['model', 'project']
state = <src.core.domain.session.SessionStateAdapter object at 0x00000150F9BC9960>
actions = {'backend': (<function UnsetCommand._create_unset_action.<locals>.action at 0x00000150FAEB5AB0>, False), 'command-pref...8490>>, True), 'dir': (<function UnsetCommand._create_unset_action.<locals>.action at 0x00000150FAEB7880>, False), ...}

    def _apply_unset_actions(
        self,
        keys_to_unset: list[str],
        state: Any,
        actions: dict[str, tuple[Callable[[Any], str], bool]],
    ) -> tuple[list[str], bool]:
        messages: list[str] = []
        persistent_change = False
        for key in keys_to_unset:
            if key not in actions:
                continue
            action_func, is_persistent = actions[key]
>           message = action_func(state)

src\commands\unset_cmd.py:237: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

state = <src.core.domain.session.SessionStateAdapter object at 0x00000150F9BC9960>

    def action(state: Any) -> str:
>       getattr(state, method_name)()
E       AttributeError: 'SessionStateAdapter' object has no attribute 'unset_override_model'. Did you mean: 'set_override_model'?

src\commands\unset_cmd.py:64: AttributeError
_____________ TestProcessTextForCommands.test_unset_model_command _____________

self = <test_process_text_for_commands.TestProcessTextForCommands object at 0x00000150F744BBE0>

    def test_unset_model_command(self):
        session = Session(session_id="test_session")
        current_session_state = session.state
        # First set a model
        new_backend_config = current_session_state.backend_config.with_model("gpt-4")
        session.state = current_session_state.with_backend_config(new_backend_config)
        assert session.state.backend_config.model == "gpt-4"
    
        # Then unset it
        text = "Actually, !/unset(model) nevermind."
>       processed_messages, commands_found = process_commands_in_messages(
            [ChatMessage(role="user", content=text)],
            session.state,
            app=self.mock_app,
            command_prefix="!/",
        )

tests\unit\proxy_logic_tests\test_process_text_for_commands.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

messages = [ChatMessage(role='user', content='Actually, !/unset(model) nevermind.', name=None, tool_calls=None, tool_call_id=None)]
current_proxy_state = <src.core.domain.session.SessionStateAdapter object at 0x00000150FB9A8880>
app = <Mock id='1447330227408'>, command_prefix = '!/'

    def process_commands_in_messages(
        messages: list[models.ChatMessage],
        current_proxy_state: SessionStateAdapter,
        app: FastAPI | None = None,
        command_prefix: str = DEFAULT_COMMAND_PREFIX,
    ) -> tuple[list[models.ChatMessage], bool]:
        """
        Processes a list of chat messages to identify and execute embedded commands.
    
        This is the primary public interface for command processing. It initializes
        a CommandParser and uses it to process the messages.
        """
        if not messages:
            logger.debug("process_commands_in_messages received empty messages list.")
            return messages, False
    
        functional_backends: set[str] | None = None
        if app and hasattr(app, "state") and hasattr(app.state, "functional_backends"):
            functional_backends = app.state.functional_backends
        else:
            logger.warning(
                "FastAPI app instance or functional_backends not available in "
                "app.state. CommandParser will be initialized without specific "
                "functional_backends.",
            )
    
        parser_config = CommandParserConfig(
            proxy_state=current_proxy_state,
            app=app,  # type: ignore
            preserve_unknown=True,
            functional_backends=functional_backends,
        )
        parser = CommandParser(parser_config, command_prefix=command_prefix)
    
>       final_messages, commands_processed = parser.process_messages(messages)

src\command_parser.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150FB9A8700>
messages = [ChatMessage(role='user', content='Actually, !/unset(model) nevermind.', name=None, tool_calls=None, tool_call_id=None)]

    def process_messages(
        self, messages: list[models.ChatMessage]
    ) -> tuple[list[models.ChatMessage], bool]:
        self.command_results.clear()
        if not messages:
            logger.debug("process_messages received empty messages list.")
            return messages, False
    
        # Find the index of the last message containing a command to avoid unnecessary processing.
        command_message_idx = -1
        for i in range(len(messages) - 1, -1, -1):
            text_for_check = get_text_for_command_check(messages[i].content)
            if self.command_pattern.search(text_for_check):
                command_message_idx = i
                break
    
        # If no command is found, return the original list reference, avoiding any copies.
        if command_message_idx == -1:
            return messages, False
    
        # A command was found. Create a shallow copy of the list and deep copy only the message
        # that needs to be modified. This is the core performance optimization.
        modified_messages = list(messages)
        msg_to_process = modified_messages[command_message_idx].model_copy(deep=True)
        modified_messages[command_message_idx] = msg_to_process
    
>       overall_commands_processed = self._execute_commands_in_target_message(
            command_message_idx, modified_messages
        )

src\command_parser.py:213: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150FB9A8700>
target_idx = 0
modified_messages = [ChatMessage(role='user', content='Actually, !/unset(model) nevermind.', name=None, tool_calls=None, tool_call_id=None)]

    def _execute_commands_in_target_message(
        self, target_idx: int, modified_messages: list[models.ChatMessage]
    ) -> bool:
        """Processes commands in the specified message and updates it.
        Returns True if a command was found and an attempt to execute it was made.
        """
        msg_to_process = modified_messages[target_idx]
        original_content = msg_to_process.content
>       processed_content, found, modified = self._process_content(msg_to_process)

src\command_parser.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150FB9A8700>
msg_to_process = ChatMessage(role='user', content='Actually, !/unset(model) nevermind.', name=None, tool_calls=None, tool_call_id=None)

    def _process_content(
        self, msg_to_process: models.ChatMessage
    ) -> tuple[str | list[models.MessageContentPart] | None, bool, bool]:
        if isinstance(msg_to_process.content, str):
>           return self.command_processor.handle_string_content(msg_to_process.content)

src\command_parser.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_processor.CommandProcessor object at 0x00000150FB9AA410>
msg_content = 'Actually, !/unset(model) nevermind.'

    def handle_string_content(
        self,
        msg_content: str,
    ) -> tuple[str, bool, bool]:
        original_content = msg_content
>       processed_text, command_found = self.process_text_and_execute_command(
            original_content
        )

src\command_processor.py:131: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_processor.CommandProcessor object at 0x00000150FB9AA410>
text_content = 'Actually, !/unset(model) nevermind.'

    def process_text_and_execute_command(self, text_content: str) -> tuple[str, bool]:
        """Processes text for a single command and executes it."""
        commands_found = False
        modified_text = text_content
    
        match = self.config.command_pattern.search(text_content)
        if match:
            commands_found = True
            command_full = match.group(0)
            command_name = match.group("cmd").lower()
            args_str = match.group("args") or ""
            logger.debug(
                "Regex match: Full='%s', Command='%s', ArgsStr='%s'",
                command_full,
                command_name,
                args_str,
            )
            args = parse_arguments(args_str)
    
            replacement = ""
            command_handler = self.config.handlers.get(command_name)
            if command_handler:
                # Get the result from the command handler
                # This could be either a legacy or new CommandResult
>               execution_result: Any = command_handler.execute(
                    args, self.config.proxy_state
                )

src\command_processor.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.commands.unset_cmd.UnsetCommand object at 0x00000150FB9AAA10>
args = {'model': True}
state = <src.core.domain.session.SessionStateAdapter object at 0x00000150FB9A8880>

    def execute(self, args: Mapping[str, Any], state: Any) -> CommandResult:
        keys_to_unset = [k for k, v in args.items() if v is True]
        actions = self._build_unset_actions()
>       messages, persistent_change = self._apply_unset_actions(
            keys_to_unset, state, actions
        )

src\commands\unset_cmd.py:72: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.commands.unset_cmd.UnsetCommand object at 0x00000150FB9AAA10>
keys_to_unset = ['model']
state = <src.core.domain.session.SessionStateAdapter object at 0x00000150FB9A8880>
actions = {'backend': (<function UnsetCommand._create_unset_action.<locals>.action at 0x00000150FAD3EB90>, False), 'command-pref...AA10>>, True), 'dir': (<function UnsetCommand._create_unset_action.<locals>.action at 0x00000150FAD3E680>, False), ...}

    def _apply_unset_actions(
        self,
        keys_to_unset: list[str],
        state: Any,
        actions: dict[str, tuple[Callable[[Any], str], bool]],
    ) -> tuple[list[str], bool]:
        messages: list[str] = []
        persistent_change = False
        for key in keys_to_unset:
            if key not in actions:
                continue
            action_func, is_persistent = actions[key]
>           message = action_func(state)

src\commands\unset_cmd.py:237: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

state = <src.core.domain.session.SessionStateAdapter object at 0x00000150FB9A8880>

    def action(state: Any) -> str:
>       getattr(state, method_name)()
E       AttributeError: 'SessionStateAdapter' object has no attribute 'unset_override_model'. Did you mean: 'set_override_model'?

src\commands\unset_cmd.py:64: AttributeError
____________ TestProcessTextForCommands.test_set_and_unset_project ____________

self = <test_process_text_for_commands.TestProcessTextForCommands object at 0x00000150F7478A60>

    def test_set_and_unset_project(self):
        session = Session(session_id="test_session")
        current_session_state = session.state
        processed_messages, _ = process_commands_in_messages(
            [ChatMessage(role="user", content="!/set(project='abc def')")],
            current_session_state,
            app=self.mock_app,
            command_prefix="!/",
        )
        processed_text = processed_messages[0].content if processed_messages else ""
        assert processed_text == ""
        assert session.state.project == "abc def"
>       processed_messages, _ = process_commands_in_messages(
            [ChatMessage(role="user", content="!/unset(project)")],
            session.state,
            app=self.mock_app,
            command_prefix="!/",
        )

tests\unit\proxy_logic_tests\test_process_text_for_commands.py:248: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

messages = [ChatMessage(role='user', content='!/unset(project)', name=None, tool_calls=None, tool_call_id=None)]
current_proxy_state = <src.core.domain.session.SessionStateAdapter object at 0x00000150FAE64490>
app = <Mock id='1447318416176'>, command_prefix = '!/'

    def process_commands_in_messages(
        messages: list[models.ChatMessage],
        current_proxy_state: SessionStateAdapter,
        app: FastAPI | None = None,
        command_prefix: str = DEFAULT_COMMAND_PREFIX,
    ) -> tuple[list[models.ChatMessage], bool]:
        """
        Processes a list of chat messages to identify and execute embedded commands.
    
        This is the primary public interface for command processing. It initializes
        a CommandParser and uses it to process the messages.
        """
        if not messages:
            logger.debug("process_commands_in_messages received empty messages list.")
            return messages, False
    
        functional_backends: set[str] | None = None
        if app and hasattr(app, "state") and hasattr(app.state, "functional_backends"):
            functional_backends = app.state.functional_backends
        else:
            logger.warning(
                "FastAPI app instance or functional_backends not available in "
                "app.state. CommandParser will be initialized without specific "
                "functional_backends.",
            )
    
        parser_config = CommandParserConfig(
            proxy_state=current_proxy_state,
            app=app,  # type: ignore
            preserve_unknown=True,
            functional_backends=functional_backends,
        )
        parser = CommandParser(parser_config, command_prefix=command_prefix)
    
>       final_messages, commands_processed = parser.process_messages(messages)

src\command_parser.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150FAE66380>
messages = [ChatMessage(role='user', content='!/unset(project)', name=None, tool_calls=None, tool_call_id=None)]

    def process_messages(
        self, messages: list[models.ChatMessage]
    ) -> tuple[list[models.ChatMessage], bool]:
        self.command_results.clear()
        if not messages:
            logger.debug("process_messages received empty messages list.")
            return messages, False
    
        # Find the index of the last message containing a command to avoid unnecessary processing.
        command_message_idx = -1
        for i in range(len(messages) - 1, -1, -1):
            text_for_check = get_text_for_command_check(messages[i].content)
            if self.command_pattern.search(text_for_check):
                command_message_idx = i
                break
    
        # If no command is found, return the original list reference, avoiding any copies.
        if command_message_idx == -1:
            return messages, False
    
        # A command was found. Create a shallow copy of the list and deep copy only the message
        # that needs to be modified. This is the core performance optimization.
        modified_messages = list(messages)
        msg_to_process = modified_messages[command_message_idx].model_copy(deep=True)
        modified_messages[command_message_idx] = msg_to_process
    
>       overall_commands_processed = self._execute_commands_in_target_message(
            command_message_idx, modified_messages
        )

src\command_parser.py:213: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150FAE66380>
target_idx = 0
modified_messages = [ChatMessage(role='user', content='!/unset(project)', name=None, tool_calls=None, tool_call_id=None)]

    def _execute_commands_in_target_message(
        self, target_idx: int, modified_messages: list[models.ChatMessage]
    ) -> bool:
        """Processes commands in the specified message and updates it.
        Returns True if a command was found and an attempt to execute it was made.
        """
        msg_to_process = modified_messages[target_idx]
        original_content = msg_to_process.content
>       processed_content, found, modified = self._process_content(msg_to_process)

src\command_parser.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150FAE66380>
msg_to_process = ChatMessage(role='user', content='!/unset(project)', name=None, tool_calls=None, tool_call_id=None)

    def _process_content(
        self, msg_to_process: models.ChatMessage
    ) -> tuple[str | list[models.MessageContentPart] | None, bool, bool]:
        if isinstance(msg_to_process.content, str):
>           return self.command_processor.handle_string_content(msg_to_process.content)

src\command_parser.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_processor.CommandProcessor object at 0x00000150FAE67F70>
msg_content = '!/unset(project)'

    def handle_string_content(
        self,
        msg_content: str,
    ) -> tuple[str, bool, bool]:
        original_content = msg_content
>       processed_text, command_found = self.process_text_and_execute_command(
            original_content
        )

src\command_processor.py:131: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_processor.CommandProcessor object at 0x00000150FAE67F70>
text_content = '!/unset(project)'

    def process_text_and_execute_command(self, text_content: str) -> tuple[str, bool]:
        """Processes text for a single command and executes it."""
        commands_found = False
        modified_text = text_content
    
        match = self.config.command_pattern.search(text_content)
        if match:
            commands_found = True
            command_full = match.group(0)
            command_name = match.group("cmd").lower()
            args_str = match.group("args") or ""
            logger.debug(
                "Regex match: Full='%s', Command='%s', ArgsStr='%s'",
                command_full,
                command_name,
                args_str,
            )
            args = parse_arguments(args_str)
    
            replacement = ""
            command_handler = self.config.handlers.get(command_name)
            if command_handler:
                # Get the result from the command handler
                # This could be either a legacy or new CommandResult
>               execution_result: Any = command_handler.execute(
                    args, self.config.proxy_state
                )

src\command_processor.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.commands.unset_cmd.UnsetCommand object at 0x00000150FAE651E0>
args = {'project': True}
state = <src.core.domain.session.SessionStateAdapter object at 0x00000150FAE64490>

    def execute(self, args: Mapping[str, Any], state: Any) -> CommandResult:
        keys_to_unset = [k for k, v in args.items() if v is True]
        actions = self._build_unset_actions()
>       messages, persistent_change = self._apply_unset_actions(
            keys_to_unset, state, actions
        )

src\commands\unset_cmd.py:72: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.commands.unset_cmd.UnsetCommand object at 0x00000150FAE651E0>
keys_to_unset = ['project']
state = <src.core.domain.session.SessionStateAdapter object at 0x00000150FAE64490>
actions = {'backend': (<function UnsetCommand._create_unset_action.<locals>.action at 0x00000150FB11CF70>, False), 'command-pref...51E0>>, True), 'dir': (<function UnsetCommand._create_unset_action.<locals>.action at 0x00000150FB11CD30>, False), ...}

    def _apply_unset_actions(
        self,
        keys_to_unset: list[str],
        state: Any,
        actions: dict[str, tuple[Callable[[Any], str], bool]],
    ) -> tuple[list[str], bool]:
        messages: list[str] = []
        persistent_change = False
        for key in keys_to_unset:
            if key not in actions:
                continue
            action_func, is_persistent = actions[key]
>           message = action_func(state)

src\commands\unset_cmd.py:237: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

state = <src.core.domain.session.SessionStateAdapter object at 0x00000150FAE64490>

    def action(state: Any) -> str:
>       getattr(state, method_name)()
E       AttributeError: 'SessionStateAdapter' object has no attribute 'unset_project'. Did you mean: 'set_project'?

src\commands\unset_cmd.py:64: AttributeError
------------------------------ Captured log call ------------------------------
INFO     src.command_parser:command_parser.py:135 Content modified by command in message index 0. Role: user.
INFO     src.command_parser:command_parser.py:170 Retaining message (role: user, index: 0) as transformed empty content because it was originally a pure command.
______ TestProcessTextForCommands.test_unset_model_and_project_together _______

self = <test_process_text_for_commands.TestProcessTextForCommands object at 0x00000150F7478C40>

    def test_unset_model_and_project_together(self):
        session = Session(session_id="test_session")
        current_session_state = session.state
    
        # Set initial model and project
        new_backend_config = current_session_state.backend_config.with_model("foo")
        new_session_state = current_session_state.with_backend_config(
            new_backend_config
        )
        new_session_state = new_session_state.with_project("bar")
        session.state = new_session_state
    
>       processed_messages, commands_found = process_commands_in_messages(
            [ChatMessage(role="user", content="!/unset(model, project)")],
            session.state,
            app=self.mock_app,
            command_prefix="!/",
        )

tests\unit\proxy_logic_tests\test_process_text_for_commands.py:270: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

messages = [ChatMessage(role='user', content='!/unset(model, project)', name=None, tool_calls=None, tool_call_id=None)]
current_proxy_state = <src.core.domain.session.SessionStateAdapter object at 0x00000150F9C9B370>
app = <Mock id='1447299748224'>, command_prefix = '!/'

    def process_commands_in_messages(
        messages: list[models.ChatMessage],
        current_proxy_state: SessionStateAdapter,
        app: FastAPI | None = None,
        command_prefix: str = DEFAULT_COMMAND_PREFIX,
    ) -> tuple[list[models.ChatMessage], bool]:
        """
        Processes a list of chat messages to identify and execute embedded commands.
    
        This is the primary public interface for command processing. It initializes
        a CommandParser and uses it to process the messages.
        """
        if not messages:
            logger.debug("process_commands_in_messages received empty messages list.")
            return messages, False
    
        functional_backends: set[str] | None = None
        if app and hasattr(app, "state") and hasattr(app.state, "functional_backends"):
            functional_backends = app.state.functional_backends
        else:
            logger.warning(
                "FastAPI app instance or functional_backends not available in "
                "app.state. CommandParser will be initialized without specific "
                "functional_backends.",
            )
    
        parser_config = CommandParserConfig(
            proxy_state=current_proxy_state,
            app=app,  # type: ignore
            preserve_unknown=True,
            functional_backends=functional_backends,
        )
        parser = CommandParser(parser_config, command_prefix=command_prefix)
    
>       final_messages, commands_processed = parser.process_messages(messages)

src\command_parser.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150F9C9A7D0>
messages = [ChatMessage(role='user', content='!/unset(model, project)', name=None, tool_calls=None, tool_call_id=None)]

    def process_messages(
        self, messages: list[models.ChatMessage]
    ) -> tuple[list[models.ChatMessage], bool]:
        self.command_results.clear()
        if not messages:
            logger.debug("process_messages received empty messages list.")
            return messages, False
    
        # Find the index of the last message containing a command to avoid unnecessary processing.
        command_message_idx = -1
        for i in range(len(messages) - 1, -1, -1):
            text_for_check = get_text_for_command_check(messages[i].content)
            if self.command_pattern.search(text_for_check):
                command_message_idx = i
                break
    
        # If no command is found, return the original list reference, avoiding any copies.
        if command_message_idx == -1:
            return messages, False
    
        # A command was found. Create a shallow copy of the list and deep copy only the message
        # that needs to be modified. This is the core performance optimization.
        modified_messages = list(messages)
        msg_to_process = modified_messages[command_message_idx].model_copy(deep=True)
        modified_messages[command_message_idx] = msg_to_process
    
>       overall_commands_processed = self._execute_commands_in_target_message(
            command_message_idx, modified_messages
        )

src\command_parser.py:213: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150F9C9A7D0>
target_idx = 0
modified_messages = [ChatMessage(role='user', content='!/unset(model, project)', name=None, tool_calls=None, tool_call_id=None)]

    def _execute_commands_in_target_message(
        self, target_idx: int, modified_messages: list[models.ChatMessage]
    ) -> bool:
        """Processes commands in the specified message and updates it.
        Returns True if a command was found and an attempt to execute it was made.
        """
        msg_to_process = modified_messages[target_idx]
        original_content = msg_to_process.content
>       processed_content, found, modified = self._process_content(msg_to_process)

src\command_parser.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150F9C9A7D0>
msg_to_process = ChatMessage(role='user', content='!/unset(model, project)', name=None, tool_calls=None, tool_call_id=None)

    def _process_content(
        self, msg_to_process: models.ChatMessage
    ) -> tuple[str | list[models.MessageContentPart] | None, bool, bool]:
        if isinstance(msg_to_process.content, str):
>           return self.command_processor.handle_string_content(msg_to_process.content)

src\command_parser.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_processor.CommandProcessor object at 0x00000150F9C995A0>
msg_content = '!/unset(model, project)'

    def handle_string_content(
        self,
        msg_content: str,
    ) -> tuple[str, bool, bool]:
        original_content = msg_content
>       processed_text, command_found = self.process_text_and_execute_command(
            original_content
        )

src\command_processor.py:131: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_processor.CommandProcessor object at 0x00000150F9C995A0>
text_content = '!/unset(model, project)'

    def process_text_and_execute_command(self, text_content: str) -> tuple[str, bool]:
        """Processes text for a single command and executes it."""
        commands_found = False
        modified_text = text_content
    
        match = self.config.command_pattern.search(text_content)
        if match:
            commands_found = True
            command_full = match.group(0)
            command_name = match.group("cmd").lower()
            args_str = match.group("args") or ""
            logger.debug(
                "Regex match: Full='%s', Command='%s', ArgsStr='%s'",
                command_full,
                command_name,
                args_str,
            )
            args = parse_arguments(args_str)
    
            replacement = ""
            command_handler = self.config.handlers.get(command_name)
            if command_handler:
                # Get the result from the command handler
                # This could be either a legacy or new CommandResult
>               execution_result: Any = command_handler.execute(
                    args, self.config.proxy_state
                )

src\command_processor.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.commands.unset_cmd.UnsetCommand object at 0x00000150F9C99E40>
args = {'model': True, 'project': True}
state = <src.core.domain.session.SessionStateAdapter object at 0x00000150F9C9B370>

    def execute(self, args: Mapping[str, Any], state: Any) -> CommandResult:
        keys_to_unset = [k for k, v in args.items() if v is True]
        actions = self._build_unset_actions()
>       messages, persistent_change = self._apply_unset_actions(
            keys_to_unset, state, actions
        )

src\commands\unset_cmd.py:72: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.commands.unset_cmd.UnsetCommand object at 0x00000150F9C99E40>
keys_to_unset = ['model', 'project']
state = <src.core.domain.session.SessionStateAdapter object at 0x00000150F9C9B370>
actions = {'backend': (<function UnsetCommand._create_unset_action.<locals>.action at 0x00000150FB1CD6C0>, False), 'command-pref...9E40>>, True), 'dir': (<function UnsetCommand._create_unset_action.<locals>.action at 0x00000150F9C36710>, False), ...}

    def _apply_unset_actions(
        self,
        keys_to_unset: list[str],
        state: Any,
        actions: dict[str, tuple[Callable[[Any], str], bool]],
    ) -> tuple[list[str], bool]:
        messages: list[str] = []
        persistent_change = False
        for key in keys_to_unset:
            if key not in actions:
                continue
            action_func, is_persistent = actions[key]
>           message = action_func(state)

src\commands\unset_cmd.py:237: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

state = <src.core.domain.session.SessionStateAdapter object at 0x00000150F9C9B370>

    def action(state: Any) -> str:
>       getattr(state, method_name)()
E       AttributeError: 'SessionStateAdapter' object has no attribute 'unset_override_model'. Did you mean: 'set_override_model'?

src\commands\unset_cmd.py:64: AttributeError
____________ TestProcessTextForCommands.test_set_interactive_mode _____________

self = <test_process_text_for_commands.TestProcessTextForCommands object at 0x00000150F7478E20>

    def test_set_interactive_mode(self):
        session = Session(session_id="test_session")
        current_session_state = session.state
        text = "hello !/set(interactive-mode=ON)"
        processed_messages, found = process_commands_in_messages(
            [ChatMessage(role="user", content=text)],
            current_session_state,
            app=self.mock_app,
            command_prefix="!/",
        )
        processed_text = processed_messages[0].content if processed_messages else ""
        assert processed_text == "hello"
        assert found
>       assert session.state.interactive_just_enabled
E       assert False
E        +  where False = <src.core.domain.session.SessionStateAdapter object at 0x00000150FAE78BE0>.interactive_just_enabled
E        +    where <src.core.domain.session.SessionStateAdapter object at 0x00000150FAE78BE0> = <src.core.domain.session.Session object at 0x00000150FAE78310>.state

tests\unit\proxy_logic_tests\test_process_text_for_commands.py:295: AssertionError
------------------------------ Captured log call ------------------------------
INFO     src.command_parser:command_parser.py:135 Content modified by command in message index 0. Role: user.
___________ TestProcessTextForCommands.test_unset_interactive_mode ____________

self = <test_process_text_for_commands.TestProcessTextForCommands object at 0x00000150F7479000>

    def test_unset_interactive_mode(self):
        session = Session(session_id="test_session")
        current_session_state = session.state
    
        # Set initial interactive mode
        session.state = current_session_state.with_interactive_just_enabled(True)
        assert session.state.interactive_just_enabled
    
        text = "!/unset(interactive)"
>       processed_messages, found = process_commands_in_messages(
            [ChatMessage(role="user", content=text)],
            session.state,
            app=self.mock_app,
            command_prefix="!/",
        )

tests\unit\proxy_logic_tests\test_process_text_for_commands.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

messages = [ChatMessage(role='user', content='!/unset(interactive)', name=None, tool_calls=None, tool_call_id=None)]
current_proxy_state = <src.core.domain.session.SessionStateAdapter object at 0x00000150FB9AAEF0>
app = <Mock id='1447330229664'>, command_prefix = '!/'

    def process_commands_in_messages(
        messages: list[models.ChatMessage],
        current_proxy_state: SessionStateAdapter,
        app: FastAPI | None = None,
        command_prefix: str = DEFAULT_COMMAND_PREFIX,
    ) -> tuple[list[models.ChatMessage], bool]:
        """
        Processes a list of chat messages to identify and execute embedded commands.
    
        This is the primary public interface for command processing. It initializes
        a CommandParser and uses it to process the messages.
        """
        if not messages:
            logger.debug("process_commands_in_messages received empty messages list.")
            return messages, False
    
        functional_backends: set[str] | None = None
        if app and hasattr(app, "state") and hasattr(app.state, "functional_backends"):
            functional_backends = app.state.functional_backends
        else:
            logger.warning(
                "FastAPI app instance or functional_backends not available in "
                "app.state. CommandParser will be initialized without specific "
                "functional_backends.",
            )
    
        parser_config = CommandParserConfig(
            proxy_state=current_proxy_state,
            app=app,  # type: ignore
            preserve_unknown=True,
            functional_backends=functional_backends,
        )
        parser = CommandParser(parser_config, command_prefix=command_prefix)
    
>       final_messages, commands_processed = parser.process_messages(messages)

src\command_parser.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150FB9AB010>
messages = [ChatMessage(role='user', content='!/unset(interactive)', name=None, tool_calls=None, tool_call_id=None)]

    def process_messages(
        self, messages: list[models.ChatMessage]
    ) -> tuple[list[models.ChatMessage], bool]:
        self.command_results.clear()
        if not messages:
            logger.debug("process_messages received empty messages list.")
            return messages, False
    
        # Find the index of the last message containing a command to avoid unnecessary processing.
        command_message_idx = -1
        for i in range(len(messages) - 1, -1, -1):
            text_for_check = get_text_for_command_check(messages[i].content)
            if self.command_pattern.search(text_for_check):
                command_message_idx = i
                break
    
        # If no command is found, return the original list reference, avoiding any copies.
        if command_message_idx == -1:
            return messages, False
    
        # A command was found. Create a shallow copy of the list and deep copy only the message
        # that needs to be modified. This is the core performance optimization.
        modified_messages = list(messages)
        msg_to_process = modified_messages[command_message_idx].model_copy(deep=True)
        modified_messages[command_message_idx] = msg_to_process
    
>       overall_commands_processed = self._execute_commands_in_target_message(
            command_message_idx, modified_messages
        )

src\command_parser.py:213: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150FB9AB010>
target_idx = 0
modified_messages = [ChatMessage(role='user', content='!/unset(interactive)', name=None, tool_calls=None, tool_call_id=None)]

    def _execute_commands_in_target_message(
        self, target_idx: int, modified_messages: list[models.ChatMessage]
    ) -> bool:
        """Processes commands in the specified message and updates it.
        Returns True if a command was found and an attempt to execute it was made.
        """
        msg_to_process = modified_messages[target_idx]
        original_content = msg_to_process.content
>       processed_content, found, modified = self._process_content(msg_to_process)

src\command_parser.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150FB9AB010>
msg_to_process = ChatMessage(role='user', content='!/unset(interactive)', name=None, tool_calls=None, tool_call_id=None)

    def _process_content(
        self, msg_to_process: models.ChatMessage
    ) -> tuple[str | list[models.MessageContentPart] | None, bool, bool]:
        if isinstance(msg_to_process.content, str):
>           return self.command_processor.handle_string_content(msg_to_process.content)

src\command_parser.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_processor.CommandProcessor object at 0x00000150FB5B66B0>
msg_content = '!/unset(interactive)'

    def handle_string_content(
        self,
        msg_content: str,
    ) -> tuple[str, bool, bool]:
        original_content = msg_content
>       processed_text, command_found = self.process_text_and_execute_command(
            original_content
        )

src\command_processor.py:131: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_processor.CommandProcessor object at 0x00000150FB5B66B0>
text_content = '!/unset(interactive)'

    def process_text_and_execute_command(self, text_content: str) -> tuple[str, bool]:
        """Processes text for a single command and executes it."""
        commands_found = False
        modified_text = text_content
    
        match = self.config.command_pattern.search(text_content)
        if match:
            commands_found = True
            command_full = match.group(0)
            command_name = match.group("cmd").lower()
            args_str = match.group("args") or ""
            logger.debug(
                "Regex match: Full='%s', Command='%s', ArgsStr='%s'",
                command_full,
                command_name,
                args_str,
            )
            args = parse_arguments(args_str)
    
            replacement = ""
            command_handler = self.config.handlers.get(command_name)
            if command_handler:
                # Get the result from the command handler
                # This could be either a legacy or new CommandResult
>               execution_result: Any = command_handler.execute(
                    args, self.config.proxy_state
                )

src\command_processor.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.commands.unset_cmd.UnsetCommand object at 0x00000150FB5B6740>
args = {'interactive': True}
state = <src.core.domain.session.SessionStateAdapter object at 0x00000150FB9AAEF0>

    def execute(self, args: Mapping[str, Any], state: Any) -> CommandResult:
        keys_to_unset = [k for k, v in args.items() if v is True]
        actions = self._build_unset_actions()
>       messages, persistent_change = self._apply_unset_actions(
            keys_to_unset, state, actions
        )

src\commands\unset_cmd.py:72: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.commands.unset_cmd.UnsetCommand object at 0x00000150FB5B6740>
keys_to_unset = ['interactive']
state = <src.core.domain.session.SessionStateAdapter object at 0x00000150FB9AAEF0>
actions = {'backend': (<function UnsetCommand._create_unset_action.<locals>.action at 0x00000150FADBBD90>, False), 'command-pref...6740>>, True), 'dir': (<function UnsetCommand._create_unset_action.<locals>.action at 0x00000150FADBACB0>, False), ...}

    def _apply_unset_actions(
        self,
        keys_to_unset: list[str],
        state: Any,
        actions: dict[str, tuple[Callable[[Any], str], bool]],
    ) -> tuple[list[str], bool]:
        messages: list[str] = []
        persistent_change = False
        for key in keys_to_unset:
            if key not in actions:
                continue
            action_func, is_persistent = actions[key]
>           message = action_func(state)

src\commands\unset_cmd.py:237: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

state = <src.core.domain.session.SessionStateAdapter object at 0x00000150FB9AAEF0>

    def action(state: Any) -> str:
>       getattr(state, method_name)()
E       AttributeError: 'SessionStateAdapter' object has no attribute 'unset_interactive_mode'. Did you mean: 'set_interactive_mode'?

src\commands\unset_cmd.py:64: AttributeError
_____ TestProcessTextForCommands.test_unknown_command_removed_interactive _____

self = <test_process_text_for_commands.TestProcessTextForCommands object at 0x00000150F7479150>

    def test_unknown_command_removed_interactive(self):
        session = Session(session_id="test_session")
        state = session.state
>       config = CommandParserConfig(
            session_state=state,
            app=self.mock_app,
            preserve_unknown=False,
            functional_backends=self.mock_app.state.functional_backends,
        )
E       TypeError: CommandParserConfig.__init__() got an unexpected keyword argument 'session_state'

tests\unit\proxy_logic_tests\test_process_text_for_commands.py:350: TypeError
________ TestProcessTextForCommands.test_set_invalid_model_interactive ________

self = <test_process_text_for_commands.TestProcessTextForCommands object at 0x00000150F74789A0>

    def test_set_invalid_model_interactive(self):
        session = Session(session_id="test_session")
        state = session.state
>       config = CommandParserConfig(
            session_state=state,
            app=self.mock_app,
            preserve_unknown=False,
            functional_backends=self.mock_app.state.functional_backends,
        )
E       TypeError: CommandParserConfig.__init__() got an unexpected keyword argument 'session_state'

tests\unit\proxy_logic_tests\test_process_text_for_commands.py:372: TypeError
______ TestProcessTextForCommands.test_set_invalid_model_noninteractive _______

self = <test_process_text_for_commands.TestProcessTextForCommands object at 0x00000150F74783A0>

    def test_set_invalid_model_noninteractive(self):
        session = Session(session_id="test_session")
        state = session.state
>       config = CommandParserConfig(
            session_state=state,
            app=self.mock_app,
            preserve_unknown=True,
            functional_backends=self.mock_app.state.functional_backends,
        )
E       TypeError: CommandParserConfig.__init__() got an unexpected keyword argument 'session_state'

tests\unit\proxy_logic_tests\test_process_text_for_commands.py:390: TypeError
___________________________ test_build_app_uses_env ___________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x00000150FB49DD80>

    def test_build_app_uses_env(monkeypatch):
        monkeypatch.delenv("GEMINI_API_KEY", raising=False)
        for i in range(1, 21):
            monkeypatch.delenv(f"GEMINI_API_KEY_{i}", raising=False)
        monkeypatch.setenv("LLM_BACKEND", "gemini")
        monkeypatch.setenv("GEMINI_API_KEY", "KEY")
        monkeypatch.setenv("COMMAND_PREFIX", "??/")
        app = app_main_build_app()
        with TestClient(app):  # Ensure lifespan runs
>           assert app.state.app_config.backends.default_backend == "gemini"
E           AssertionError: assert 'openai' == 'gemini'
E             
E             - gemini
E             + openai

tests\unit\test_cli.py:132: AssertionError
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:40.569047Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:40.569546Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:40.578046Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:40.592546Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:354 2025-08-17T00:00:40.594546Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: help
INFO     src.core.services.command_service:command_service.py:78 Registered command: backend
INFO     src.core.services.command_service:command_service.py:78 Registered command: model
INFO     src.core.services.command_service:command_service.py:78 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:78 Registered command: project
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:78 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:78 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:78 Registered command: set
INFO     src.core.services.command_service:command_service.py:78 Registered command: unset
INFO     src.core.services.command_service:command_service.py:78 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:78 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:78 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:78 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:78 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:269 Registered 26 command handlers with registry
INFO     src.core.app.application_factory:application_factory.py:368 2025-08-17T00:00:40.604546Z [info     ] Shutting down application      [src.core.app.application_factory]
____________________ test_save_and_load_persistent_config _____________________

tmp_path = WindowsPath('C:/Users/Mateusz/AppData/Local/Temp/pytest-of-Mateusz/pytest-2396/test_save_and_load_persistent_0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x00000150FB5BC760>

    def test_save_and_load_persistent_config(tmp_path, monkeypatch):
        cfg_path = tmp_path / "cfg.json"
        # Ensure a clean slate for keys that might be set by other tests or global env
        monkeypatch.delenv("OPENROUTER_API_KEY", raising=False)
        monkeypatch.delenv("GEMINI_API_KEY", raising=False)
        monkeypatch.setenv("OPENROUTER_API_KEY_1", "K")  # Use numbered keys for persistence
        monkeypatch.setenv("GEMINI_API_KEY_1", "G")
        monkeypatch.setenv("LLM_BACKEND", "openrouter")
        app_config = load_config(str(cfg_path))
        app = build_app(config=app_config)
        with TestClient(
            app
        ) as client:  # Auth headers not needed if client fixture handles it
            client.app.state.app_config.failover_routes["r1"] = {
                "policy": "k",
                "elements": ["openrouter:model-a"],
            }
            client.app.state.app_config.session.default_interactive_mode = True
            client.app.state.app_config.backends.default_backend = (
                "gemini"  # This is the runtime state
            )
            client.app.state.app_config.auth.redact_api_keys_in_prompts = False
            client.app.state.app_config.command_prefix = "$/"
            # Assuming config_manager is now part of the service provider or app_config handles saving
            # For now, let's assume app_config.save() is the correct way if it exists, or remove if not needed.
            # Based on app_config.py, there's no save method directly on AppConfig.
            # This might need a separate service for persistence. For now, commenting out.
            # client.app.state.config_manager.save() # This line needs to be re-evaluated.
    
>       data = json.loads(cfg_path.read_text())

tests\unit\test_config_persistence.py:49: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = WindowsPath('C:/Users/Mateusz/AppData/Local/Temp/pytest-of-Mateusz/pytest-2396/test_save_and_load_persistent_0/cfg.json')
encoding = 'locale', errors = None

    def read_text(self, encoding=None, errors=None):
        """
        Open the file in text mode, read it, and close the file.
        """
        encoding = io.text_encoding(encoding)
>       with self.open(mode='r', encoding=encoding, errors=errors) as f:

C:\Program Files\Python310\lib\pathlib.py:1134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = WindowsPath('C:/Users/Mateusz/AppData/Local/Temp/pytest-of-Mateusz/pytest-2396/test_save_and_load_persistent_0/cfg.json')
mode = 'r', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return self._accessor.open(self, mode, buffering, encoding, errors,
                                   newline)
E       FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\Mateusz\\AppData\\Local\\Temp\\pytest-of-Mateusz\\pytest-2396\\test_save_and_load_persistent_0\\cfg.json'

C:\Program Files\Python310\lib\pathlib.py:1119: FileNotFoundError
------------------------------ Captured log call ------------------------------
WARNING  src.core.config.app_config:app_config.py:387 Configuration file not found: C:\Users\Mateusz\AppData\Local\Temp\pytest-of-Mateusz\pytest-2396\test_save_and_load_persistent_0\cfg.json
INFO     root:_base.py:223 2025-08-17T00:00:40.815545Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:40.816044Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:40.823543Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:40.836043Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:354 2025-08-17T00:00:40.837544Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: help
INFO     src.core.services.command_service:command_service.py:78 Registered command: backend
INFO     src.core.services.command_service:command_service.py:78 Registered command: model
INFO     src.core.services.command_service:command_service.py:78 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:78 Registered command: project
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:78 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:78 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:78 Registered command: set
INFO     src.core.services.command_service:command_service.py:78 Registered command: unset
INFO     src.core.services.command_service:command_service.py:78 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:78 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:78 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:78 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:78 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:269 Registered 26 command handlers with registry
INFO     src.core.app.application_factory:application_factory.py:368 2025-08-17T00:00:40.846044Z [info     ] Shutting down application      [src.core.app.application_factory]
_______________________ test_invalid_persisted_backend ________________________

tmp_path = WindowsPath('C:/Users/Mateusz/AppData/Local/Temp/pytest-of-Mateusz/pytest-2396/test_invalid_persisted_backend0')
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x00000150FB4B0CA0>

    def test_invalid_persisted_backend(tmp_path, monkeypatch):
        cfg_path = tmp_path / "cfg.json"
        # Persist an invalid default_backend
        invalid_cfg_data = {"default_backend": "non_existent_backend"}
        cfg_path.write_text(json.dumps(invalid_cfg_data))
    
        # Ensure no functional backends are accidentally configured via env that might match
        monkeypatch.delenv("OPENROUTER_API_KEY", raising=False)
        monkeypatch.delenv("GEMINI_API_KEY", raising=False)
        monkeypatch.setenv(
            "OPENROUTER_API_KEY_1", "K_temp"
        )  # Ensure some backend could be functional
    
        app_config = load_config(str(cfg_path))
        app = build_app(config=app_config)
        with pytest.raises(ValueError) as excinfo:
            with TestClient(app):
                pass  # Context manager needs a body
            assert (
                "Default backend 'non_existent_backend' is not in functional_backends."
>               in str(excinfo.value)
            )

tests\unit\test_config_persistence.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <ExceptionInfo for raises contextmanager>

    @property
    def value(self) -> E:
        """The exception value."""
>       assert self._excinfo is not None, (
            ".value can only be used after the context manager exits"
        )
E       AssertionError: .value can only be used after the context manager exits

.venv\lib\site-packages\_pytest\_code\code.py:607: AssertionError
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:40.873544Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:40.873544Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:40.881544Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:40.893545Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:354 2025-08-17T00:00:40.895043Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: help
INFO     src.core.services.command_service:command_service.py:78 Registered command: backend
INFO     src.core.services.command_service:command_service.py:78 Registered command: model
INFO     src.core.services.command_service:command_service.py:78 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:78 Registered command: project
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:78 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:78 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:78 Registered command: set
INFO     src.core.services.command_service:command_service.py:78 Registered command: unset
INFO     src.core.services.command_service:command_service.py:78 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:78 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:78 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:78 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:78 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:269 Registered 26 command handlers with registry
INFO     src.core.app.application_factory:application_factory.py:368 2025-08-17T00:00:40.904543Z [info     ] Shutting down application      [src.core.app.application_factory]
__________ TestFailoverRoutes.test_create_route_enables_interactive ___________

self = <tests.unit.test_failover_routes.TestFailoverRoutes object at 0x00000150F74F5270>

    def test_create_route_enables_interactive(self):
        session = Session(session_id="test_session")
        state = session.state
        config = CommandParserConfig(
            proxy_state=SessionStateAdapter(state),
            app=self.mock_app,
            functional_backends=self.mock_app.state.functional_backends,
            preserve_unknown=True,
        )
        parser = CommandParser(config, command_prefix="!/")
>       parser.process_messages(
            [
                ChatMessage(
                    role="user", content="!/create-failover-route(name=foo,policy=k)"
                )
            ]
        )

tests\unit\test_failover_routes.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150FB5982E0>
messages = [ChatMessage(role='user', content='!/create-failover-route(name=foo,policy=k)', name=None, tool_calls=None, tool_call_id=None)]

    def process_messages(
        self, messages: list[models.ChatMessage]
    ) -> tuple[list[models.ChatMessage], bool]:
        self.command_results.clear()
        if not messages:
            logger.debug("process_messages received empty messages list.")
            return messages, False
    
        # Find the index of the last message containing a command to avoid unnecessary processing.
        command_message_idx = -1
        for i in range(len(messages) - 1, -1, -1):
            text_for_check = get_text_for_command_check(messages[i].content)
            if self.command_pattern.search(text_for_check):
                command_message_idx = i
                break
    
        # If no command is found, return the original list reference, avoiding any copies.
        if command_message_idx == -1:
            return messages, False
    
        # A command was found. Create a shallow copy of the list and deep copy only the message
        # that needs to be modified. This is the core performance optimization.
        modified_messages = list(messages)
        msg_to_process = modified_messages[command_message_idx].model_copy(deep=True)
        modified_messages[command_message_idx] = msg_to_process
    
>       overall_commands_processed = self._execute_commands_in_target_message(
            command_message_idx, modified_messages
        )

src\command_parser.py:213: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150FB5982E0>
target_idx = 0
modified_messages = [ChatMessage(role='user', content='!/create-failover-route(name=foo,policy=k)', name=None, tool_calls=None, tool_call_id=None)]

    def _execute_commands_in_target_message(
        self, target_idx: int, modified_messages: list[models.ChatMessage]
    ) -> bool:
        """Processes commands in the specified message and updates it.
        Returns True if a command was found and an attempt to execute it was made.
        """
        msg_to_process = modified_messages[target_idx]
        original_content = msg_to_process.content
>       processed_content, found, modified = self._process_content(msg_to_process)

src\command_parser.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150FB5982E0>
msg_to_process = ChatMessage(role='user', content='!/create-failover-route(name=foo,policy=k)', name=None, tool_calls=None, tool_call_id=None)

    def _process_content(
        self, msg_to_process: models.ChatMessage
    ) -> tuple[str | list[models.MessageContentPart] | None, bool, bool]:
        if isinstance(msg_to_process.content, str):
>           return self.command_processor.handle_string_content(msg_to_process.content)

src\command_parser.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_processor.CommandProcessor object at 0x00000150FB5997E0>
msg_content = '!/create-failover-route(name=foo,policy=k)'

    def handle_string_content(
        self,
        msg_content: str,
    ) -> tuple[str, bool, bool]:
        original_content = msg_content
>       processed_text, command_found = self.process_text_and_execute_command(
            original_content
        )

src\command_processor.py:131: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_processor.CommandProcessor object at 0x00000150FB5997E0>
text_content = '!/create-failover-route(name=foo,policy=k)'

    def process_text_and_execute_command(self, text_content: str) -> tuple[str, bool]:
        """Processes text for a single command and executes it."""
        commands_found = False
        modified_text = text_content
    
        match = self.config.command_pattern.search(text_content)
        if match:
            commands_found = True
            command_full = match.group(0)
            command_name = match.group("cmd").lower()
            args_str = match.group("args") or ""
            logger.debug(
                "Regex match: Full='%s', Command='%s', ArgsStr='%s'",
                command_full,
                command_name,
                args_str,
            )
            args = parse_arguments(args_str)
    
            replacement = ""
            command_handler = self.config.handlers.get(command_name)
            if command_handler:
                # Get the result from the command handler
                # This could be either a legacy or new CommandResult
>               execution_result: Any = command_handler.execute(
                    args, self.config.proxy_state
                )

src\command_processor.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.commands.create_failover_route_cmd.CreateFailoverRouteCommand object at 0x00000150FB59A110>
args = {'name': 'foo', 'policy': 'k'}
state = <src.core.domain.session.SessionStateAdapter object at 0x00000150FB598760>

    def execute(self, args: Mapping[str, Any], state: Any) -> CommandResult:
        msgs: list[str] = []
        self._ensure_interactive(state, msgs)
        name = args.get("name")
        policy = str(args.get("policy", "")).lower()
        if not name or policy not in {"k", "m", "km", "mk"}:
            return CommandResult(
                self.name, False, "create-failover-route requires name and valid policy"
            )
>       state.create_failover_route(name, policy)
E       AttributeError: 'SessionStateAdapter' object has no attribute 'create_failover_route'

src\commands\create_failover_route_cmd.py:36: AttributeError
________________ TestFailoverRoutes.test_route_append_and_list ________________

self = <tests.unit.test_failover_routes.TestFailoverRoutes object at 0x00000150F74F5450>

    def test_route_append_and_list(self):
        session = Session(session_id="test_session")
        state = session.state
        config = CommandParserConfig(
            proxy_state=SessionStateAdapter(state),
            app=self.mock_app,
            functional_backends=self.mock_app.state.functional_backends,
            preserve_unknown=True,
        )
        parser = CommandParser(config, command_prefix="!/")
>       parser.process_messages(
            [
                ChatMessage(
                    role="user", content="!/create-failover-route(name=foo,policy=k)"
                ),
                ChatMessage(
                    role="user", content="!/route-append(name=foo,element=bar)"
                ),
                ChatMessage(
                    role="user", content="!/route-append(name=foo,element=baz:qux)"
                ),
            ]
        )

tests\unit\test_failover_routes.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150FB5BF2B0>
messages = [ChatMessage(role='user', content='!/create-failover-route(name=foo,policy=k)', name=None, tool_calls=None, tool_call_...essage(role='user', content='!/route-append(name=foo,element=baz:qux)', name=None, tool_calls=None, tool_call_id=None)]

    def process_messages(
        self, messages: list[models.ChatMessage]
    ) -> tuple[list[models.ChatMessage], bool]:
        self.command_results.clear()
        if not messages:
            logger.debug("process_messages received empty messages list.")
            return messages, False
    
        # Find the index of the last message containing a command to avoid unnecessary processing.
        command_message_idx = -1
        for i in range(len(messages) - 1, -1, -1):
            text_for_check = get_text_for_command_check(messages[i].content)
            if self.command_pattern.search(text_for_check):
                command_message_idx = i
                break
    
        # If no command is found, return the original list reference, avoiding any copies.
        if command_message_idx == -1:
            return messages, False
    
        # A command was found. Create a shallow copy of the list and deep copy only the message
        # that needs to be modified. This is the core performance optimization.
        modified_messages = list(messages)
        msg_to_process = modified_messages[command_message_idx].model_copy(deep=True)
        modified_messages[command_message_idx] = msg_to_process
    
>       overall_commands_processed = self._execute_commands_in_target_message(
            command_message_idx, modified_messages
        )

src\command_parser.py:213: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150FB5BF2B0>
target_idx = 2
modified_messages = [ChatMessage(role='user', content='!/create-failover-route(name=foo,policy=k)', name=None, tool_calls=None, tool_call_...essage(role='user', content='!/route-append(name=foo,element=baz:qux)', name=None, tool_calls=None, tool_call_id=None)]

    def _execute_commands_in_target_message(
        self, target_idx: int, modified_messages: list[models.ChatMessage]
    ) -> bool:
        """Processes commands in the specified message and updates it.
        Returns True if a command was found and an attempt to execute it was made.
        """
        msg_to_process = modified_messages[target_idx]
        original_content = msg_to_process.content
>       processed_content, found, modified = self._process_content(msg_to_process)

src\command_parser.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150FB5BF2B0>
msg_to_process = ChatMessage(role='user', content='!/route-append(name=foo,element=baz:qux)', name=None, tool_calls=None, tool_call_id=None)

    def _process_content(
        self, msg_to_process: models.ChatMessage
    ) -> tuple[str | list[models.MessageContentPart] | None, bool, bool]:
        if isinstance(msg_to_process.content, str):
>           return self.command_processor.handle_string_content(msg_to_process.content)

src\command_parser.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_processor.CommandProcessor object at 0x00000150FB2D0310>
msg_content = '!/route-append(name=foo,element=baz:qux)'

    def handle_string_content(
        self,
        msg_content: str,
    ) -> tuple[str, bool, bool]:
        original_content = msg_content
>       processed_text, command_found = self.process_text_and_execute_command(
            original_content
        )

src\command_processor.py:131: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_processor.CommandProcessor object at 0x00000150FB2D0310>
text_content = '!/route-append(name=foo,element=baz:qux)'

    def process_text_and_execute_command(self, text_content: str) -> tuple[str, bool]:
        """Processes text for a single command and executes it."""
        commands_found = False
        modified_text = text_content
    
        match = self.config.command_pattern.search(text_content)
        if match:
            commands_found = True
            command_full = match.group(0)
            command_name = match.group("cmd").lower()
            args_str = match.group("args") or ""
            logger.debug(
                "Regex match: Full='%s', Command='%s', ArgsStr='%s'",
                command_full,
                command_name,
                args_str,
            )
            args = parse_arguments(args_str)
    
            replacement = ""
            command_handler = self.config.handlers.get(command_name)
            if command_handler:
                # Get the result from the command handler
                # This could be either a legacy or new CommandResult
>               execution_result: Any = command_handler.execute(
                    args, self.config.proxy_state
                )

src\command_processor.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.commands.route_append_cmd.RouteAppendCommand object at 0x00000150FB5BC400>
args = {'element': 'baz:qux', 'name': 'foo'}
state = <src.core.domain.session.SessionStateAdapter object at 0x00000150FB5BCAF0>

    def execute(self, args: Mapping[str, Any], state: Any) -> CommandResult:
        msgs: list[str] = []
        self._ensure_interactive(state, msgs)
        name = args.get("name")
>       if not name or name not in state.failover_routes:
E       AttributeError: 'SessionStateAdapter' object has no attribute 'failover_routes'

src\commands\route_append_cmd.py:31: AttributeError
_______________ TestFailoverRoutes.test_routes_are_server_wide ________________

self = <tests.unit.test_failover_routes.TestFailoverRoutes object at 0x00000150F74F5630>

    def test_routes_are_server_wide(self):
        session1 = Session(session_id="session1")
        state1 = session1.state
        config1 = CommandParserConfig(
            proxy_state=SessionStateAdapter(state1),
            app=self.mock_app,
            functional_backends=self.mock_app.state.functional_backends,
            preserve_unknown=True,
        )
        parser1 = CommandParser(config1, command_prefix="!/")
    
        session2 = Session(session_id="session2")
        state2 = session2.state
        config2 = CommandParserConfig(
            proxy_state=SessionStateAdapter(state2),
            app=self.mock_app,
            functional_backends=self.mock_app.state.functional_backends,
            preserve_unknown=True,
        )
        # parser2 is unused - removed to fix F841
    
        # Create a route in session1
>       parser1.process_messages(
            [
                ChatMessage(
                    role="user", content="!/create-failover-route(name=test,policy=m)"
                )
            ]
        )

tests\unit\test_failover_routes.py:103: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150FB5A4EB0>
messages = [ChatMessage(role='user', content='!/create-failover-route(name=test,policy=m)', name=None, tool_calls=None, tool_call_id=None)]

    def process_messages(
        self, messages: list[models.ChatMessage]
    ) -> tuple[list[models.ChatMessage], bool]:
        self.command_results.clear()
        if not messages:
            logger.debug("process_messages received empty messages list.")
            return messages, False
    
        # Find the index of the last message containing a command to avoid unnecessary processing.
        command_message_idx = -1
        for i in range(len(messages) - 1, -1, -1):
            text_for_check = get_text_for_command_check(messages[i].content)
            if self.command_pattern.search(text_for_check):
                command_message_idx = i
                break
    
        # If no command is found, return the original list reference, avoiding any copies.
        if command_message_idx == -1:
            return messages, False
    
        # A command was found. Create a shallow copy of the list and deep copy only the message
        # that needs to be modified. This is the core performance optimization.
        modified_messages = list(messages)
        msg_to_process = modified_messages[command_message_idx].model_copy(deep=True)
        modified_messages[command_message_idx] = msg_to_process
    
>       overall_commands_processed = self._execute_commands_in_target_message(
            command_message_idx, modified_messages
        )

src\command_parser.py:213: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150FB5A4EB0>
target_idx = 0
modified_messages = [ChatMessage(role='user', content='!/create-failover-route(name=test,policy=m)', name=None, tool_calls=None, tool_call_id=None)]

    def _execute_commands_in_target_message(
        self, target_idx: int, modified_messages: list[models.ChatMessage]
    ) -> bool:
        """Processes commands in the specified message and updates it.
        Returns True if a command was found and an attempt to execute it was made.
        """
        msg_to_process = modified_messages[target_idx]
        original_content = msg_to_process.content
>       processed_content, found, modified = self._process_content(msg_to_process)

src\command_parser.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_parser.CommandParser object at 0x00000150FB5A4EB0>
msg_to_process = ChatMessage(role='user', content='!/create-failover-route(name=test,policy=m)', name=None, tool_calls=None, tool_call_id=None)

    def _process_content(
        self, msg_to_process: models.ChatMessage
    ) -> tuple[str | list[models.MessageContentPart] | None, bool, bool]:
        if isinstance(msg_to_process.content, str):
>           return self.command_processor.handle_string_content(msg_to_process.content)

src\command_parser.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_processor.CommandProcessor object at 0x00000150FB5A4D00>
msg_content = '!/create-failover-route(name=test,policy=m)'

    def handle_string_content(
        self,
        msg_content: str,
    ) -> tuple[str, bool, bool]:
        original_content = msg_content
>       processed_text, command_found = self.process_text_and_execute_command(
            original_content
        )

src\command_processor.py:131: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.command_processor.CommandProcessor object at 0x00000150FB5A4D00>
text_content = '!/create-failover-route(name=test,policy=m)'

    def process_text_and_execute_command(self, text_content: str) -> tuple[str, bool]:
        """Processes text for a single command and executes it."""
        commands_found = False
        modified_text = text_content
    
        match = self.config.command_pattern.search(text_content)
        if match:
            commands_found = True
            command_full = match.group(0)
            command_name = match.group("cmd").lower()
            args_str = match.group("args") or ""
            logger.debug(
                "Regex match: Full='%s', Command='%s', ArgsStr='%s'",
                command_full,
                command_name,
                args_str,
            )
            args = parse_arguments(args_str)
    
            replacement = ""
            command_handler = self.config.handlers.get(command_name)
            if command_handler:
                # Get the result from the command handler
                # This could be either a legacy or new CommandResult
>               execution_result: Any = command_handler.execute(
                    args, self.config.proxy_state
                )

src\command_processor.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <src.commands.create_failover_route_cmd.CreateFailoverRouteCommand object at 0x00000150FB5A47F0>
args = {'name': 'test', 'policy': 'm'}
state = <src.core.domain.session.SessionStateAdapter object at 0x00000150FB5A5900>

    def execute(self, args: Mapping[str, Any], state: Any) -> CommandResult:
        msgs: list[str] = []
        self._ensure_interactive(state, msgs)
        name = args.get("name")
        policy = str(args.get("policy", "")).lower()
        if not name or policy not in {"k", "m", "km", "mk"}:
            return CommandResult(
                self.name, False, "create-failover-route requires name and valid policy"
            )
>       state.create_failover_route(name, policy)
E       AttributeError: 'SessionStateAdapter' object has no attribute 'create_failover_route'

src\commands\create_failover_route_cmd.py:36: AttributeError
________________________ test_openrouter_models_cached ________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x00000150FADBC970>

    def test_openrouter_models_cached(monkeypatch):
        monkeypatch.setenv("OPENROUTER_API_KEY", "KEY")
        monkeypatch.delenv("GEMINI_API_KEY", raising=False)
        for i in range(1, 21):
            monkeypatch.delenv(f"GEMINI_API_KEY_{i}", raising=False)
        for i in range(1, 21):
            monkeypatch.delenv(f"OPENROUTER_API_KEY_{i}", raising=False)
        monkeypatch.setenv("LLM_BACKEND", "openrouter")
        response = {"data": [{"id": "m1"}, {"id": "m2"}]}
        with patch.object(
            OpenRouterBackend, "list_models", new=AsyncMock(return_value=response)
        ) as mock_list:
            app = app_main.build_app()
            with TestClient(
                app, headers={"Authorization": "Bearer test-proxy-key"}
            ) as client:
>               assert client.app.state.openrouter_backend.get_available_models() == [
                    "m1",
                    "m2",
                ]
E               AssertionError: assert [] == ['m1', 'm2']
E                 
E                 Right contains 2 more items, first extra item: 'm1'
E                 
E                 Full diff:
E                 + []
E                 - [
E                 -     'm1',
E                 -     'm2',
E                 - ]

tests\unit\test_model_discovery.py:24: AssertionError
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:41.091547Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:41.091547Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:41.101547Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:41.115044Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:354 2025-08-17T00:00:41.117545Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: help
INFO     src.core.services.command_service:command_service.py:78 Registered command: backend
INFO     src.core.services.command_service:command_service.py:78 Registered command: model
INFO     src.core.services.command_service:command_service.py:78 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:78 Registered command: project
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:78 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:78 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:78 Registered command: set
INFO     src.core.services.command_service:command_service.py:78 Registered command: unset
INFO     src.core.services.command_service:command_service.py:78 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:78 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:78 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:78 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:78 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:269 Registered 26 command handlers with registry
INFO     src.core.app.application_factory:application_factory.py:368 2025-08-17T00:00:41.126544Z [info     ] Shutting down application      [src.core.app.application_factory]
__________________________ test_gemini_models_cached __________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x00000150F9C1D5D0>

    def test_gemini_models_cached(monkeypatch):
        monkeypatch.setenv("LLM_BACKEND", "gemini")
        monkeypatch.setenv("GEMINI_API_KEY", "KEY")
        monkeypatch.delenv("OPENROUTER_API_KEY", raising=False)
        for i in range(1, 21):
            monkeypatch.delenv(f"OPENROUTER_API_KEY_{i}", raising=False)
        for i in range(1, 21):
            monkeypatch.delenv(f"GEMINI_API_KEY_{i}", raising=False)
        response = {"models": [{"name": "g1"}]}
        with patch.object(
            GeminiBackend, "list_models", new=AsyncMock(return_value=response)
        ) as mock_list:
            app = app_main.build_app()
            from fastapi.testclient import TestClient
    
            with TestClient(
                app, headers={"Authorization": "Bearer test-proxy-key"}
            ) as client:
>               assert client.app.state.gemini_backend.get_available_models() == ["g1"]
E               AssertionError: assert [] == ['g1']
E                 
E                 Right contains one more item: 'g1'
E                 
E                 Full diff:
E                 + []
E                 - [
E                 -     'g1',
E                 - ]

tests\unit\test_model_discovery.py:49: AssertionError
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:41.134547Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:41.134547Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:41.142045Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:41.156043Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:354 2025-08-17T00:00:41.157544Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: help
INFO     src.core.services.command_service:command_service.py:78 Registered command: backend
INFO     src.core.services.command_service:command_service.py:78 Registered command: model
INFO     src.core.services.command_service:command_service.py:78 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:78 Registered command: project
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:78 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:78 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:78 Registered command: set
INFO     src.core.services.command_service:command_service.py:78 Registered command: unset
INFO     src.core.services.command_service:command_service.py:78 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:78 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:78 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:78 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:78 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:269 Registered 26 command handlers with registry
INFO     src.core.app.application_factory:application_factory.py:368 2025-08-17T00:00:41.168045Z [info     ] Shutting down application      [src.core.app.application_factory]
__________________________ test_auto_default_backend __________________________

self = <starlette.datastructures.State object at 0x00000150FB88A470>
key = 'backend_type'

    def __getattr__(self, key: Any) -> Any:
        try:
>           return self._state[key]
E           KeyError: 'backend_type'

.venv\lib\site-packages\starlette\datastructures.py:686: KeyError

During handling of the above exception, another exception occurred:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x00000150FB88BFD0>

    def test_auto_default_backend(monkeypatch):
        # Test a scenario where only one backend is functional for auto-detection
        monkeypatch.delenv("LLM_BACKEND", raising=False)
        monkeypatch.delenv("OPENROUTER_API_KEY", raising=False)
        monkeypatch.delenv("GEMINI_API_KEY", raising=False)
        monkeypatch.delenv("ANTHROPIC_API_KEY", raising=False)
        for i in range(1, 21):
            monkeypatch.delenv(f"GEMINI_API_KEY_{i}", raising=False)
            monkeypatch.delenv(f"OPENROUTER_API_KEY_{i}", raising=False)
            monkeypatch.delenv(f"ANTHROPIC_API_KEY_{i}", raising=False)
    
        # Also ensure Qwen OAuth credentials are not available by mocking the credential loading
        with patch(
            "src.connectors.qwen_oauth.QwenOAuthConnector._load_oauth_credentials",
            return_value=False,
        ):
            # With no API keys set and no OAuth credentials, no backends should be functional
            app = app_main.build_app()
            from fastapi.testclient import TestClient
    
            with TestClient(
                app, headers={"Authorization": "Bearer test-proxy-key"}
            ) as client:
                # Should have no backend selected
>               assert client.app.state.backend_type is None

tests\unit\test_model_discovery.py:77: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.datastructures.State object at 0x00000150FB88A470>
key = 'backend_type'

    def __getattr__(self, key: Any) -> Any:
        try:
            return self._state[key]
        except KeyError:
            message = "'{}' object has no attribute '{}'"
>           raise AttributeError(message.format(self.__class__.__name__, key))
E           AttributeError: 'State' object has no attribute 'backend_type'

.venv\lib\site-packages\starlette\datastructures.py:689: AttributeError
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:41.174547Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:41.174547Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:41.184544Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:41.198544Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:354 2025-08-17T00:00:41.201058Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: help
INFO     src.core.services.command_service:command_service.py:78 Registered command: backend
INFO     src.core.services.command_service:command_service.py:78 Registered command: model
INFO     src.core.services.command_service:command_service.py:78 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:78 Registered command: project
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:78 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:78 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:78 Registered command: set
INFO     src.core.services.command_service:command_service.py:78 Registered command: unset
INFO     src.core.services.command_service:command_service.py:78 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:78 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:78 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:78 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:78 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:269 Registered 26 command handlers with registry
INFO     src.core.app.application_factory:application_factory.py:368 2025-08-17T00:00:41.212043Z [info     ] Shutting down application      [src.core.app.application_factory]
_____________________ test_multiple_backends_requires_arg _____________________

self = <starlette.datastructures.State object at 0x00000150FB334340>
key = 'backend_type'

    def __getattr__(self, key: Any) -> Any:
        try:
>           return self._state[key]
E           KeyError: 'backend_type'

.venv\lib\site-packages\starlette\datastructures.py:686: KeyError

During handling of the above exception, another exception occurred:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x00000150FADA1060>

    def test_multiple_backends_requires_arg(monkeypatch):
        # This test is now obsolete because the behavior has changed
        # Now the system automatically selects a default backend when multiple are available
        # We'll test that it selects one of the available backends
        monkeypatch.delenv("LLM_BACKEND", raising=False)
        monkeypatch.setenv("OPENROUTER_API_KEY", "K1")
        monkeypatch.setenv("GEMINI_API_KEY", "K2")
        for i in range(1, 21):
            monkeypatch.delenv(f"GEMINI_API_KEY_{i}", raising=False)
            monkeypatch.delenv(f"OPENROUTER_API_KEY_{i}", raising=False)
        resp_or = {"data": [{"id": "x"}]}
        resp_ge = {"models": [{"name": "g"}]}
        with (
            patch.object(
                OpenRouterBackend, "list_models", new=AsyncMock(return_value=resp_or)
            ),
            patch.object(GeminiBackend, "list_models", new=AsyncMock(return_value=resp_ge)),
            patch(
                "src.connectors.qwen_oauth.QwenOAuthConnector._load_oauth_credentials",
                return_value=False,
            ),
        ):
            app = app_main.build_app()
            from fastapi.testclient import TestClient
    
            with TestClient(
                app, headers={"Authorization": "Bearer test-proxy-key"}
            ) as client:
                # Should have selected one of the backends
>               assert client.app.state.backend_type in ["openrouter", "gemini"]

tests\unit\test_model_discovery.py:109: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.datastructures.State object at 0x00000150FB334340>
key = 'backend_type'

    def __getattr__(self, key: Any) -> Any:
        try:
            return self._state[key]
        except KeyError:
            message = "'{}' object has no attribute '{}'"
>           raise AttributeError(message.format(self.__class__.__name__, key))
E           AttributeError: 'State' object has no attribute 'backend_type'

.venv\lib\site-packages\starlette\datastructures.py:689: AttributeError
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:41.240544Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:41.240544Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:41.248077Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:41.261544Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:354 2025-08-17T00:00:41.263544Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: help
INFO     src.core.services.command_service:command_service.py:78 Registered command: backend
INFO     src.core.services.command_service:command_service.py:78 Registered command: model
INFO     src.core.services.command_service:command_service.py:78 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:78 Registered command: project
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:78 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:78 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:78 Registered command: set
INFO     src.core.services.command_service:command_service.py:78 Registered command: unset
INFO     src.core.services.command_service:command_service.py:78 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:78 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:78 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:78 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:78 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:269 Registered 26 command handlers with registry
INFO     src.core.app.application_factory:application_factory.py:368 2025-08-17T00:00:41.272544Z [info     ] Shutting down application      [src.core.app.application_factory]
_______________________ test_models_endpoint_lists_all ________________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x00000150FB37F550>

    def test_models_endpoint_lists_all(monkeypatch):
        monkeypatch.delenv(
            "OPENROUTER_API_KEY", raising=False
        )  # Ensure unnumbered key is not present
        monkeypatch.delenv(
            "GEMINI_API_KEY", raising=False
        )  # Ensure unnumbered key is not present
        monkeypatch.setenv("OPENROUTER_API_KEY_1", "K1")
        monkeypatch.setenv("GEMINI_API_KEY_1", "K2")  # Changed to numbered key
        monkeypatch.setenv("LLM_BACKEND", "openrouter")
        app = app_main.build_app()
        with TestClient(app, headers={"Authorization": "Bearer test-proxy-key"}) as client:
            resp = client.get("/models")
            assert resp.status_code == 200
            ids = {m["id"] for m in resp.json()["data"]}
>           assert "openrouter:model-a" in ids
E           AssertionError: assert 'openrouter:model-a' in {'claude-3-opus-20240229', 'claude-3-sonnet-20240229', 'gemini-1.5-flash', 'gemini-1.5-pro', 'gpt-3.5-turbo', 'gpt-4'}

tests\unit\test_models_endpoint.py:20: AssertionError
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:41.293544Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:41.293544Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:41.301544Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:41.314544Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:354 2025-08-17T00:00:41.317044Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: help
INFO     src.core.services.command_service:command_service.py:78 Registered command: backend
INFO     src.core.services.command_service:command_service.py:78 Registered command: model
INFO     src.core.services.command_service:command_service.py:78 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:78 Registered command: project
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:78 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:78 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:78 Registered command: set
INFO     src.core.services.command_service:command_service.py:78 Registered command: unset
INFO     src.core.services.command_service:command_service.py:78 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:78 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:78 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:78 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:78 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:269 Registered 26 command handlers with registry
INFO     src.core.app.controllers.models_controller:models_controller.py:47 Listing available models
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openrouter: {}
WARNING  src.core.app.controllers.models_controller:models_controller.py:122 Failed to get models from openrouter: Failed to create backend openrouter: api_key is required for OpenRouterBackend
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for gemini: {}
WARNING  src.core.app.controllers.models_controller:models_controller.py:122 Failed to get models from gemini: Failed to create backend gemini: gemini_api_base_url, key_name, and api_key are required for GeminiBackend
INFO     src.core.app.controllers.models_controller:models_controller.py:128 No models discovered from backends, using default models
INFO     src.core.app.controllers.models_controller:models_controller.py:146 Returning 6 models
INFO     src.core.app.application_factory:application_factory.py:368 2025-08-17T00:00:41.328043Z [info     ] Shutting down application      [src.core.app.application_factory]
______________________ test_v1_models_endpoint_lists_all ______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x00000150F8A1A980>

    def test_v1_models_endpoint_lists_all(monkeypatch):
        monkeypatch.delenv("OPENROUTER_API_KEY", raising=False)
        monkeypatch.delenv("GEMINI_API_KEY", raising=False)
        monkeypatch.setenv("OPENROUTER_API_KEY_1", "K1")
        monkeypatch.setenv("GEMINI_API_KEY_1", "K2")
        monkeypatch.setenv("LLM_BACKEND", "openrouter")
        app = app_main.build_app()
        with TestClient(app, headers={"Authorization": "Bearer test-proxy-key"}) as client:
            resp = client.get("/v1/models")
            assert resp.status_code == 200
            ids = {m["id"] for m in resp.json()["data"]}
>           assert "openrouter:model-a" in ids
E           AssertionError: assert 'openrouter:model-a' in {'claude-3-opus-20240229', 'claude-3-sonnet-20240229', 'gemini-1.5-flash', 'gemini-1.5-pro', 'gpt-3.5-turbo', 'gpt-4'}

tests\unit\test_models_endpoint.py:35: AssertionError
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:41.335054Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:41.335054Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:37 2025-08-17T00:00:41.342578Z [info     ] API Key authentication is enabled [src.core.app.middleware_config] key_count=1
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:41.357046Z [info     ] Application built successfully [src.core.app.application_factory]
INFO     src.core.app.application_factory:application_factory.py:354 2025-08-17T00:00:41.359079Z [info     ] Starting application           [src.core.app.application_factory]
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: help
INFO     src.core.services.command_service:command_service.py:78 Registered command: backend
INFO     src.core.services.command_service:command_service.py:78 Registered command: model
INFO     src.core.services.command_service:command_service.py:78 Registered command: temperature
INFO     src.core.services.command_service:command_service.py:78 Registered command: project
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.services.command_service:command_service.py:78 Registered command: oneoff
INFO     src.core.services.command_service:command_service.py:78 Registered command: pwd
INFO     src.core.services.command_service:command_service.py:78 Registered command: project-dir
INFO     src.core.services.command_service:command_service.py:78 Registered command: set
INFO     src.core.services.command_service:command_service.py:78 Registered command: unset
INFO     src.core.services.command_service:command_service.py:78 Registered command: reasoning-effort
INFO     src.core.services.command_service:command_service.py:78 Registered command: thinking-budget
INFO     src.core.services.command_service:command_service.py:78 Registered command: gemini-generation-config
INFO     src.core.services.command_service:command_service.py:78 Registered command: loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-detection
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-max-repeats
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-ttl
INFO     src.core.services.command_service:command_service.py:78 Registered command: tool-loop-mode
INFO     src.core.services.command_service:command_service.py:78 Registered command: create-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: delete-failover-route
INFO     src.core.services.command_service:command_service.py:78 Registered command: list-failover-routes
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-append
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-clear
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-list
INFO     src.core.services.command_service:command_service.py:78 Registered command: route-prepend
INFO     src.core.commands.handler_factory:handler_factory.py:269 Registered 26 command handlers with registry
INFO     src.core.app.controllers.models_controller:models_controller.py:47 Listing available models
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for openrouter: {}
WARNING  src.core.app.controllers.models_controller:models_controller.py:122 Failed to get models from openrouter: Failed to create backend openrouter: api_key is required for OpenRouterBackend
INFO     src.core.services.backend_service:backend_service.py:337 Backend config for gemini: {}
WARNING  src.core.app.controllers.models_controller:models_controller.py:122 Failed to get models from gemini: Failed to create backend gemini: gemini_api_base_url, key_name, and api_key are required for GeminiBackend
INFO     src.core.app.controllers.models_controller:models_controller.py:128 No models discovered from backends, using default models
INFO     src.core.app.controllers.models_controller:models_controller.py:146 Returning 6 models
INFO     src.core.app.application_factory:application_factory.py:368 2025-08-17T00:00:41.371585Z [info     ] Shutting down application      [src.core.app.application_factory]
__________________________ test_no_print_statements ___________________________

    def test_no_print_statements() -> None:
        repo_root = pathlib.Path(__file__).resolve().parents[2]
        for path in repo_root.rglob("*.py"):
            if (
                "tests" in path.parts
                or ".venv" in path.parts
                or "site-packages" in path.parts
                or ".git" in path.parts
                or "dev" in path.parts
                or "examples" in path.parts
                or "tools" in path.parts
            ):
                continue
            if path in ALLOWED_FILES:
                continue
            if not path.is_file():  # Ensure it's a file, not a directory or symlink
                continue
            try:
                source = path.read_text()
                tree = ast.parse(source)
                for node in ast.walk(tree):
                    if (
                        isinstance(node, ast.Call)
                        and isinstance(node.func, ast.Name)
                        and node.func.id == "print"
                    ):
>                       raise AssertionError(
                            f"print() found in {path} at line {node.lineno}"
                        )
E                       AssertionError: print() found in C:\Users\Mateusz\source\repos\llm-interactive-proxy\VERIFY_FIXES.py at line 11

tests\unit\test_no_prints.py:39: AssertionError
_ TestQwenOAuthInteractiveCommands.test_functional_backends_includes_qwen_oauth _

self = <starlette.datastructures.State object at 0x00000150FB554EB0>
key = 'functional_backends'

    def __getattr__(self, key: Any) -> Any:
        try:
>           return self._state[key]
E           KeyError: 'functional_backends'

.venv\lib\site-packages\starlette\datastructures.py:686: KeyError

During handling of the above exception, another exception occurred:

self = <tests.unit.test_qwen_oauth_interactive_commands.TestQwenOAuthInteractiveCommands object at 0x00000150F759FE50>

    def test_functional_backends_includes_qwen_oauth(self):
        """Test that functional_backends can include qwen-oauth."""
        with patch.dict(
            os.environ, {"DISABLE_AUTH": "true", "DISABLE_ACCOUNTING": "true"}
        ):
            app = build_app()
    
            # Add qwen-oauth to functional backends for testing
>           app.state.functional_backends.add("qwen-oauth")

tests\unit\test_qwen_oauth_interactive_commands.py:172: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <starlette.datastructures.State object at 0x00000150FB554EB0>
key = 'functional_backends'

    def __getattr__(self, key: Any) -> Any:
        try:
            return self._state[key]
        except KeyError:
            message = "'{}' object has no attribute '{}'"
>           raise AttributeError(message.format(self.__class__.__name__, key))
E           AttributeError: 'State' object has no attribute 'functional_backends'

.venv\lib\site-packages\starlette\datastructures.py:689: AttributeError
------------------------------ Captured log call ------------------------------
INFO     root:_base.py:223 2025-08-17T00:00:41.630550Z [info     ] Logging configured             [root] env=development file=None format=console
INFO     src.core.app.application_factory:application_factory.py:308 2025-08-17T00:00:41.630550Z [info     ] Building application with improved factory [src.core.app.application_factory]
INFO     src.core.services.rate_limiter:rate_limiter.py:37 Initialized InMemoryRateLimiter with defaults: 60/60s
INFO     src.core.services.command_service:command_service.py:78 Registered command: hello
INFO     src.core.app.middleware_config:middleware_config.py:40 2025-08-17T00:00:41.640544Z [info     ] API Key authentication is disabled [src.core.app.middleware_config]
INFO     src.core.app.controllers:__init__.py:158 Routes registered successfully
INFO     src.core.app.application_factory:application_factory.py:338 2025-08-17T00:00:41.665044Z [info     ] Application built successfully [src.core.app.application_factory]
=========================== short test summary info ===========================
FAILED tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_models_endpoint_via_http
FAILED tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_messages_endpoint_validation_via_http
FAILED tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_messages_endpoint_with_system_message
FAILED tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_messages_endpoint_streaming_request
FAILED tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_messages_endpoint_with_stop_sequences
FAILED tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_conversation_flow_via_http
FAILED tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_error_handling_invalid_model
FAILED tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_error_handling_missing_required_fields
FAILED tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_parameter_validation_ranges
FAILED tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_health_and_info_endpoints
FAILED tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_concurrent_requests
FAILED tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_large_payload_handling
FAILED tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_unicode_and_special_characters
FAILED tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_content_type_headers
FAILED tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_anthropic_specific_model_names
FAILED tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_endpoint_not_found
FAILED tests/integration/test_anthropic_frontend_integration.py::TestAnthropicFrontendIntegration::test_method_not_allowed
FAILED tests/integration/test_cline_tool_call_implementation.py::TestClineCommandResponses::test_cline_hello_command_returns_tool_calls
FAILED tests/integration/test_cline_tool_call_implementation.py::TestClineCommandResponses::test_cline_set_command_returns_tool_calls
FAILED tests/integration/test_end_to_end_loop_detection.py::test_loop_detection_with_mocked_backend
FAILED tests/integration/test_end_to_end_loop_detection.py::test_loop_detection_in_streaming_response
FAILED tests/integration/test_failover_routes.py::test_failover_route_commands
FAILED tests/integration/test_loop_detection_integration.py::TestLoopDetectionIntegration::test_loop_detection_initialization_on_startup
FAILED tests/integration/test_loop_detection_integration.py::TestLoopDetectionIntegration::test_environment_variable_configuration
FAILED tests/integration/test_loop_detection_middleware.py::test_end_to_end_with_real_app
FAILED tests/integration/test_models_endpoints.py::TestModelsEndpoints::test_models_endpoint_with_auth
FAILED tests/integration/test_models_endpoints.py::TestModelsEndpoints::test_models_with_configured_backends
FAILED tests/integration/test_models_endpoints.py::TestModelsEndpoints::test_models_endpoint_error_handling
FAILED tests/integration/test_phase1_integration.py::test_integration_bridge_async_initialization
FAILED tests/integration/test_pwd_command_integration.py::test_pwd_command_integration_with_project_dir
FAILED tests/integration/test_pwd_command_integration.py::test_pwd_command_integration_without_project_dir
FAILED tests/integration/test_tool_call_loop_detection.py::TestToolCallLoopDetection::test_break_mode_blocks_repeated_tool_calls
FAILED tests/integration/test_tool_call_loop_detection.py::TestToolCallLoopDetection::test_chance_then_break_mode_transparent_retry_success
FAILED tests/integration/test_tool_call_loop_detection.py::TestToolCallLoopDetection::test_chance_then_break_mode_transparent_retry_fail
FAILED tests/integration/test_tool_call_loop_detection.py::TestToolCallLoopDetection::test_different_tool_calls_not_blocked
FAILED tests/integration/test_tool_call_loop_detection.py::TestToolCallLoopDetection::test_disabled_tool_call_loop_detection
FAILED tests/integration/test_updated_hybrid_controller.py::test_hybrid_chat_completions
FAILED tests/integration/test_updated_hybrid_controller.py::test_hybrid_anthropic_messages
FAILED tests/integration/test_versioned_api.py::test_versioned_endpoint_with_commands
FAILED tests/regression/test_chat_completion_regression.py::TestChatCompletionRegression::test_streaming_chat_completion
FAILED tests/unit/chat_completions_tests/test_failover.py::test_failover_missing_keys
FAILED tests/unit/chat_completions_tests/test_multimodal_cross_protocol.py::test_openai_frontend_to_gemini_backend_multimodal
FAILED tests/unit/chat_completions_tests/test_multimodal_cross_protocol.py::test_gemini_frontend_to_openai_backend_multimodal
FAILED tests/unit/chat_completions_tests/test_project_dir_commands.py::test_set_project_dir_command_valid[project-dir]
FAILED tests/unit/chat_completions_tests/test_project_dir_commands.py::test_set_project_dir_command_valid[dir]
FAILED tests/unit/chat_completions_tests/test_project_dir_commands.py::test_set_project_dir_command_valid[project-directory]
FAILED tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureProxyState::test_proxy_state_set_temperature_invalid
FAILED tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceCompletions::test_call_completion_streaming_error
FAILED tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceCompletions::test_call_completion_invalid_response
FAILED tests/unit/core/test_backend_service_enhanced.py::TestBackendServiceCompletions::test_call_completion_invalid_streaming_response
FAILED tests/unit/core/test_config.py::test_app_config_to_legacy_config - pyd...
FAILED tests/unit/core/test_config.py::test_app_config_from_legacy_config - p...
FAILED tests/unit/core/test_config.py::test_app_config_from_env - AssertionEr...
FAILED tests/unit/core/test_config.py::test_load_config - AssertionError: ass...
FAILED tests/unit/openai_connector_tests/test_integration.py::test_set_openai_url_command
FAILED tests/unit/openai_connector_tests/test_url_override.py::test_initialize_with_custom_url
FAILED tests/unit/openai_connector_tests/test_url_override.py::test_set_command_openai_url_integration
FAILED tests/unit/openrouter_connector_tests/test_http_error_streaming.py::test_chat_completions_http_error_streaming
FAILED tests/unit/openrouter_connector_tests/test_request_error.py::test_chat_completions_request_error
FAILED tests/unit/openrouter_connector_tests/test_temperature_handling.py::TestOpenRouterTemperatureHandling::test_temperature_streaming_request
FAILED tests/unit/proxy_logic_tests/test_process_commands_in_messages.py::TestProcessCommandsInMessages::test_unset_model_and_project_in_message
FAILED tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_unset_model_command
FAILED tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_set_and_unset_project
FAILED tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_unset_model_and_project_together
FAILED tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_set_interactive_mode
FAILED tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_unset_interactive_mode
FAILED tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_unknown_command_removed_interactive
FAILED tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_set_invalid_model_interactive
FAILED tests/unit/proxy_logic_tests/test_process_text_for_commands.py::TestProcessTextForCommands::test_set_invalid_model_noninteractive
FAILED tests/unit/test_cli.py::test_build_app_uses_env - AssertionError: asse...
FAILED tests/unit/test_config_persistence.py::test_save_and_load_persistent_config
FAILED tests/unit/test_config_persistence.py::test_invalid_persisted_backend
FAILED tests/unit/test_failover_routes.py::TestFailoverRoutes::test_create_route_enables_interactive
FAILED tests/unit/test_failover_routes.py::TestFailoverRoutes::test_route_append_and_list
FAILED tests/unit/test_failover_routes.py::TestFailoverRoutes::test_routes_are_server_wide
FAILED tests/unit/test_model_discovery.py::test_openrouter_models_cached - As...
FAILED tests/unit/test_model_discovery.py::test_gemini_models_cached - Assert...
FAILED tests/unit/test_model_discovery.py::test_auto_default_backend - Attrib...
FAILED tests/unit/test_model_discovery.py::test_multiple_backends_requires_arg
FAILED tests/unit/test_models_endpoint.py::test_models_endpoint_lists_all - A...
FAILED tests/unit/test_models_endpoint.py::test_v1_models_endpoint_lists_all
FAILED tests/unit/test_no_prints.py::test_no_print_statements - AssertionErro...
FAILED tests/unit/test_qwen_oauth_interactive_commands.py::TestQwenOAuthInteractiveCommands::test_functional_backends_includes_qwen_oauth
ERROR tests/chat_completions_tests/test_anthropic_api_compatibility.py::test_anthropic_messages_non_streaming
ERROR tests/chat_completions_tests/test_anthropic_api_compatibility.py::test_anthropic_messages_with_tool_use_from_openai_tool_calls
ERROR tests/chat_completions_tests/test_anthropic_frontend.py::test_anthropic_messages_non_streaming_frontend
ERROR tests/chat_completions_tests/test_anthropic_frontend.py::test_anthropic_messages_streaming_frontend
ERROR tests/chat_completions_tests/test_anthropic_frontend.py::test_anthropic_messages_auth_failure
ERROR tests/chat_completions_tests/test_anthropic_frontend.py::test_models_endpoint_includes_anthropic
ERROR tests/integration/test_multiple_oneoff_commands.py::TestMultipleOneoffCommands::test_multiple_oneoff_commands_sequence
ERROR tests/integration/test_multiple_oneoff_commands.py::TestMultipleOneoffCommands::test_oneoff_commands_different_sessions
ERROR tests/integration/test_multiple_oneoff_commands.py::TestMultipleOneoffCommands::test_oneoff_command_with_prompt_in_same_message
ERROR tests/integration/test_oneoff_command_integration.py::test_oneoff_command_integration
ERROR tests/unit/chat_completions_tests/test_basic_proxying.py::test_basic_request_proxying_non_streaming
ERROR tests/unit/chat_completions_tests/test_basic_proxying.py::test_basic_request_proxying_streaming
ERROR tests/unit/chat_completions_tests/test_cline_all_commands.py::test_cline_xml_wrapping_for_all_commands
ERROR tests/unit/chat_completions_tests/test_cline_all_commands.py::test_cline_xml_wrapping_error_commands
ERROR tests/unit/chat_completions_tests/test_cline_all_commands.py::test_cline_xml_wrapping_pure_error_commands
ERROR tests/unit/chat_completions_tests/test_cline_all_commands.py::test_non_cline_commands_no_xml_wrapping
ERROR tests/unit/chat_completions_tests/test_cline_hello_command.py::test_cline_hello_command_tool_calls
ERROR tests/unit/chat_completions_tests/test_cline_hello_command.py::test_cline_hello_command_same_request
ERROR tests/unit/chat_completions_tests/test_cline_hello_command.py::test_cline_hello_with_attempt_completion_only
ERROR tests/unit/chat_completions_tests/test_cline_hello_command.py::test_cline_hello_command_first_message
ERROR tests/unit/chat_completions_tests/test_cline_hello_command.py::test_non_cline_hello_command_no_xml_wrapping
ERROR tests/unit/chat_completions_tests/test_cline_isolation.py::test_non_cline_clients_no_xml_wrapping
ERROR tests/unit/chat_completions_tests/test_cline_isolation.py::test_remote_llm_responses_never_xml_wrapped
ERROR tests/unit/chat_completions_tests/test_cline_isolation.py::test_mixed_cline_command_and_llm_prompt
ERROR tests/unit/chat_completions_tests/test_cline_isolation.py::test_streaming_responses_never_xml_wrapped
ERROR tests/unit/chat_completions_tests/test_cline_isolation.py::test_error_responses_from_llm_never_xml_wrapped
ERROR tests/unit/chat_completions_tests/test_cline_output_format.py::test_cline_output_format_exact
ERROR tests/unit/chat_completions_tests/test_cline_output_format.py::test_cline_output_format_other_commands
ERROR tests/unit/chat_completions_tests/test_command_only_requests.py::test_command_only_request_direct_response
ERROR tests/unit/chat_completions_tests/test_command_only_requests.py::test_command_plus_text_direct_response
ERROR tests/unit/chat_completions_tests/test_command_only_requests.py::test_command_with_agent_prefix_direct_response
ERROR tests/unit/chat_completions_tests/test_command_only_requests.py::test_command_only_request_direct_response_explicit_mock
ERROR tests/unit/chat_completions_tests/test_command_only_requests.py::test_hello_command_with_agent_prefix
ERROR tests/unit/chat_completions_tests/test_command_only_requests.py::test_hello_command_followed_by_text
ERROR tests/unit/chat_completions_tests/test_command_only_requests.py::test_hello_command_with_prefix_and_suffix
ERROR tests/unit/chat_completions_tests/test_commands_disabled.py::test_commands_ignored
ERROR tests/unit/chat_completions_tests/test_error_handling.py::test_empty_messages_after_processing_no_commands_bad_request
ERROR tests/unit/chat_completions_tests/test_error_handling.py::test_get_openrouter_headers_no_api_key
ERROR tests/unit/chat_completions_tests/test_error_handling.py::test_invalid_model_noninteractive
ERROR tests/unit/chat_completions_tests/test_failover.py::test_failover_key_rotation
ERROR tests/unit/chat_completions_tests/test_help_command.py::test_help_list_commands
ERROR tests/unit/chat_completions_tests/test_help_command.py::test_help_specific_command
ERROR tests/unit/chat_completions_tests/test_interactive_banner.py::test_first_reply_no_automatic_banner
ERROR tests/unit/chat_completions_tests/test_interactive_banner.py::test_hello_command_returns_banner
ERROR tests/unit/chat_completions_tests/test_interactive_banner.py::test_hello_command_returns_xml_banner_for_cline_agent
ERROR tests/unit/chat_completions_tests/test_interactive_banner.py::test_set_command_returns_xml_for_cline_agent
ERROR tests/unit/chat_completions_tests/test_interactive_commands.py::test_unknown_command_error
ERROR tests/unit/chat_completions_tests/test_interactive_commands.py::test_set_command_confirmation
ERROR tests/unit/chat_completions_tests/test_interactive_commands.py::test_set_backend_confirmation
ERROR tests/unit/chat_completions_tests/test_interactive_commands.py::test_set_backend_nonfunctional
ERROR tests/unit/chat_completions_tests/test_interactive_commands.py::test_set_redaction_flag
ERROR tests/unit/chat_completions_tests/test_interactive_commands.py::test_unset_redaction_flag
ERROR tests/unit/chat_completions_tests/test_model_commands.py::test_set_model_command_integration
ERROR tests/unit/chat_completions_tests/test_model_commands.py::test_unset_model_command_integration
ERROR tests/unit/chat_completions_tests/test_oneoff_command.py::test_oneoff_command[oneoff_with_prompt-request_payload0]
ERROR tests/unit/chat_completions_tests/test_oneoff_command.py::test_oneoff_command[oneoff_alias_with_prompt-request_payload1]
ERROR tests/unit/chat_completions_tests/test_oneoff_command.py::test_oneoff_command[oneoff_without_prompt-request_payload2]
ERROR tests/unit/chat_completions_tests/test_project_commands.py::test_set_project_command_integration
ERROR tests/unit/chat_completions_tests/test_project_commands.py::test_unset_project_command_integration
ERROR tests/unit/chat_completions_tests/test_project_commands.py::test_set_project_name_alias_integration
ERROR tests/unit/chat_completions_tests/test_project_commands.py::test_unset_project_name_alias_integration
ERROR tests/unit/chat_completions_tests/test_project_commands.py::test_force_set_project_blocks_requests
ERROR tests/unit/chat_completions_tests/test_project_commands.py::test_force_set_project_allows_after_set
ERROR tests/unit/chat_completions_tests/test_project_dir_commands.py::test_set_project_dir_command_invalid
ERROR tests/unit/chat_completions_tests/test_project_dir_commands.py::test_unset_project_dir_command[project-dir]
ERROR tests/unit/chat_completions_tests/test_project_dir_commands.py::test_unset_project_dir_command[dir]
ERROR tests/unit/chat_completions_tests/test_project_dir_commands.py::test_unset_project_dir_command[project-directory]
ERROR tests/unit/chat_completions_tests/test_pwd_command.py::test_pwd_command_with_project_dir_set
ERROR tests/unit/chat_completions_tests/test_pwd_command.py::test_pwd_command_without_project_dir_set
ERROR tests/unit/chat_completions_tests/test_rate_limit_wait.py::test_wait_for_rate_limited_backends
ERROR tests/unit/chat_completions_tests/test_rate_limiting.py::test_rate_limit_memory
ERROR tests/unit/chat_completions_tests/test_real_cline_response.py::test_real_cline_hello_response
ERROR tests/unit/chat_completions_tests/test_real_cline_response.py::test_cline_pure_hello_command
ERROR tests/unit/chat_completions_tests/test_real_cline_response.py::test_cline_no_session_id
ERROR tests/unit/chat_completions_tests/test_real_cline_response.py::test_cline_non_command_message
ERROR tests/unit/chat_completions_tests/test_real_cline_response.py::test_cline_first_message_hello
ERROR tests/unit/chat_completions_tests/test_real_cline_response.py::test_cline_first_message_with_detection
ERROR tests/unit/chat_completions_tests/test_real_cline_response.py::test_realistic_cline_hello_request
ERROR tests/unit/chat_completions_tests/test_session_history.py::test_session_records_proxy_and_backend_interactions
ERROR tests/unit/chat_completions_tests/test_session_history.py::test_session_records_streaming_placeholder
ERROR tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_set_temperature_command_valid_float
ERROR tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_set_temperature_command_valid_int
ERROR tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_set_temperature_command_valid_string_number
ERROR tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_set_temperature_command_zero_value
ERROR tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_set_temperature_command_max_openai_value
ERROR tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_set_temperature_command_invalid_negative
ERROR tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_set_temperature_command_invalid_too_high
ERROR tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_set_temperature_command_invalid_string
ERROR tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_unset_temperature_command
ERROR tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_temperature_persistence_across_requests
ERROR tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureCommands::test_temperature_with_message_content
ERROR tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureAPIParameters::test_direct_api_temperature_parameter
ERROR tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureAPIParameters::test_api_temperature_overrides_session_setting
ERROR tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureModelDefaults::test_temperature_model_defaults_applied
ERROR tests/unit/chat_completions_tests/test_temperature_commands.py::TestTemperatureModelDefaults::test_session_temperature_overrides_model_defaults
==== 83 failed, 600 passed, 20 skipped, 79 deselected, 95 errors in 10.34s ====
