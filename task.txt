Onboarding: README.md, instructions for agents: AGENTS.md.

This proxy contains an integrated algorithm which is supposed to automatically detect loops in remote LLM streamed responses.

By loops I mean situations when LLM model at some point of the chat starts endlessly repeating some part of reply, either a few chars, or even multiple sentences. But this task is solely related to in-chat (single streaming response loops), not loops when agent keeps asking LLM a question and it replies with the same response over and over again (but without repeating strings inside single chat), thats a different kind of error and we don't focus on it right now.

Discovered problem is that this algorithm is seriously slow, to the degree that it is stalling the whole proxy. Even small replies takes forever to complete.

Idea that I have is instead of re-inventing the wheel, we could use battle-proven, high-performance loop detection algorithm which is integrated in the `gemini-cli` (terminal based agentic coding tool). `gemini-cli` is written in TypeScript so we would need to port it back to Python. But this still can be much more productive than trying to implement this on our own.

For your convenience I put the source code of `gemini-cli` agent inside this dir: `dev\thrdparty\gemini-cli`.

Please scan it's code and locate code responsible to in-chat (inside one streaming chat response) loop detection. Read/analyze it thoroughly and provide your analysis of the problem.

We need to decide if we keep currently implemented version of this algo (which is buggy), do we try to fix it or do we port related code from the `gemini-cli` loop detector. We need to do what would be more productive. Think hard about this problem and come up with your conclusions. Don't jump into implementation